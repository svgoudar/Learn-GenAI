{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6791e95",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## Types and Architectures\n",
    "\n",
    "Generative AI models can be classified based on **how** they learn from data and **what kind of output** they generate.\n",
    "There are **five main types** of generative AI architectures:\n",
    "\n",
    "1. **Generative Adversarial Networks (GANs)**\n",
    "2. **Variational Autoencoders (VAEs)**\n",
    "3. **Autoregressive Models (Transformers, LLMs)**\n",
    "4. **Diffusion Models**\n",
    "5. **Flow-based Models**\n",
    "\n",
    "Letâ€™s go through them in detail.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. âš”ï¸ Generative Adversarial Networks (GANs)\n",
    "\n",
    "### Concept:\n",
    "\n",
    "Two neural networks â€” a **Generator (G)** and a **Discriminator (D)** â€” compete against each other.\n",
    "\n",
    "* **Generator:** Tries to create *fake data* that looks real.\n",
    "* **Discriminator:** Tries to distinguish *real data* from *fake data*.\n",
    "\n",
    "Through this competition, both networks improve until the generator becomes so good that the discriminator can no longer tell fake from real.\n",
    "\n",
    "### Architecture:\n",
    "\n",
    "```\n",
    "Noise â†’ Generator â†’ Fake Data â†’ Discriminator â†’ Real or Fake\n",
    "```\n",
    "\n",
    "### Training Process:\n",
    "\n",
    "1. The generator creates synthetic data from random noise.\n",
    "2. The discriminator evaluates whether the data is real or generated.\n",
    "3. Both networks learn:\n",
    "\n",
    "   * The generator learns to fool the discriminator.\n",
    "   * The discriminator learns to detect fakes.\n",
    "\n",
    "### Use Cases:\n",
    "\n",
    "* **Image generation:** DeepFake, face synthesis, super-resolution.\n",
    "* **Art & design:** AI art tools like Artbreeder.\n",
    "* **Video synthesis:** Creating realistic motion frames.\n",
    "\n",
    "### Example:\n",
    "\n",
    "* **StyleGAN** (NVIDIA) â€” generates ultra-realistic human faces.\n",
    "* **CycleGAN** â€” converts one image domain to another (e.g., horse â†’ zebra).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. ðŸŒ€ Variational Autoencoders (VAEs)\n",
    "\n",
    "### Concept:\n",
    "\n",
    "VAEs learn to **encode** data into a lower-dimensional space (latent representation) and then **decode** it back into new data that resembles the input.\n",
    "\n",
    "### Architecture:\n",
    "\n",
    "```\n",
    "Input â†’ Encoder â†’ Latent Space â†’ Decoder â†’ Output\n",
    "```\n",
    "\n",
    "### Key Mechanism:\n",
    "\n",
    "* Instead of encoding data into a fixed vector, VAEs encode it into a **distribution** (mean and variance).\n",
    "* Random samples from this distribution are decoded to create new, realistic variations.\n",
    "\n",
    "### Use Cases:\n",
    "\n",
    "* **Image generation & editing**\n",
    "* **Data compression**\n",
    "* **Anomaly detection** (by comparing reconstruction error)\n",
    "\n",
    "### Example:\n",
    "\n",
    "* **DALLÂ·Eâ€™s early versions** used concepts similar to VAEs for image representation.\n",
    "* Used in **medical imaging** for synthetic data generation.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. ðŸ“š Autoregressive Models (Transformers and LLMs)\n",
    "\n",
    "### Concept:\n",
    "\n",
    "These models **predict the next token (word, pixel, note, etc.)** based on previous tokens.\n",
    "They generate data **sequentially**, one element at a time.\n",
    "\n",
    "### Architecture:\n",
    "\n",
    "* Uses **Transformer** architecture with **self-attention** mechanism.\n",
    "* Trained on massive datasets of text, images, or code.\n",
    "\n",
    "### Training Objective:\n",
    "\n",
    "Predict the probability of the next token:\n",
    "[\n",
    "P(x_t | x_1, x_2, ..., x_{t-1})\n",
    "]\n",
    "\n",
    "### Use Cases:\n",
    "\n",
    "* **Text generation:** ChatGPT, GPT-4, Claude, Gemini.\n",
    "* **Code generation:** GitHub Copilot, Code Llama.\n",
    "* **Music & speech:** Autoregressive audio synthesis.\n",
    "* **Multimodal models:** Generate text, images, or video jointly.\n",
    "\n",
    "### Examples:\n",
    "\n",
    "* **GPT (OpenAI)** â€” text generation.\n",
    "* **T5, BERT, LLaMA, Claude** â€” language and reasoning.\n",
    "* **Gemini, GPT-4V** â€” multimodal (text + image understanding).\n",
    "\n",
    "---\n",
    "\n",
    "## 4. ðŸŒ«ï¸ Diffusion Models\n",
    "\n",
    "### Concept:\n",
    "\n",
    "Diffusion models generate data by **starting with random noise** and **gradually removing the noise** to reconstruct meaningful content.\n",
    "\n",
    "They learn the **reverse process** of adding noise during training.\n",
    "\n",
    "### Process:\n",
    "\n",
    "1. Start with an image â†’ add random noise step-by-step â†’ it becomes pure noise.\n",
    "2. Train the model to **reverse this process** (denoising).\n",
    "3. During generation, start with random noise â†’ iteratively denoise to get a realistic image.\n",
    "\n",
    "### Architecture:\n",
    "\n",
    "* Based on **U-Net** and **Transformer** hybrids.\n",
    "* Uses **denoising score matching** or **variational inference**.\n",
    "\n",
    "### Use Cases:\n",
    "\n",
    "* **Image & video generation:** DALLÂ·E 2, Stable Diffusion, Midjourney.\n",
    "* **3D & animation:** Generating realistic 3D textures or animations.\n",
    "* **Audio generation:** Text-to-speech synthesis.\n",
    "\n",
    "### Example:\n",
    "\n",
    "* **Stable Diffusion:** Open-source text-to-image model.\n",
    "* **Imagen (Google), Midjourney:** High-quality art creation.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. ðŸ” Flow-based Models\n",
    "\n",
    "### Concept:\n",
    "\n",
    "Flow-based models learn an **invertible mapping** between the data space and a simple distribution (like a Gaussian).\n",
    "\n",
    "Unlike GANs or VAEs, they can exactly compute the **likelihood** of data and generate samples by reversing the flow.\n",
    "\n",
    "### Architecture:\n",
    "\n",
    "```\n",
    "Data â†” Latent Space\n",
    "```\n",
    "\n",
    "The same function is used both ways (invertible transformation).\n",
    "\n",
    "### Use Cases:\n",
    "\n",
    "* **Image synthesis**\n",
    "* **Super-resolution**\n",
    "* **Density estimation**\n",
    "\n",
    "### Examples:\n",
    "\n",
    "* **Glow (OpenAI)**\n",
    "* **RealNVP (Dinh et al.)**\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸ“Š Comparison Summary\n",
    "\n",
    "| Model Type                        | Key Idea                         | Output Type       | Strengths                    | Weaknesses                 |\n",
    "| --------------------------------- | -------------------------------- | ----------------- | ---------------------------- | -------------------------- |\n",
    "| **GANs**                          | Generator vs Discriminator       | Images, videos    | Sharp & realistic outputs    | Unstable training          |\n",
    "| **VAEs**                          | Encode & Decode latent variables | Images, audio     | Smooth latent space          | Blurry outputs             |\n",
    "| **Autoregressive (Transformers)** | Next-token prediction            | Text, code, audio | Contextual understanding     | Slow sequential generation |\n",
    "| **Diffusion Models**              | Denoising process                | Images, video     | High-quality outputs, stable | Computationally heavy      |\n",
    "| **Flow-based Models**             | Invertible transformations       | Images            | Exact likelihoods            | Limited scalability        |\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸ§© Hybrid and Advanced Architectures\n",
    "\n",
    "Modern generative systems often **combine** multiple types:\n",
    "\n",
    "* **DALLÂ·E 3:** Uses Diffusion + Transformer.\n",
    "* **Stable Diffusion XL:** Diffusion + CLIP encoder.\n",
    "* **ChatGPT-4 / Gemini:** Autoregressive Transformer + RLHF.\n",
    "* **MusicLM / AudioLM:** Transformer-based audio generation.\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸ§  Simplified Analogy\n",
    "\n",
    "| Type                    | Analogy                                                        |\n",
    "| ----------------------- | -------------------------------------------------------------- |\n",
    "| **GANs**                | Two artists: one creates fakes, the other spots them.          |\n",
    "| **VAEs**                | An artist who sketches a pattern and adds creative variations. |\n",
    "| **Transformers (LLMs)** | A writer predicting the next word in a story.                  |\n",
    "| **Diffusion**           | A sculptor carving meaning out of noisy marble.                |\n",
    "| **Flow Models**         | A reversible mold â€” turn design into object and back.          |\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸš€ Real-World Examples by Domain\n",
    "\n",
    "| Domain          | Model                              | Type              |\n",
    "| --------------- | ---------------------------------- | ----------------- |\n",
    "| **Text**        | GPT-4, Claude, LLaMA               | Transformer       |\n",
    "| **Image**       | Stable Diffusion, DALLÂ·E           | Diffusion         |\n",
    "| **Video**       | Runway Gen-2                       | Diffusion         |\n",
    "| **Audio**       | Jukebox (OpenAI), MusicLM (Google) | Transformer / VAE |\n",
    "| **3D & Design** | Point-E, DreamFusion               | Diffusion / VAE   |\n",
    "| **Multimodal**  | Gemini, GPT-4V                     | Transformer       |\n",
    "\n",
    "---\n",
    "\n",
    "# âœ… Summary\n",
    "\n",
    "| Type                    | Concept                    | Best For                              |\n",
    "| ----------------------- | -------------------------- | ------------------------------------- |\n",
    "| **GANs**                | Adversarial learning       | Realistic image generation            |\n",
    "| **VAEs**                | Latent space encoding      | Image reconstruction & variation      |\n",
    "| **Transformers (LLMs)** | Sequence modeling          | Text, code, multimodal generation     |\n",
    "| **Diffusion Models**    | Denoising noise            | High-quality art & visuals            |\n",
    "| **Flow-based**          | Reversible transformations | Density modeling, exact probabilities |\n",
    "\n",
    "---\n",
    "\n",
    "**In short:**\n",
    "\n",
    "> Generative AI spans multiple model types â€” from *GANs that imagine faces*, to *Transformers that write essays*, to *Diffusion models that paint from text.*\n",
    "> Together, they enable machines not just to **understand**, but to **create**.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
