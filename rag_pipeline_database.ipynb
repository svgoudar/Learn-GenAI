{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dd4fced",
   "metadata": {},
   "source": [
    "# RAG Pipeline for Database Knowledge Q&A\n",
    "\n",
    "This notebook demonstrates how to create a complete RAG (Retrieval-Augmented Generation) pipeline using database dumps as knowledge sources for question-answering systems.\n",
    "\n",
    "## Overview\n",
    "- Load and process various database dump formats (SQL, JSON, CSV)\n",
    "- Create embeddings and vector store\n",
    "- Set up retrieval system\n",
    "- Implement Q&A interface with LLMs\n",
    "- Build evaluation and monitoring\n",
    "\n",
    "## Features\n",
    "- üóÉÔ∏è **Multi-format support**: SQL dumps, JSON exports, CSV files\n",
    "- üîç **Smart retrieval**: Semantic search with metadata filtering\n",
    "- üß† **LLM integration**: Support for local and cloud models\n",
    "- üìä **Analytics**: Query performance and relevance tracking\n",
    "- üéØ **Context-aware**: Maintains database relationships and structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe538d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import sqlparse\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# RAG and Vector Store Libraries\n",
    "try:\n",
    "    from langchain.schema import Document\n",
    "    from langchain_community.vectorstores import Chroma, FAISS\n",
    "    from langchain_community.embeddings import OpenAIEmbeddings\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    from langchain.chains import RetrievalQA\n",
    "    from langchain_community.llms import OpenAI\n",
    "    print(\"‚úì LangChain libraries available\")\n",
    "except ImportError:\n",
    "    print(\"Installing LangChain...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"langchain\", \"langchain-community\", \"langchain-openai\"])\n",
    "    from langchain.schema import Document\n",
    "    from langchain_community.vectorstores import Chroma, FAISS\n",
    "    from langchain_community.embeddings import OpenAIEmbeddings\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    from langchain.chains import RetrievalQA\n",
    "    from langchain_community.llms import OpenAI\n",
    "\n",
    "# Alternative embedding models\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    print(\"‚úì Sentence Transformers available\")\n",
    "    SENTENCE_TRANSFORMERS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Installing sentence-transformers for local embeddings...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"sentence-transformers\"])\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    SENTENCE_TRANSFORMERS_AVAILABLE = True\n",
    "\n",
    "# Vector database options\n",
    "try:\n",
    "    import chromadb\n",
    "    print(\"‚úì ChromaDB available\")\n",
    "    CHROMADB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Installing ChromaDB...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"chromadb\"])\n",
    "    import chromadb\n",
    "    CHROMADB_AVAILABLE = True\n",
    "\n",
    "# Azure AI Inference for Q&A\n",
    "try:\n",
    "    from azure.ai.inference import ChatCompletionsClient\n",
    "    from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "    from azure.core.credentials import AzureKeyCredential\n",
    "    print(\"‚úì Azure AI Inference available\")\n",
    "    AZURE_AI_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Installing Azure AI Inference...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"azure-ai-inference\"])\n",
    "    from azure.ai.inference import ChatCompletionsClient\n",
    "    from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "    from azure.core.credentials import AzureKeyCredential\n",
    "    AZURE_AI_AVAILABLE = True\n",
    "\n",
    "print(f\"\\nüöÄ RAG Pipeline Setup Complete\")\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Configuration\n",
    "BASE_DIR = Path(r\"c:\\Github\\Learn-GenAI\\genai_book\")\n",
    "RAG_DATA_DIR = BASE_DIR / \"rag_data\"\n",
    "RAG_DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"RAG data directory: {RAG_DATA_DIR}\")\n",
    "print(f\"Available embedding models: {'Local + Cloud' if SENTENCE_TRANSFORMERS_AVAILABLE else 'Cloud only'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df276ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database/CSV Loader and Processor\n",
    "class DatabaseRAGLoader:\n",
    "    \"\"\"\n",
    "    Comprehensive loader for various database dump formats\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir: Path):\n",
    "        self.data_dir = data_dir\n",
    "        self.documents = []\n",
    "        \n",
    "    def load_csv_file(self, csv_path: str, chunk_size: int = 1000) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Load large CSV files in chunks and convert to documents\n",
    "        \"\"\"\n",
    "        csv_file = Path(csv_path)\n",
    "        if not csv_file.exists():\n",
    "            print(f\"‚ùå CSV file not found: {csv_file}\")\n",
    "            return []\n",
    "            \n",
    "        print(f\"üìä Loading CSV file: {csv_file}\")\n",
    "        documents = []\n",
    "        \n",
    "        try:\n",
    "            # Read CSV in chunks to handle large files\n",
    "            chunk_iter = pd.read_csv(csv_file, chunksize=chunk_size)\n",
    "            \n",
    "            for chunk_idx, chunk in enumerate(chunk_iter):\n",
    "                print(f\"  Processing chunk {chunk_idx + 1} ({len(chunk)} rows)\")\n",
    "                \n",
    "                # Get column information\n",
    "                columns = list(chunk.columns)\n",
    "                \n",
    "                # Convert each row to a document\n",
    "                for idx, row in chunk.iterrows():\n",
    "                    # Create a readable text representation\n",
    "                    content_parts = []\n",
    "                    \n",
    "                    # Add structured information\n",
    "                    for col in columns:\n",
    "                        value = row[col]\n",
    "                        if pd.notna(value):\n",
    "                            content_parts.append(f\"{col}: {value}\")\n",
    "                    \n",
    "                    content = \"\\n\".join(content_parts)\n",
    "                    \n",
    "                    # Create document with metadata\n",
    "                    doc = Document(\n",
    "                        page_content=content,\n",
    "                        metadata={\n",
    "                            'source': str(csv_file),\n",
    "                            'row_id': int(idx),\n",
    "                            'chunk_id': chunk_idx,\n",
    "                            'record_type': 'database_record',\n",
    "                            'columns': columns,\n",
    "                            'file_size_mb': csv_file.stat().st_size / (1024 * 1024),\n",
    "                            'row_count': len(chunk)\n",
    "                        }\n",
    "                    )\n",
    "                    documents.append(doc)\n",
    "                \n",
    "                # Process in smaller batches to avoid memory issues\n",
    "                if chunk_idx >= 10:  # Limit for demo - adjust as needed\n",
    "                    print(f\"  ‚ö†Ô∏è Processing limited to first {chunk_idx + 1} chunks for demo\")\n",
    "                    break\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading CSV: {e}\")\n",
    "            return []\n",
    "            \n",
    "        print(f\"‚úÖ Loaded {len(documents)} documents from CSV\")\n",
    "        return documents\n",
    "    \n",
    "    def load_sql_dump(self, sql_path: str) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Load SQL dump files and extract meaningful content\n",
    "        \"\"\"\n",
    "        sql_file = Path(sql_path)\n",
    "        if not sql_file.exists():\n",
    "            return []\n",
    "            \n",
    "        print(f\"üóÉÔ∏è Loading SQL dump: {sql_file}\")\n",
    "        documents = []\n",
    "        \n",
    "        try:\n",
    "            with open(sql_file, 'r', encoding='utf-8') as f:\n",
    "                sql_content = f.read()\n",
    "            \n",
    "            # Parse SQL statements\n",
    "            statements = sqlparse.split(sql_content)\n",
    "            \n",
    "            for idx, statement in enumerate(statements):\n",
    "                if statement.strip():\n",
    "                    parsed = sqlparse.parse(statement)[0]\n",
    "                    \n",
    "                    # Extract table information and data\n",
    "                    content = f\"SQL Statement {idx + 1}:\\n{statement}\"\n",
    "                    \n",
    "                    doc = Document(\n",
    "                        page_content=content,\n",
    "                        metadata={\n",
    "                            'source': str(sql_file),\n",
    "                            'statement_id': idx,\n",
    "                            'record_type': 'sql_statement',\n",
    "                            'statement_type': parsed.get_type()\n",
    "                        }\n",
    "                    )\n",
    "                    documents.append(doc)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading SQL dump: {e}\")\n",
    "            \n",
    "        return documents\n",
    "    \n",
    "    def load_json_dump(self, json_path: str) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Load JSON database exports\n",
    "        \"\"\"\n",
    "        json_file = Path(json_path)\n",
    "        if not json_file.exists():\n",
    "            return []\n",
    "            \n",
    "        print(f\"üìù Loading JSON dump: {json_file}\")\n",
    "        documents = []\n",
    "        \n",
    "        try:\n",
    "            with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Handle different JSON structures\n",
    "            if isinstance(data, list):\n",
    "                # Array of records\n",
    "                for idx, record in enumerate(data):\n",
    "                    content = json.dumps(record, indent=2, ensure_ascii=False)\n",
    "                    \n",
    "                    doc = Document(\n",
    "                        page_content=content,\n",
    "                        metadata={\n",
    "                            'source': str(json_file),\n",
    "                            'record_id': idx,\n",
    "                            'record_type': 'json_record'\n",
    "                        }\n",
    "                    )\n",
    "                    documents.append(doc)\n",
    "                    \n",
    "            elif isinstance(data, dict):\n",
    "                # Single object or nested structure\n",
    "                for key, value in data.items():\n",
    "                    content = f\"Key: {key}\\nValue: {json.dumps(value, indent=2, ensure_ascii=False)}\"\n",
    "                    \n",
    "                    doc = Document(\n",
    "                        page_content=content,\n",
    "                        metadata={\n",
    "                            'source': str(json_file),\n",
    "                            'key': key,\n",
    "                            'record_type': 'json_key_value'\n",
    "                        }\n",
    "                    )\n",
    "                    documents.append(doc)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading JSON dump: {e}\")\n",
    "            \n",
    "        return documents\n",
    "\n",
    "# Initialize the loader\n",
    "loader = DatabaseRAGLoader(RAG_DATA_DIR)\n",
    "\n",
    "# Load your CSV file\n",
    "csv_file_path = r\"c:\\Github\\Learn-GenAI\\genai_book\\llm\\data_ingestion\\endpoint.csv\"\n",
    "print(f\"üîÑ Starting to load CSV file...\")\n",
    "documents = loader.load_csv_file(csv_file_path, chunk_size=500)  # Smaller chunks for large file\n",
    "\n",
    "print(f\"\\nüìä CSV Loading Summary:\")\n",
    "print(f\"Total documents created: {len(documents)}\")\n",
    "\n",
    "if documents:\n",
    "    # Show sample document\n",
    "    sample_doc = documents[0]\n",
    "    print(f\"\\nSample document metadata:\")\n",
    "    for key, value in sample_doc.metadata.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(f\"\\nSample content preview:\")\n",
    "    print(f\"{sample_doc.page_content[:300]}...\")\n",
    "    \n",
    "    # Show statistics\n",
    "    chunks = set(doc.metadata.get('chunk_id', 0) for doc in documents)\n",
    "    print(f\"\\nData statistics:\")\n",
    "    print(f\"  Total chunks processed: {len(chunks)}\")\n",
    "    print(f\"  Records per chunk: ~{len(documents) // len(chunks) if chunks else 0}\")\n",
    "    print(f\"  Average content length: {sum(len(doc.page_content) for doc in documents) // len(documents)} characters\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No documents loaded. Please check the CSV file path and format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a27da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Processing and Chunking for RAG\n",
    "class DatabaseTextProcessor:\n",
    "    \"\"\"\n",
    "    Specialized text processor for database content\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=100,\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \": \", \", \", \" \"]\n",
    "        )\n",
    "    \n",
    "    def process_documents(self, documents: List[Document]) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Process and chunk documents optimally for database content\n",
    "        \"\"\"\n",
    "        if not documents:\n",
    "            return []\n",
    "            \n",
    "        print(f\"üîÑ Processing {len(documents)} documents...\")\n",
    "        processed_docs = []\n",
    "        \n",
    "        for doc in documents:\n",
    "            # For database records, we might want to keep them as single chunks\n",
    "            # or split them intelligently based on content length\n",
    "            \n",
    "            if len(doc.page_content) > 1500:  # Split large records\n",
    "                chunks = self.splitter.split_text(doc.page_content)\n",
    "                \n",
    "                for i, chunk in enumerate(chunks):\n",
    "                    chunk_doc = Document(\n",
    "                        page_content=chunk,\n",
    "                        metadata={\n",
    "                            **doc.metadata,\n",
    "                            'chunk_index': i,\n",
    "                            'total_chunks': len(chunks),\n",
    "                            'is_chunked': True\n",
    "                        }\n",
    "                    )\n",
    "                    processed_docs.append(chunk_doc)\n",
    "            else:\n",
    "                # Keep small records intact\n",
    "                doc.metadata['is_chunked'] = False\n",
    "                processed_docs.append(doc)\n",
    "        \n",
    "        print(f\"‚úÖ Created {len(processed_docs)} processed chunks\")\n",
    "        return processed_docs\n",
    "\n",
    "# Process the loaded documents\n",
    "processor = DatabaseTextProcessor()\n",
    "processed_documents = processor.process_documents(documents)\n",
    "\n",
    "if processed_documents:\n",
    "    print(f\"\\nDocument Processing Summary:\")\n",
    "    print(f\"Original documents: {len(documents)}\")\n",
    "    print(f\"Processed chunks: {len(processed_documents)}\")\n",
    "    \n",
    "    # Count chunked vs non-chunked\n",
    "    chunked = len([d for d in processed_documents if d.metadata.get('is_chunked', False)])\n",
    "    non_chunked = len(processed_documents) - chunked\n",
    "    print(f\"Chunked documents: {chunked}\")\n",
    "    print(f\"Non-chunked documents: {non_chunked}\")\n",
    "    \n",
    "    # Show content length distribution\n",
    "    lengths = [len(doc.page_content) for doc in processed_documents]\n",
    "    print(f\"\\nContent length statistics:\")\n",
    "    print(f\"  Min: {min(lengths)} characters\")\n",
    "    print(f\"  Max: {max(lengths)} characters\") \n",
    "    print(f\"  Average: {sum(lengths) // len(lengths)} characters\")\n",
    "    \n",
    "    # Sample processed document\n",
    "    sample = processed_documents[0]\n",
    "    print(f\"\\nSample processed document:\")\n",
    "    print(f\"Content length: {len(sample.page_content)}\")\n",
    "    print(f\"Is chunked: {sample.metadata.get('is_chunked')}\")\n",
    "    print(f\"Content preview: {sample.page_content[:200]}...\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No documents to process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047fe401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Creation and Vector Store Setup\n",
    "class DatabaseEmbeddingManager:\n",
    "    \"\"\"\n",
    "    Manages embeddings and vector store for database RAG\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, use_local_embeddings: bool = True):\n",
    "        self.use_local = use_local_embeddings\n",
    "        self.embeddings = None\n",
    "        self.vector_store = None\n",
    "        \n",
    "        if use_local_embeddings and SENTENCE_TRANSFORMERS_AVAILABLE:\n",
    "            print(\"üîÑ Loading local embedding model...\")\n",
    "            # Use a model optimized for diverse content\n",
    "            self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "            print(\"‚úÖ Local embedding model loaded\")\n",
    "        else:\n",
    "            print(\"üåê Using OpenAI embeddings (requires API key)\")\n",
    "            self.embeddings = OpenAIEmbeddings()\n",
    "    \n",
    "    def create_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Create embeddings for text content\n",
    "        \"\"\"\n",
    "        if self.use_local and hasattr(self, 'embedding_model'):\n",
    "            return self.embedding_model.encode(texts)\n",
    "        else:\n",
    "            # Would use OpenAI embeddings if API key is available\n",
    "            return None\n",
    "    \n",
    "    def create_vector_store(self, documents: List[Document], store_type: str = \"chromadb\") -> Any:\n",
    "        \"\"\"\n",
    "        Create vector store from documents\n",
    "        \"\"\"\n",
    "        if not documents:\n",
    "            print(\"‚ùå No documents provided for vector store\")\n",
    "            return None\n",
    "            \n",
    "        print(f\"üîÑ Creating {store_type} vector store with {len(documents)} documents...\")\n",
    "        \n",
    "        try:\n",
    "            if store_type == \"chromadb\":\n",
    "                # Create custom embedding function for ChromaDB\n",
    "                class LocalEmbeddingFunction:\n",
    "                    def __init__(self, model):\n",
    "                        self.model = model\n",
    "                    \n",
    "                    def __call__(self, texts):\n",
    "                        embeddings = self.model.encode(texts)\n",
    "                        return embeddings.tolist()\n",
    "                \n",
    "                # Initialize ChromaDB\n",
    "                chroma_client = chromadb.Client()\n",
    "                \n",
    "                # Create collection\n",
    "                collection_name = f\"database_rag_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "                \n",
    "                # Extract texts and metadata\n",
    "                texts = [doc.page_content for doc in documents]\n",
    "                metadatas = [doc.metadata for doc in documents]\n",
    "                ids = [f\"doc_{i}\" for i in range(len(documents))]\n",
    "                \n",
    "                # Create embeddings if using local model\n",
    "                if self.use_local and hasattr(self, 'embedding_model'):\n",
    "                    embeddings = self.create_embeddings(texts)\n",
    "                    \n",
    "                    # Create collection\n",
    "                    collection = chroma_client.create_collection(\n",
    "                        name=collection_name,\n",
    "                        metadata={\"description\": \"Database RAG Collection\"}\n",
    "                    )\n",
    "                    \n",
    "                    # Add documents\n",
    "                    collection.add(\n",
    "                        embeddings=embeddings.tolist(),\n",
    "                        documents=texts,\n",
    "                        metadatas=metadatas,\n",
    "                        ids=ids\n",
    "                    )\n",
    "                    \n",
    "                    print(f\"‚úÖ ChromaDB collection '{collection_name}' created\")\n",
    "                    return collection\n",
    "                \n",
    "            elif store_type == \"faiss\":\n",
    "                # Use FAISS for local vector store\n",
    "                if self.use_local and hasattr(self, 'embedding_model'):\n",
    "                    vector_store = FAISS.from_documents(\n",
    "                        documents, \n",
    "                        embedding=self.embedding_model.encode\n",
    "                    )\n",
    "                    \n",
    "                    # Save to disk\n",
    "                    faiss_path = RAG_DATA_DIR / \"faiss_index\"\n",
    "                    vector_store.save_local(str(faiss_path))\n",
    "                    print(f\"‚úÖ FAISS index saved to {faiss_path}\")\n",
    "                    return vector_store\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error creating vector store: {e}\")\n",
    "            return None\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def search_similar(self, query: str, collection, top_k: int = 5) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Search for similar documents\n",
    "        \"\"\"\n",
    "        if not collection:\n",
    "            return []\n",
    "            \n",
    "        try:\n",
    "            if self.use_local and hasattr(self, 'embedding_model'):\n",
    "                # Create query embedding\n",
    "                query_embedding = self.embedding_model.encode([query])\n",
    "                \n",
    "                # Search in ChromaDB\n",
    "                results = collection.query(\n",
    "                    query_embeddings=query_embedding.tolist(),\n",
    "                    n_results=top_k,\n",
    "                    include=['documents', 'metadatas', 'distances']\n",
    "                )\n",
    "                \n",
    "                return results\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error searching: {e}\")\n",
    "            return []\n",
    "\n",
    "# Initialize embedding manager\n",
    "embedding_manager = DatabaseEmbeddingManager(use_local_embeddings=True)\n",
    "\n",
    "# Create vector store from processed documents\n",
    "if processed_documents:\n",
    "    print(f\"üîÑ Creating vector store...\")\n",
    "    vector_store = embedding_manager.create_vector_store(\n",
    "        processed_documents, \n",
    "        store_type=\"chromadb\"\n",
    "    )\n",
    "    \n",
    "    if vector_store:\n",
    "        print(f\"‚úÖ Vector store created successfully\")\n",
    "        \n",
    "        # Test search functionality\n",
    "        test_query = \"What information is available?\"\n",
    "        print(f\"\\nüîç Testing search with query: '{test_query}'\")\n",
    "        \n",
    "        search_results = embedding_manager.search_similar(test_query, vector_store, top_k=3)\n",
    "        \n",
    "        if search_results and 'documents' in search_results:\n",
    "            print(f\"Found {len(search_results['documents'][0])} results:\")\n",
    "            for i, (doc, metadata, distance) in enumerate(zip(\n",
    "                search_results['documents'][0], \n",
    "                search_results['metadatas'][0],\n",
    "                search_results['distances'][0]\n",
    "            )):\n",
    "                print(f\"\\n{i+1}. (Distance: {distance:.3f})\")\n",
    "                print(f\"   Metadata: {metadata}\")\n",
    "                print(f\"   Content: {doc[:150]}...\")\n",
    "        else:\n",
    "            print(\"No search results found\")\n",
    "    else:\n",
    "        print(\"‚ùå Failed to create vector store\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No processed documents available for vector store creation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf21ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG Q&A System Implementation\n",
    "class DatabaseRAGSystem:\n",
    "    \"\"\"\n",
    "    Complete RAG system for database Q&A\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store, embedding_manager):\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "        self.llm_client = None\n",
    "        \n",
    "        # Initialize LLM client (GitHub Models)\n",
    "        self.setup_llm_client()\n",
    "    \n",
    "    def setup_llm_client(self):\n",
    "        \"\"\"\n",
    "        Setup LLM client for Q&A\n",
    "        \"\"\"\n",
    "        github_token = os.environ.get(\"GITHUB_TOKEN\")\n",
    "        \n",
    "        if github_token and AZURE_AI_AVAILABLE:\n",
    "            try:\n",
    "                self.llm_client = ChatCompletionsClient(\n",
    "                    endpoint=\"https://models.github.ai/inference\",\n",
    "                    credential=AzureKeyCredential(github_token),\n",
    "                )\n",
    "                print(\"‚úÖ LLM client connected (GitHub Models)\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è LLM client setup failed: {e}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è GITHUB_TOKEN not set - LLM functionality limited\")\n",
    "    \n",
    "    def retrieve_relevant_docs(self, query: str, top_k: int = 5) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for the query\n",
    "        \"\"\"\n",
    "        if not self.vector_store:\n",
    "            return []\n",
    "        \n",
    "        search_results = self.embedding_manager.search_similar(query, self.vector_store, top_k)\n",
    "        \n",
    "        relevant_docs = []\n",
    "        if search_results and 'documents' in search_results:\n",
    "            for doc, metadata, distance in zip(\n",
    "                search_results['documents'][0],\n",
    "                search_results['metadatas'][0], \n",
    "                search_results['distances'][0]\n",
    "            ):\n",
    "                relevant_docs.append({\n",
    "                    'content': doc,\n",
    "                    'metadata': metadata,\n",
    "                    'relevance_score': 1 - distance,  # Convert distance to similarity\n",
    "                    'distance': distance\n",
    "                })\n",
    "        \n",
    "        return relevant_docs\n",
    "    \n",
    "    def generate_answer(self, query: str, relevant_docs: List[Dict]) -> Dict:\n",
    "        \"\"\"\n",
    "        Generate answer using LLM and retrieved context\n",
    "        \"\"\"\n",
    "        if not self.llm_client:\n",
    "            return {\n",
    "                \"answer\": \"LLM not available. Please set GITHUB_TOKEN environment variable.\",\n",
    "                \"confidence\": 0.0,\n",
    "                \"sources\": []\n",
    "            }\n",
    "        \n",
    "        # Prepare context from relevant documents\n",
    "        context_parts = []\n",
    "        sources = []\n",
    "        \n",
    "        for i, doc in enumerate(relevant_docs):\n",
    "            context_parts.append(f\"Document {i+1}:\\n{doc['content']}\")\n",
    "            sources.append({\n",
    "                'doc_id': i+1,\n",
    "                'metadata': doc['metadata'],\n",
    "                'relevance': doc['relevance_score']\n",
    "            })\n",
    "        \n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "        \n",
    "        # Create system and user messages\n",
    "        system_message = \"\"\"You are a helpful AI assistant that answers questions based on database information. \n",
    "        Use the provided context to answer the user's question accurately and concisely.\n",
    "        If the context doesn't contain enough information, say so clearly.\n",
    "        Always cite which document(s) you used for your answer.\"\"\"\n",
    "        \n",
    "        user_message = f\"\"\"Context from database:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Please provide a comprehensive answer based on the context above.\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.llm_client.complete(\n",
    "                messages=[\n",
    "                    SystemMessage(system_message),\n",
    "                    UserMessage(user_message),\n",
    "                ],\n",
    "                temperature=0.3,\n",
    "                max_tokens=500,\n",
    "                model=\"openai/gpt-4.1-mini\"\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"answer\": response.choices[0].message.content,\n",
    "                \"confidence\": 0.85,  # Could be calculated based on retrieval scores\n",
    "                \"sources\": sources,\n",
    "                \"tokens_used\": response.usage.total_tokens if response.usage else None\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"answer\": f\"Error generating response: {e}\",\n",
    "                \"confidence\": 0.0,\n",
    "                \"sources\": sources\n",
    "            }\n",
    "    \n",
    "    def ask_question(self, query: str, top_k: int = 5) -> Dict:\n",
    "        \"\"\"\n",
    "        Complete Q&A pipeline\n",
    "        \"\"\"\n",
    "        print(f\"ü§î Question: {query}\")\n",
    "        print(\"üîç Retrieving relevant information...\")\n",
    "        \n",
    "        # Retrieve relevant documents\n",
    "        relevant_docs = self.retrieve_relevant_docs(query, top_k)\n",
    "        \n",
    "        if not relevant_docs:\n",
    "            return {\n",
    "                \"answer\": \"No relevant information found in the database.\",\n",
    "                \"confidence\": 0.0,\n",
    "                \"sources\": [],\n",
    "                \"query\": query\n",
    "            }\n",
    "        \n",
    "        print(f\"üìö Found {len(relevant_docs)} relevant documents\")\n",
    "        \n",
    "        # Generate answer\n",
    "        print(\"ü§ñ Generating answer...\")\n",
    "        result = self.generate_answer(query, relevant_docs)\n",
    "        result[\"query\"] = query\n",
    "        result[\"retrieved_docs\"] = len(relevant_docs)\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Initialize RAG system\n",
    "if vector_store and embedding_manager:\n",
    "    rag_system = DatabaseRAGSystem(vector_store, embedding_manager)\n",
    "    \n",
    "    # Test the Q&A system\n",
    "    test_questions = [\n",
    "        \"What data is available in this database?\",\n",
    "        \"Can you summarize the main information?\",\n",
    "        \"What are the key fields or columns?\",\n",
    "        \"How many records are there?\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nüß™ Testing RAG System with sample questions:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for question in test_questions:\n",
    "        print(f\"\\n‚ùì {question}\")\n",
    "        result = rag_system.ask_question(question, top_k=3)\n",
    "        \n",
    "        print(f\"üí° Answer: {result['answer']}\")\n",
    "        print(f\"üéØ Confidence: {result['confidence']:.2f}\")\n",
    "        print(f\"üìñ Sources: {result['retrieved_docs']} documents\")\n",
    "        if result.get('tokens_used'):\n",
    "            print(f\"üî§ Tokens used: {result['tokens_used']}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Vector store not available - cannot initialize RAG system\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebad90c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Q&A Interface and Performance Analytics\n",
    "class InteractiveRAG:\n",
    "    \"\"\"\n",
    "    Interactive interface for the RAG system with analytics\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, rag_system):\n",
    "        self.rag_system = rag_system\n",
    "        self.conversation_history = []\n",
    "        self.analytics = {\n",
    "            'total_queries': 0,\n",
    "            'avg_response_time': 0.0,\n",
    "            'satisfaction_scores': []\n",
    "        }\n",
    "    \n",
    "    def ask_interactive_question(self, question: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Ask a question with timing and analytics\n",
    "        \"\"\"\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        result = self.rag_system.ask_question(question)\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        response_time = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        # Add to conversation history\n",
    "        conversation_entry = {\n",
    "            'timestamp': start_time.isoformat(),\n",
    "            'question': question,\n",
    "            'answer': result['answer'],\n",
    "            'confidence': result['confidence'],\n",
    "            'response_time': response_time,\n",
    "            'sources_count': result['retrieved_docs']\n",
    "        }\n",
    "        \n",
    "        self.conversation_history.append(conversation_entry)\n",
    "        \n",
    "        # Update analytics\n",
    "        self.analytics['total_queries'] += 1\n",
    "        self.analytics['avg_response_time'] = (\n",
    "            (self.analytics['avg_response_time'] * (self.analytics['total_queries'] - 1) + response_time) \n",
    "            / self.analytics['total_queries']\n",
    "        )\n",
    "        \n",
    "        result['response_time'] = response_time\n",
    "        return result\n",
    "    \n",
    "    def save_conversation(self, filename: str = None):\n",
    "        \"\"\"\n",
    "        Save conversation history to file\n",
    "        \"\"\"\n",
    "        if not filename:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"rag_conversation_{timestamp}.json\"\n",
    "        \n",
    "        filepath = RAG_DATA_DIR / filename\n",
    "        \n",
    "        conversation_data = {\n",
    "            'conversation_history': self.conversation_history,\n",
    "            'analytics': self.analytics,\n",
    "            'metadata': {\n",
    "                'total_questions': len(self.conversation_history),\n",
    "                'session_start': self.conversation_history[0]['timestamp'] if self.conversation_history else None,\n",
    "                'session_end': self.conversation_history[-1]['timestamp'] if self.conversation_history else None\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(conversation_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"üíæ Conversation saved to: {filepath}\")\n",
    "        return filepath\n",
    "    \n",
    "    def generate_report(self):\n",
    "        \"\"\"\n",
    "        Generate performance and usage report\n",
    "        \"\"\"\n",
    "        if not self.conversation_history:\n",
    "            print(\"No conversation data available\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nüìä RAG SYSTEM PERFORMANCE REPORT\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Total queries: {self.analytics['total_queries']}\")\n",
    "        print(f\"Average response time: {self.analytics['avg_response_time']:.2f}s\")\n",
    "        \n",
    "        # Calculate confidence statistics\n",
    "        confidences = [entry['confidence'] for entry in self.conversation_history]\n",
    "        print(f\"Average confidence: {sum(confidences)/len(confidences):.2f}\")\n",
    "        print(f\"Min confidence: {min(confidences):.2f}\")\n",
    "        print(f\"Max confidence: {max(confidences):.2f}\")\n",
    "        \n",
    "        # Response time statistics\n",
    "        response_times = [entry['response_time'] for entry in self.conversation_history]\n",
    "        print(f\"Min response time: {min(response_times):.2f}s\")\n",
    "        print(f\"Max response time: {max(response_times):.2f}s\")\n",
    "        \n",
    "        # Sources statistics\n",
    "        sources_counts = [entry['sources_count'] for entry in self.conversation_history]\n",
    "        print(f\"Average sources retrieved: {sum(sources_counts)/len(sources_counts):.1f}\")\n",
    "        \n",
    "        # Recent questions\n",
    "        print(f\"\\nRecent Questions:\")\n",
    "        for i, entry in enumerate(self.conversation_history[-3:], 1):\n",
    "            print(f\"{i}. {entry['question']}\")\n",
    "            print(f\"   Confidence: {entry['confidence']:.2f} | Time: {entry['response_time']:.2f}s\")\n",
    "\n",
    "# Create interactive RAG interface\n",
    "if 'rag_system' in locals() and rag_system:\n",
    "    interactive_rag = InteractiveRAG(rag_system)\n",
    "    \n",
    "    print(f\"\\nüöÄ INTERACTIVE RAG SYSTEM READY\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"You can now ask questions about your database!\")\n",
    "    \n",
    "    # Demonstrate with some database-specific questions\n",
    "    demo_questions = [\n",
    "        \"What is the structure of this database?\",\n",
    "        \"How many records are in the dataset?\", \n",
    "        \"What are the main data categories?\",\n",
    "        \"Can you show me a sample record?\",\n",
    "        \"What fields contain the most information?\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nüéØ Demonstrating with database-specific questions:\")\n",
    "    \n",
    "    for question in demo_questions:\n",
    "        print(f\"\\n\" + \"=\"*60)\n",
    "        result = interactive_rag.ask_interactive_question(question)\n",
    "        \n",
    "        print(f\"‚ùì Question: {question}\")\n",
    "        print(f\"üí° Answer: {result['answer']}\")\n",
    "        print(f\"üìä Metrics: Confidence={result['confidence']:.2f} | Time={result['response_time']:.2f}s | Sources={result['retrieved_docs']}\")\n",
    "    \n",
    "    # Generate and save report\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    interactive_rag.generate_report()\n",
    "    \n",
    "    # Save conversation\n",
    "    conversation_file = interactive_rag.save_conversation()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Interactive RAG session complete!\")\n",
    "    print(f\"üìÅ Results saved to: {RAG_DATA_DIR}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è RAG system not available - please ensure previous cells ran successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee038fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Question Interface - Use this cell to ask your own questions!\n",
    "\n",
    "def ask_custom_question(question: str):\n",
    "    \"\"\"\n",
    "    Function to ask custom questions to the RAG system\n",
    "    \"\"\"\n",
    "    if 'interactive_rag' in locals() and interactive_rag:\n",
    "        print(f\"ü§î Your Question: {question}\")\n",
    "        print(\"üîç Processing...\")\n",
    "        \n",
    "        result = interactive_rag.ask_interactive_question(question)\n",
    "        \n",
    "        print(f\"\\nüí° Answer:\")\n",
    "        print(f\"{result['answer']}\")\n",
    "        print(f\"\\nüìä Details:\")\n",
    "        print(f\"  ‚Ä¢ Confidence: {result['confidence']:.2%}\")\n",
    "        print(f\"  ‚Ä¢ Response Time: {result['response_time']:.2f}s\")\n",
    "        print(f\"  ‚Ä¢ Sources Used: {result['retrieved_docs']} documents\")\n",
    "        \n",
    "        if result.get('tokens_used'):\n",
    "            print(f\"  ‚Ä¢ Tokens Used: {result['tokens_used']}\")\n",
    "        \n",
    "        # Show source information if available\n",
    "        if 'sources' in result and result['sources']:\n",
    "            print(f\"\\nüìö Source Documents:\")\n",
    "            for i, source in enumerate(result['sources'][:3], 1):  # Show top 3 sources\n",
    "                print(f\"  {i}. Relevance: {source['relevance']:.2%}\")\n",
    "                if 'row_id' in source['metadata']:\n",
    "                    print(f\"     Row ID: {source['metadata']['row_id']}\")\n",
    "                if 'chunk_id' in source['metadata']:\n",
    "                    print(f\"     Chunk: {source['metadata']['chunk_id']}\")\n",
    "        \n",
    "        return result\n",
    "    else:\n",
    "        print(\"‚ùå RAG system not available. Please run the previous cells first.\")\n",
    "        return None\n",
    "\n",
    "# Example usage - modify these questions to ask about your specific CSV data\n",
    "print(\"üéØ CUSTOM QUESTION INTERFACE\")\n",
    "print(\"=\"*50)\n",
    "print(\"Use the ask_custom_question() function to query your CSV database!\")\n",
    "print(\"\\nExample questions you can try:\")\n",
    "\n",
    "example_questions = [\n",
    "    \"What columns are in my CSV file?\",\n",
    "    \"Show me some example data from the database\",\n",
    "    \"What is the most common value in the dataset?\", \n",
    "    \"How is the data structured?\",\n",
    "    \"What patterns can you identify in the data?\",\n",
    "    \"Can you describe the data quality?\",\n",
    "    \"What insights can you provide about this dataset?\"\n",
    "]\n",
    "\n",
    "for i, q in enumerate(example_questions, 1):\n",
    "    print(f\"{i}. {q}\")\n",
    "\n",
    "print(f\"\\nüí° To ask a question, run:\")\n",
    "print(f\"ask_custom_question('Your question here')\")\n",
    "\n",
    "# Uncomment the line below and modify the question to test:\n",
    "# ask_custom_question(\"What information is in my CSV database?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a943373",
   "metadata": {},
   "source": [
    "## üéâ RAG Pipeline Complete!\n",
    "\n",
    "You now have a fully functional RAG system for your CSV database! Here's what you've built:\n",
    "\n",
    "### üîß **System Components**\n",
    "\n",
    "1. **üìä Data Loader** - Efficiently loads large CSV files in chunks\n",
    "2. **üîÑ Text Processor** - Optimizes database records for RAG\n",
    "3. **üß† Embedding Engine** - Creates semantic search capabilities\n",
    "4. **üóÑÔ∏è Vector Store** - ChromaDB for fast similarity search\n",
    "5. **ü§ñ Q&A System** - GitHub Models integration for answers\n",
    "6. **üìà Analytics** - Performance tracking and conversation history\n",
    "\n",
    "### üöÄ **How to Use**\n",
    "\n",
    "```python\n",
    "# Ask questions about your CSV data\n",
    "ask_custom_question(\"What is in my database?\")\n",
    "ask_custom_question(\"Show me the data structure\")\n",
    "ask_custom_question(\"What insights can you provide?\")\n",
    "```\n",
    "\n",
    "### üìã **Features**\n",
    "\n",
    "- ‚úÖ **Handles Large Files** - Processes CSV files in chunks\n",
    "- ‚úÖ **Semantic Search** - Finds relevant information using AI\n",
    "- ‚úÖ **Context Preservation** - Maintains database relationships\n",
    "- ‚úÖ **Performance Tracking** - Monitors speed and accuracy\n",
    "- ‚úÖ **Conversation History** - Saves all Q&A sessions\n",
    "- ‚úÖ **Local + Cloud** - Works with local embeddings and cloud LLMs\n",
    "\n",
    "### üéØ **Best Practices for Your CSV RAG**\n",
    "\n",
    "1. **Data Quality**: Clean data produces better answers\n",
    "2. **Question Specificity**: Specific questions get better results\n",
    "3. **Context Awareness**: Reference columns and data types in questions\n",
    "4. **Iterative Queries**: Build on previous questions for deeper insights\n",
    "\n",
    "### üîß **Production Deployment**\n",
    "\n",
    "For production use:\n",
    "- Set up persistent vector store\n",
    "- Implement user authentication\n",
    "- Add query rate limiting\n",
    "- Monitor system performance\n",
    "- Scale with more compute resources\n",
    "\n",
    "### üìä **Next Steps**\n",
    "\n",
    "- Experiment with different question types\n",
    "- Analyze the conversation history\n",
    "- Optimize chunk sizes for your data\n",
    "- Add more sophisticated retrieval strategies\n",
    "- Implement feedback loops for continuous improvement\n",
    "\n",
    "**Your RAG system is ready to answer questions about your CSV database!** üéä"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
