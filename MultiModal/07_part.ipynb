{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f05cd032",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## Embeddings\n",
    "\n",
    "An **embedding** is a vector representation of data.\n",
    "\n",
    "For text:\n",
    "\n",
    "```\n",
    "The cat ‚Üí [0.23, -0.11, 0.89, ...]\n",
    "```\n",
    "\n",
    "For images:\n",
    "\n",
    "```\n",
    "Image pixels ‚Üí meaningful feature vector\n",
    "```\n",
    "\n",
    "For videos:\n",
    "\n",
    "```\n",
    "Sequence of frames ‚Üí sequence-level embedding\n",
    "```\n",
    "\n",
    "Embeddings capture:\n",
    "\n",
    "* meaning\n",
    "* semantics\n",
    "* relationships\n",
    "* similarity\n",
    "* context\n",
    "\n",
    "They allow computers to **understand high-dimensional raw data** using **dense numerical vectors**.\n",
    "\n",
    "---\n",
    "\n",
    "### Image Embeddings**\n",
    "\n",
    "### üìå What are Image Embeddings?\n",
    "\n",
    "Image embeddings are **high-level feature vectors** extracted from images using a pretrained model (CNN, ViT, CLIP).\n",
    "\n",
    "Instead of raw pixels (millions of numbers), embeddings compress the visual meaning into a vector such as:\n",
    "\n",
    "```\n",
    "Shape: (1, 512) or (1, 1024) or (1, 4096)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Why Image Embeddings?\n",
    "\n",
    "Because raw pixels are not meaningful to neural networks.\n",
    "\n",
    "Image embeddings allow:\n",
    "\n",
    "* Image search\n",
    "* Similarity detection\n",
    "* Image classification\n",
    "* Captioning\n",
    "* Multi-modal tasks (vision+text)\n",
    "* Clustering of images\n",
    "* Face recognition\n",
    "* Feature extraction in LLMs like CLIP/LLaVA\n",
    "\n",
    "---\n",
    "\n",
    "### How Image Embeddings Are Computed\n",
    "\n",
    "Use any pretrained **vision encoder**:\n",
    "\n",
    "1. **CNN (ResNet, VGG)** ‚Äî old method\n",
    "2. **Vision Transformer (ViT)** ‚Äî modern\n",
    "3. **CLIP ViT** ‚Äî best for multimodal\n",
    "4. **ConvNeXt** ‚Äî new CNN\n",
    "5. **EfficientNet** ‚Äî optimized CNN\n",
    "\n",
    "### Pipeline:\n",
    "\n",
    "```\n",
    "Image ‚Üí Preprocess (resize/normalize)\n",
    "      ‚Üí Vision model\n",
    "      ‚Üí Extract feature vector (embedding)\n",
    "```\n",
    "\n",
    "Example shape:\n",
    "\n",
    "```\n",
    "(768,) for ViT-B/16\n",
    "(512,) for CLIP RN50\n",
    "(1024,) for ConvNeXt-large\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Image Embeddings with CLIP (Python Demo)\n",
    "\n",
    "Install:\n",
    "\n",
    "```bash\n",
    "pip install transformers pillow torch\n",
    "```\n",
    "\n",
    "Code:\n",
    "\n",
    "```python\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "image = Image.open(\"dog.jpg\")\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    img_emb = model.get_image_features(**inputs)\n",
    "\n",
    "print(img_emb.shape)   # (1, 512)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Intuition: What Does the Embedding Meaningfully Represent?\n",
    "\n",
    "For an image of a **dog**:\n",
    "\n",
    "The embedding captures attributes like:\n",
    "\n",
    "* it‚Äôs an animal\n",
    "* has fur\n",
    "* looks similar to ‚Äúdog‚Äù images\n",
    "* is not a car, human, or tree\n",
    "\n",
    "Thus, embeddings cluster similar images together.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Video Embeddings**\n",
    "\n",
    "A video is not a single image; it is a sequence of frames **over time**.\n",
    "Therefore, video embeddings must capture:\n",
    "\n",
    "* **Spatial features** (what is in the frame)\n",
    "* **Temporal features** (how things move)\n",
    "* **Sequence dynamics** (actions/events)\n",
    "\n",
    "---\n",
    "\n",
    "# ‚≠ê How Video Embeddings Are Computed\n",
    "\n",
    "There are 3 main strategies:\n",
    "\n",
    "---\n",
    "\n",
    "### **Strategy 1: Frame-Level Embeddings + Pooling**\n",
    "\n",
    "1. Extract CLIP/VIT embeddings from each frame\n",
    "2. Average (mean pool) them\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "Video with 10 frames ‚Üí 10 image embeddings (512-dim each)\n",
    "Final video embedding = average of embeddings\n",
    "```\n",
    "\n",
    "Used in:\n",
    "\n",
    "* Quick video search\n",
    "* Fast retrieval\n",
    "* Lightweight systems\n",
    "\n",
    "---\n",
    "\n",
    "### **Strategy 2: 3D CNN Models (old but fast)**\n",
    "\n",
    "Models:\n",
    "\n",
    "* C3D\n",
    "* I3D (Google)\n",
    "* R(2+1)D\n",
    "\n",
    "These operate directly on **(T √ó H √ó W)** tensors.\n",
    "\n",
    "They learn motion patterns:\n",
    "\n",
    "* walking\n",
    "* jumping\n",
    "* running\n",
    "\n",
    "---\n",
    "\n",
    "### **Strategy 3: Video Transformers (state-of-the-art)**\n",
    "\n",
    "Models:\n",
    "\n",
    "* **ViViT**\n",
    "* **TimeSformer**\n",
    "* **VideoMAE**\n",
    "* **XCLIP**\n",
    "* **LLaVA-Video / GPT-4V video models**\n",
    "\n",
    "These operate on:\n",
    "\n",
    "* frame patches\n",
    "* with temporal and spatial attention\n",
    "\n",
    "Produce embeddings like:\n",
    "\n",
    "```\n",
    "(1024,) or (2048,)\n",
    "```\n",
    "\n",
    "Best modern approach for:\n",
    "\n",
    "* action recognition\n",
    "* video understanding\n",
    "* VLMs\n",
    "* summarization\n",
    "\n",
    "---\n",
    "\n",
    "### Video Embedding Example (Frames + CLIP)\n",
    "\n",
    "```bash\n",
    "pip install opencv-python transformers torch\n",
    "```\n",
    "\n",
    "```python\n",
    "import cv2\n",
    "import torch\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "cap = cv2.VideoCapture(\"video.mp4\")\n",
    "embeddings = []\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        emb = model.get_image_features(**inputs)\n",
    "    embeddings.append(emb)\n",
    "\n",
    "# Convert list to tensor and average\n",
    "video_embedding = torch.mean(torch.stack(embeddings), dim=0)\n",
    "\n",
    "print(video_embedding.shape)  # (1, 512)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Why Video Embeddings Are Harder Than Image Embeddings\n",
    "\n",
    "Videos require:\n",
    "\n",
    "* long temporal context\n",
    "* understanding motion\n",
    "* memory across frames\n",
    "* action recognition\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "Frame 1: man holding ball  \n",
    "Frame 2: man moves arm  \n",
    "Frame 3: ball leaves hand  \n",
    "```\n",
    "\n",
    "Action = ‚Äúthrowing a ball‚Äù.\n",
    "\n",
    "An image alone can‚Äôt capture that.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary Table**\n",
    "\n",
    "| Feature    | Image Embeddings            | Video Embeddings                  |\n",
    "| ---------- | --------------------------- | --------------------------------- |\n",
    "| Input      | Single image                | Multiple frames over time         |\n",
    "| Model      | CNN / ViT / CLIP            | Video Transformer / 3D CNN        |\n",
    "| Captures   | Objects / scenes            | Motion + actions                  |\n",
    "| Used for   | Search, VLM, classification | Action recognition, summarization |\n",
    "| Difficulty | Easy                        | Hard                              |\n",
    "\n",
    "---\n",
    "\n",
    "**Final One-Sentence Summary**\n",
    "\n",
    "**Image embeddings capture visual features from a single image, while video embeddings capture both visual features and temporal motion dynamics across multiple frames, enabling deep understanding of scenes, actions, and events.**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
