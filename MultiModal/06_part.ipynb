{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eb563dd",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "\n",
    "## Audio Spectrogram\n",
    "\n",
    "A **spectrogram** is a **visual representation of sound** that shows:\n",
    "\n",
    "* **Time** → horizontal axis\n",
    "* **Frequency** → vertical axis\n",
    "* **Energy/Intensity** → color/brightness\n",
    "\n",
    "In simple terms:\n",
    "\n",
    "> A spectrogram shows **which frequencies exist** in the audio and **how their strength changes over time**.\n",
    "\n",
    "It converts a **1D audio waveform** → into a **2D image-like representation**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Do We Need Spectrograms?**\n",
    "\n",
    "Raw audio waveform looks like this:\n",
    "\n",
    "```\n",
    "Amplitude vs time\n",
    "```\n",
    "\n",
    "But **waveforms do not show frequency**, and speech/music is defined by frequency patterns.\n",
    "\n",
    "Neural networks (CNNs, Transformers, Conformers) understand audio far better in spectrogram form.\n",
    "\n",
    "Spectrograms:\n",
    "\n",
    "* reveal phonemes\n",
    "* capture pitch\n",
    "* show harmonics\n",
    "* detect noise\n",
    "* encode timbre\n",
    "\n",
    "Thus, **almost all modern AI audio tasks use spectrograms**, including:\n",
    "\n",
    "* Speech-to-Text (Whisper)\n",
    "* Text-to-Speech (VITS, FastSpeech)\n",
    "* Music analysis\n",
    "* Audio classification\n",
    "* Voice cloning\n",
    "\n",
    "---\n",
    "\n",
    "### **How Spectrograms Are Created**\n",
    "\n",
    "A spectrogram is computed using **STFT (Short-Time Fourier Transform)**.\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "#### **1. Break audio into small windows (frames)**\n",
    "\n",
    "Example:\n",
    "\n",
    "* window size = 25 ms\n",
    "* hop size = 10 ms\n",
    "\n",
    "#### **2. Apply FFT (Fast Fourier Transform)**\n",
    "\n",
    "This converts each window to frequency domain.\n",
    "\n",
    "#### **3. Stack all windows over time**\n",
    "\n",
    "You get a 2D matrix:\n",
    "\n",
    "```\n",
    "time frames × frequency bins\n",
    "```\n",
    "\n",
    "#### **4. Apply log scale or Mel scale**\n",
    "\n",
    "To match human perception.\n",
    "\n",
    "---\n",
    "\n",
    "### ⭐ Types of Spectrograms\n",
    "\n",
    "#### **1. Linear Spectrogram**\n",
    "\n",
    "Computed directly from FFT.\n",
    "\n",
    "* Accurate\n",
    "* Used in audio engineering\n",
    "\n",
    "But not human-like.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Log Spectrogram**\n",
    "\n",
    "Applies log() on frequency magnitudes.\n",
    "\n",
    "* reduces dynamic range\n",
    "* easier for models to learn\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Mel Spectrogram (Most Used in AI)**\n",
    "\n",
    "Transforms frequencies to **Mel scale**, which matches human hearing:\n",
    "\n",
    "Humans hear lower frequencies with higher sensitivity.\n",
    "\n",
    "Thus:\n",
    "\n",
    "* 100–500 Hz → high resolution\n",
    "* > 4000 Hz → low resolution\n",
    "\n",
    "#### Used in:\n",
    "\n",
    "* Whisper\n",
    "* Tacotron2\n",
    "* VITS\n",
    "* FastSpeech2\n",
    "* SpeechT5\n",
    "* Music generation models\n",
    "\n",
    "---\n",
    "\n",
    "### **Mel Spectrogram Formula**\n",
    "\n",
    "1. Apply STFT\n",
    "2. Apply Mel filter banks\n",
    "3. Apply log transform\n",
    "\n",
    "$$\n",
    "M = \\log(\\text{MelFilterBank} \\times |\\text{STFT}(x)|^2)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Visual Example\n",
    "\n",
    "#### Waveform → Spectrogram\n",
    "\n",
    "**Waveform**\n",
    "\n",
    "```\n",
    "Fast oscillating line, not visually meaningful for patterns.\n",
    "```\n",
    "\n",
    "**Spectrogram**\n",
    "\n",
    "```\n",
    "Frequencies on Y  \n",
    "Time on X  \n",
    "Colors = energy\n",
    "```\n",
    "\n",
    "Patterns become visible:\n",
    "\n",
    "* vowels (steady harmonic bands)\n",
    "* consonants (noise bursts)\n",
    "* silence (dark area)\n",
    "\n",
    "---\n",
    "\n",
    "### Why Spectrograms Are Like Images for AI\n",
    "\n",
    "Spectrogram is a **2D grid**, so:\n",
    "\n",
    "* CNNs detect patterns\n",
    "* Transformers process patches\n",
    "* Vision models can understand audio\n",
    "\n",
    "This is why:\n",
    "\n",
    "* Whisper uses audio patches like a Vision Transformer\n",
    "* TTS models generate spectrograms then convert to waveform\n",
    "* AudioLM / GPT-4o models learn audio tokens derived from spectrograms\n",
    "\n",
    "---\n",
    "\n",
    "### PyTorch Demo — Create a Mel Spectrogram\n",
    "\n",
    "Install:\n",
    "\n",
    "```bash\n",
    "pip install torchaudio\n",
    "```\n",
    "\n",
    "### Code:\n",
    "\n",
    "```python\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import torch\n",
    "\n",
    "# Load audio\n",
    "waveform, sr = torchaudio.load(\"audio.wav\")\n",
    "\n",
    "# Create Mel Spectrogram Transformer\n",
    "mel_spectrogram = T.MelSpectrogram(\n",
    "    sample_rate=sr,\n",
    "    n_fft=1024,\n",
    "    hop_length=256,\n",
    "    n_mels=80\n",
    ")\n",
    "\n",
    "mel = mel_spectrogram(waveform)  # Shape: [channels, n_mels, time]\n",
    "\n",
    "print(\"Mel spectrogram shape:\", mel.shape)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Interpreting Values\n",
    "\n",
    "Mel spectrogram gives:\n",
    "\n",
    "* rows = Mel frequency bins (80 commonly used)\n",
    "* columns = time steps (depends on hop size)\n",
    "* pixel values = loudness at that frequency and time\n",
    "\n",
    "This is exactly what models use internally.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**\n",
    "\n",
    "| Concept     | Meaning                                      |\n",
    "| ----------- | -------------------------------------------- |\n",
    "| Spectrogram | Time–frequency representation of audio       |\n",
    "| Why         | Makes speech/music patterns visible for AI   |\n",
    "| How         | STFT → frequency bins → mel/log filters      |\n",
    "| Used in     | STT, TTS, music AI, sound classification     |\n",
    "| Models      | Whisper, VITS, FastSpeech, Wav2Vec, SpeechT5 |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
