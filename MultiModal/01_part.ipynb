{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f5ff87b",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## Vision Transformers (ViT)\n",
    "\n",
    "A **Vision Transformer (ViT)** is a neural network that applies **Transformer architecture**—originally designed for NLP—to **image understanding**.\n",
    "\n",
    "Instead of using convolutional layers like CNNs, ViTs process an image as a **sequence of patches**, similar to a sequence of words in a sentence.\n",
    "\n",
    "---\n",
    "\n",
    "###  WHY ViTs WERE INVENTED\n",
    "\n",
    "CNNs (ResNet, VGG, MobileNet) dominated vision because they:\n",
    "\n",
    "* detect local patterns (edges, textures)\n",
    "* use convolutions with fixed receptive fields\n",
    "\n",
    "But CNNs struggle with:\n",
    "\n",
    "* capturing **long-range dependencies**\n",
    "* global context\n",
    "* scalability\n",
    "\n",
    "Transformers naturally capture **global relationships** with **self-attention**.\n",
    "\n",
    "So researchers asked:\n",
    "**What if we treat an image like a text sequence?**\n",
    "\n",
    "This became the Vision Transformer.\n",
    "\n",
    "---\n",
    "\n",
    "### HOW VISION TRANSFORMERS WORK (Step-by-Step)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Split the Image into Patches**\n",
    "\n",
    "Example:\n",
    "Image size: **224 × 224**\n",
    "Patch size: **16 × 16**\n",
    "\n",
    "→ Total patches = (224/16)² = **196 patches**\n",
    "Each patch = flattened into a vector.\n",
    "\n",
    "Think of patches as \"visual tokens.\"\n",
    "\n",
    "---\n",
    "\n",
    "#### **Convert Each Patch to an Embedding (Linear Layer)**\n",
    "\n",
    "For each patch:\n",
    "\n",
    "1. Flatten: 16×16×3 → 768 numbers\n",
    "2. Linear projection:\n",
    "\n",
    "   ```\n",
    "   patch_embedding = W * flattened_patch\n",
    "   ```\n",
    "\n",
    "Now each patch becomes a vector in a high-dimensional space (like word embeddings).\n",
    "\n",
    "---\n",
    "\n",
    "#### **Add Positional Embeddings**\n",
    "\n",
    "Transformers don’t know the order of tokens.\n",
    "So we add **positional embeddings** to encode patch location:\n",
    "\n",
    "```\n",
    "patch_embedding + position_embedding\n",
    "```\n",
    "\n",
    "This tells the model:\n",
    "\n",
    "* where each patch came from\n",
    "* how patches relate spatially\n",
    "\n",
    "---\n",
    "\n",
    "#### **Feed Patch Embeddings into Transformer Encoder**\n",
    "\n",
    "Exactly like BERT:\n",
    "\n",
    "* Multi-head Self-Attention\n",
    "* LayerNorm\n",
    "* Feed Forward Network\n",
    "* Residual Connections\n",
    "\n",
    "Self-attention allows each patch to \"look at\" every other patch:\n",
    "\n",
    "> “Which other parts of the image matter to understanding this patch?”\n",
    "\n",
    "This gives ViTs **global receptive fields** from the beginning.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Use CLS Token for Classification (like BERT)**\n",
    "\n",
    "A special learnable vector **[CLS]** is prepended to all patches.\n",
    "\n",
    "After processing, the corresponding output embedding encodes the entire image.\n",
    "\n",
    "Finally, pass CLS output → MLP → label prediction.\n",
    "\n",
    "---\n",
    "\n",
    "### VIOLENTLY SIMPLE ARCHITECTURE\n",
    "\n",
    "A Vision Transformer has:\n",
    "\n",
    "```\n",
    "Patch Embedding\n",
    "+ Transformer Encoder Layers\n",
    "+ MLP Classifier\n",
    "```\n",
    "\n",
    "No convolutions\n",
    "No pooling\n",
    "No feature maps\n",
    "\n",
    "---\n",
    "\n",
    "### WHY VISION TRANSFORMERS WORK WELL\n",
    "\n",
    "#### ✔ 1. **Global Context From the Start**\n",
    "\n",
    "Self-attention lets ViTs focus on:\n",
    "\n",
    "* long-range dependencies\n",
    "* relationships between objects\n",
    "* global structure\n",
    "\n",
    "CNNs only gain wide context in deeper layers.\n",
    "\n",
    "---\n",
    "\n",
    "#### ✔ 2. **Scales Extremely Well**\n",
    "\n",
    "The more data you give ViTs, the better they get.\n",
    "With large datasets (ImageNet-21k, JFT-300M), ViTs **outperform CNNs**.\n",
    "\n",
    "---\n",
    "\n",
    "#### ✔ 3. **Uniform Architecture**\n",
    "\n",
    "Same Transformer blocks for:\n",
    "\n",
    "* images\n",
    "* text\n",
    "* multimodal models (CLIP, Flamingo, Gemini)\n",
    "\n",
    "This makes ViTs ideal for **multimodal AI**.\n",
    "\n",
    "---\n",
    "\n",
    "**LIMITATIONS OF ViTs**\n",
    "\n",
    "| Issue                                       | Why                      |\n",
    "| ------------------------------------------- | ------------------------ |\n",
    "| **Needs lots of data**                      | Lacks CNN inductive bias |\n",
    "| **Patch-level processing may miss details** | Especially small objects |\n",
    "| **High compute cost**                       | Attention is O(N²)       |\n",
    "\n",
    "Hybrid models (ConvNeXt, Swin Transformer) fix these.\n",
    "\n",
    "---\n",
    "\n",
    "**TYPES OF ViTs**\n",
    "\n",
    "| Model                    | Idea                                  |\n",
    "| ------------------------ | ------------------------------------- |\n",
    "| **ViT**                  | Basic transformer for images          |\n",
    "| **DeiT**                 | Data-efficient ViT (less data needed) |\n",
    "| **Swin Transformer**     | Hierarchical ViT with local windows   |\n",
    "| **ViT-Huge / ViT-Giant** | Massive models for SOTA               |\n",
    "\n",
    "---\n",
    "\n",
    "**A SIMPLE INTUITION**\n",
    "\n",
    "> ViT slices an image into small squares, treats each square like a word, and uses attention to understand how all squares relate to each other.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32f9089b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 1. PATCH EMBEDDING\n",
    "# ----------------------------------------------------\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "        # Create patch embedding using a Conv2d layer\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels,\n",
    "            embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size  # non-overlapping patches\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (B, 3, H, W)\n",
    "        x = self.proj(x)         # (B, embed_dim, H/patch, W/patch)\n",
    "        x = x.flatten(2)         # flatten: (B, embed_dim, N_patches)\n",
    "        x = x.transpose(1, 2)    # (B, N_patches, embed_dim)\n",
    "        return x\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 2. MULTI-HEAD SELF ATTENTION\n",
    "# ----------------------------------------------------\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim=768, num_heads=12):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Self-attention: Q = K = V = x\n",
    "        attn_output, _ = self.mha(x, x, x)\n",
    "        return attn_output\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 3. TRANSFORMER ENCODER BLOCK\n",
    "# ----------------------------------------------------\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim=768, num_heads=12, mlp_ratio=4.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        \n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        hidden_dim = int(embed_dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Attention with residual\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        \n",
    "        # MLP with residual\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 4. COMPLETE VISION TRANSFORMER (ViT)\n",
    "# ----------------------------------------------------\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=224,\n",
    "        patch_size=16,\n",
    "        in_channels=3,\n",
    "        num_classes=10,\n",
    "        embed_dim=768,\n",
    "        depth=6,\n",
    "        num_heads=12,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Patch Embedding\n",
    "        self.patch_embed = PatchEmbedding(\n",
    "            img_size, patch_size, in_channels, embed_dim\n",
    "        )\n",
    "\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        # CLS token (learnable)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "\n",
    "        # Positional Embedding (learnable)\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.zeros(1, num_patches + 1, embed_dim)\n",
    "        )\n",
    "\n",
    "        # Transformer Encoder Layers\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerEncoderBlock(embed_dim, num_heads)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        # Final classification head\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "\n",
    "        x = self.patch_embed(x)  # (B, N_patches, embed_dim)\n",
    "\n",
    "        # Add CLS token at position 0\n",
    "        cls = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls, x), dim=1)\n",
    "\n",
    "        # Add positional embedding\n",
    "        x = x + self.pos_embed\n",
    "\n",
    "        # Pass through transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # Use CLS token output for classification\n",
    "        cls_output = self.norm(x[:, 0])\n",
    "        return self.fc(cls_output)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 5. TEST THE MODEL\n",
    "# ----------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    model = VisionTransformer()\n",
    "    img = torch.randn(2, 3, 224, 224)  # batch of 2 images\n",
    "    out = model(img)\n",
    "\n",
    "    print(\"Output shape:\", out.shape)  # (2, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab59f57a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
