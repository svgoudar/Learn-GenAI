{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91d0b783",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## OCR\n",
    "\n",
    "OCR (**Optical Character Recognition**) is the process of converting **images of text** into **machine-readable digital text**.\n",
    "\n",
    "Example:\n",
    "\n",
    "| Input               | Output                  |\n",
    "| ------------------- | ----------------------- |\n",
    "| Photo of a receipt  | Structured digital text |\n",
    "| Scanned PDF         | Searchable text         |\n",
    "| Handwriting picture | Recognized characters   |\n",
    "\n",
    "OCR is used in:\n",
    "\n",
    "* Scanned documents\n",
    "* Invoices/receipts processing\n",
    "* Passport/ID scanning\n",
    "* License plate recognition (ANPR)\n",
    "* Bank cheques\n",
    "* Digitizing printed books\n",
    "* Multimodal LLMs (GPT-4o, Gemini)\n",
    "\n",
    "---\n",
    "\n",
    "### OCR Pipeline (Step-by-Step)\n",
    "\n",
    "Modern OCR systems have **four main components**:\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Image Preprocessing**\n",
    "\n",
    "Goal: Improve image quality for recognition.\n",
    "\n",
    "Techniques:\n",
    "\n",
    "* Grayscale conversion\n",
    "* Binarization (Otsu thresholding)\n",
    "* Noise removal\n",
    "* Deskewing (fix tilted scans)\n",
    "* Contrast enhancement\n",
    "* Resizing\n",
    "* Smoothing/sharpening\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "Original → cleaned → easier for model to detect text\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Text Detection (Where is the text?)**\n",
    "\n",
    "The system finds **regions containing text**.\n",
    "\n",
    "Modern text detectors:\n",
    "\n",
    "* **EAST** (Efficient and Accurate Scene Text Detector)\n",
    "* **CRAFT** (Character Region Awareness)\n",
    "* **DB (Differentiable Binarization)** – SOTA\n",
    "* **YOLO-based text detectors**\n",
    "* **MMOCR (OpenMMLab)**\n",
    "* **TrOCR Vision Encoder**\n",
    "\n",
    "Output:\n",
    "\n",
    "* bounding boxes (quadrilateral or rotated)\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "[ x1, y1, x2, y2, x3, y3, x4, y4 ]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Text Recognition (What does the text say?)**\n",
    "\n",
    "Once text regions are extracted, they are fed into a **recognition model**.\n",
    "\n",
    "Two classes:\n",
    "\n",
    "---\n",
    "\n",
    "#### **A. Traditional OCR (Old, rule-based)**\n",
    "\n",
    "* Tesseract (Google)\n",
    "* Works on printed text\n",
    "* Uses:\n",
    "\n",
    "  * segmentation\n",
    "  * template matching\n",
    "  * HMMs (Hidden Markov Models)\n",
    "\n",
    "Limitations:\n",
    "\n",
    "* fails with blurry/angled text\n",
    "* poor on handwriting\n",
    "* not good for complex fonts\n",
    "* not robust for real-world images\n",
    "\n",
    "---\n",
    "\n",
    "#### **B. Deep Learning OCR (Modern SOTA)**\n",
    "\n",
    "##### Architectures:\n",
    "\n",
    "1. **CRNN (Convolutional Recurrent Neural Network)**\n",
    "   CNN → RNN → CTC decoder\n",
    "   (Used for scene text)\n",
    "\n",
    "2. **Transformer-based OCR (State of the Art)**\n",
    "\n",
    "   * **TrOCR (Microsoft)**\n",
    "   * Donut (OCR-free document understanding)\n",
    "   * LayoutLMv3 (document transformer)\n",
    "\n",
    "3. **Vision Encoders + Sequence Decoders**\n",
    "   Encoder: ViT or CNN\n",
    "   Decoder: Transformer autoregressive\n",
    "\n",
    "These models handle:\n",
    "\n",
    "* handwriting\n",
    "* noisy images\n",
    "* rotated text\n",
    "* multilingual content\n",
    "* natural scenes\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Post-Processing**\n",
    "\n",
    "Improves text accuracy:\n",
    "\n",
    "* Spell correction\n",
    "* Dictionary lookup\n",
    "* Language modeling\n",
    "* Layout reconstruction (paragraphs, tables)\n",
    "* Regex extraction (emails, IDs, prices)\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "\"Ths is an exmple\" → \"This is an example\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Modern OCR Systems (What Real Organizations Use)\n",
    "\n",
    "| Tool                  | Type                       | Notes                        |\n",
    "| --------------------- | -------------------------- | ---------------------------- |\n",
    "| **Tesseract**         | classical                  | free, best for clean scans   |\n",
    "| **Google Vision API** | deep learning              | robust, cloud                |\n",
    "| **AWS Textract**      | deep learning              | forms, receipts, tables      |\n",
    "| **Azure Read**        | SOTA OCR                   | handles handwriting          |\n",
    "| **PaddleOCR**         | open-source, very accurate | supports 80+ languages       |\n",
    "| **MMOCR**             | deep learning framework    | academic & industrial use    |\n",
    "| **TrOCR**             | transformer OCR            | state of the art open-source |\n",
    "\n",
    "---\n",
    "\n",
    "### Example: OCR Using Tesseract (Python)\n",
    "\n",
    "### Install\n",
    "\n",
    "```bash\n",
    "pip install pytesseract pillow\n",
    "```\n",
    "\n",
    "### Code\n",
    "\n",
    "```python\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "img = Image.open(\"text_image.jpg\")\n",
    "text = pytesseract.image_to_string(img)\n",
    "\n",
    "print(text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Example: OCR Using Transformers (TrOCR – SOTA)\n",
    "\n",
    "#### Install:\n",
    "\n",
    "```bash\n",
    "pip install transformers pillow torch\n",
    "```\n",
    "\n",
    "#### Code:\n",
    "\n",
    "```python\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "\n",
    "image = Image.open(\"handwriting.jpg\").convert(\"RGB\")\n",
    "\n",
    "inputs = processor(image, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    generated = model.generate(**inputs)\n",
    "\n",
    "text = processor.batch_decode(generated, skip_special_tokens=True)[0]\n",
    "print(text)\n",
    "```\n",
    "\n",
    "This handles **handwriting**, **blur**, **complex scenes**, etc.\n",
    "\n",
    "---\n",
    "\n",
    "#### How OCR Models Handle Real-World Challenges\n",
    "\n",
    "OCR must handle:\n",
    "\n",
    "* rotated text → rotated boxes\n",
    "* curved text → segmentation\n",
    "* low contrast → image enhancement\n",
    "* handwriting → transformer decoders\n",
    "* multi-language → multilingual vocab\n",
    "* scanned PDFs → layout modeling\n",
    "* natural scenes → CNN + ViT encoders\n",
    "\n",
    "Modern approaches solve these using:\n",
    "\n",
    "* attention\n",
    "* self-supervision\n",
    "* synthetic data generation\n",
    "* convolution + transformer hybrids\n",
    "\n",
    "---\n",
    "\n",
    "**Final Summary**\n",
    "\n",
    "**OCR systems convert images of text into digital text using a pipeline of:**\n",
    "\n",
    "1. **Preprocessing**\n",
    "2. **Text Detection** (EAST, CRAFT, DB)\n",
    "3. **Text Recognition** (CRNN, Transformers, TrOCR)\n",
    "4. **Post-processing** (spell correction, layout)\n",
    "\n",
    "Modern OCR uses **deep learning** and can read:\n",
    "\n",
    "* handwriting\n",
    "* curved text\n",
    "* noisy photos\n",
    "* multi-language content\n",
    "* real-world images (street signs, menus, documents)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
