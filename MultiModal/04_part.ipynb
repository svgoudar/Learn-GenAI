{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce647377",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## Speech-to-Text (STT)\n",
    "\n",
    "Speech-to-Text (also called **Automatic Speech Recognition**, ASR) converts **audio (speech)** into **written text**.\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "Audio: \"Hello, how are you?\"\n",
    "↓\n",
    "Text: \"Hello, how are you?\"\n",
    "```\n",
    "\n",
    "STT systems are used in:\n",
    "\n",
    "* Voice assistants (Siri, Alexa, Google Assistant)\n",
    "* Call-center analytics\n",
    "* Transcription tools\n",
    "* Meeting summarizers\n",
    "* Voice-controlled devices\n",
    "* Multimodal LLMs (GPT-4o, Gemini, Whisper, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "### Why Speech-to-Text Is Challenging\n",
    "\n",
    "Speech is complex:\n",
    "\n",
    "* Noisy environments\n",
    "* Different accents\n",
    "* Fast speech\n",
    "* Co-articulation (words blend together)\n",
    "* Background sounds\n",
    "* Microphone quality\n",
    "\n",
    "An STT model must:\n",
    "\n",
    "* Understand acoustics\n",
    "* Decode phonemes\n",
    "* Recognize words\n",
    "* Fix grammar\n",
    "* Handle ambiguity\n",
    "\n",
    "---\n",
    "\n",
    "### How Speech-to-Text Works (Processing Pipeline)\n",
    "\n",
    "#### **1️⃣ Audio Input (Waveform)**\n",
    "\n",
    "Raw audio waveform:\n",
    "\n",
    "```\n",
    "16 kHz sampling → 16000 values per second\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **2️⃣ Feature Extraction (Mel Spectrograms)**\n",
    "\n",
    "Audio is converted into **spectrograms**, similar to images.\n",
    "\n",
    "Most STT models use:\n",
    "\n",
    "* **Mel Spectrograms**\n",
    "* Log-mel features\n",
    "\n",
    "Why?\n",
    "\n",
    "* Speech patterns become clearer\n",
    "* Easier for neural networks to learn\n",
    "\n",
    "---\n",
    "\n",
    "#### **3️⃣ Acoustic Modeling**\n",
    "\n",
    "Neural networks convert sound patterns into **phoneme probabilities** or **token embeddings**.\n",
    "\n",
    "Modern models use:\n",
    "\n",
    "* CNNs\n",
    "* RNNs / LSTMs\n",
    "* Transformers\n",
    "* Conformer blocks\n",
    "* Whisper architecture\n",
    "\n",
    "---\n",
    "\n",
    "#### **4️⃣ Sequence Modeling**\n",
    "\n",
    "Speech is time-dependent; models must learn:\n",
    "\n",
    "* sequence order\n",
    "* pronunciation patterns\n",
    "* long-range dependencies\n",
    "\n",
    "Used architectures:\n",
    "\n",
    "* RNNs (old)\n",
    "* CTC models\n",
    "* Encoder–Decoder Transformers\n",
    "* Conformers (state of the art)\n",
    "\n",
    "---\n",
    "\n",
    "#### **5️⃣ Decoding (Language Modeling)**\n",
    "\n",
    "Converts acoustic tokens → text.\n",
    "\n",
    "Decoding strategies:\n",
    "\n",
    "* Greedy decoding\n",
    "* Beam search\n",
    "* CTC beam search\n",
    "* Token prediction (autoregressive like Whisper)\n",
    "\n",
    "Language model helps fix:\n",
    "\n",
    "* spelling\n",
    "* grammar\n",
    "* missing words\n",
    "\n",
    "---\n",
    "\n",
    "#### **6️⃣ Output Text**\n",
    "\n",
    "Final transcript after post-processing:\n",
    "\n",
    "* punctuation insertion\n",
    "* casing\n",
    "* removing filler words\n",
    "* grammar correction\n",
    "\n",
    "---\n",
    "\n",
    "### Modern Architectures for Speech-to-Text\n",
    "\n",
    "#### CTC-Based Models (classical)\n",
    "\n",
    "CTC = Connectionist Temporal Classification\n",
    "Model outputs characters independently.\n",
    "\n",
    "Pros: fast\n",
    "Cons: lower quality\n",
    "\n",
    "---\n",
    "\n",
    "#### Encoder-Decoder Models (seq2seq)\n",
    "\n",
    "Speech → encoder → decoder predicts text tokens\n",
    "\n",
    "Examples:\n",
    "\n",
    "* LAS (Listen-Attend-Spell)\n",
    "* RNN-T (Recurrent Neural Network Transducer)\n",
    "\n",
    "---\n",
    "\n",
    "#### Conformer Models\n",
    "\n",
    "Combination of:\n",
    "\n",
    "* Convolution (local features)\n",
    "* Transformer (global long-term dependencies)\n",
    "\n",
    "Used in:\n",
    "\n",
    "* Google’s ASR\n",
    "* OpenAI Whisper\n",
    "\n",
    "Conformer = current SOTA for speech.\n",
    "\n",
    "---\n",
    "\n",
    "#### Whisper (OpenAI)\n",
    "\n",
    "Whisper is the most popular open-source STT model:\n",
    "\n",
    "* trained on 680,000 hours of multilingual audio\n",
    "* extremely robust (noise, accents, background sounds)\n",
    "* works offline\n",
    "\n",
    "Pipeline:\n",
    "\n",
    "```\n",
    "Audio → Log-Mel → Encoder → Decoder → Text\n",
    "```\n",
    "\n",
    "Whisper is used as foundation for many current STT products.\n",
    "\n",
    "---\n",
    "\n",
    "### Intuition Behind Speech-to-Text\n",
    "\n",
    "####  Think of speech as a changing picture over time.\n",
    "\n",
    "Each small slice of audio is like a pixel row.\n",
    "\n",
    "Spectrogram → a 2D “image of sound”.\n",
    "\n",
    "Transformers and CNNs can “read” this image to identify:\n",
    "\n",
    "* pitch\n",
    "* phonemes\n",
    "* words\n",
    "\n",
    "---\n",
    "\n",
    "### Example Using Whisper (Python Demo)\n",
    "\n",
    "```python\n",
    "pip install transformers torch librosa soundfile\n",
    "```\n",
    "\n",
    "```python\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import soundfile as sf\n",
    "\n",
    "audio_path = \"sample.wav\"\n",
    "audio, sr = sf.read(audio_path)\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "inputs = processor(audio, sampling_rate=sr, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    predicted = model.generate(**inputs)\n",
    "\n",
    "text = processor.decode(predicted[0])\n",
    "print(text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Why LLM-Based STT Models (Whisper, GPT-4o) Are So Good\n",
    "\n",
    "#### ✔ Trained on huge multilingual datasets\n",
    "\n",
    "#### ✔ Use transformers (excellent at sequence modeling)\n",
    "\n",
    "#### ✔ Language model improves grammar\n",
    "\n",
    "#### ✔ Noise robustness\n",
    "\n",
    "#### ✔ Punctuation and casing included\n",
    "\n",
    "#### ✔ Zero-shot adaptability\n",
    "\n",
    "---\n",
    "\n",
    "**One-Sentence Summary**\n",
    "\n",
    "**Speech-to-Text converts audio into text using feature extraction (spectrograms), acoustic modeling, transformers/conformers, and language decoding to produce accurate transcripts.**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
