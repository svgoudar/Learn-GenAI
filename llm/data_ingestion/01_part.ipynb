{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "680e3054",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "\n",
    "## Batch Ingestion vs. Streaming Ingestion (Tutor Explanation)\n",
    "\n",
    "When building Machine Learning or Generative AI systems—especially Retrieval-Augmented Generation (RAG) pipelines—we need to **move, process, and prepare data** so that models and vector stores stay updated. Two main approaches are used for this: **Batch Ingestion** and **Streaming Ingestion**. They serve different needs depending on whether the system requires **periodic updates** or **real-time responsiveness**.\n",
    "\n",
    "---\n",
    "\n",
    "### Batch Ingestion (Offline / Periodic Processing)\n",
    "\n",
    "Batch ingestion means that data is **collected and processed in large chunks** at scheduled intervals, such as **hourly, daily, or weekly**.\n",
    "\n",
    "#### **How it Works**\n",
    "\n",
    "* Data accumulates over time.\n",
    "* At a scheduled time, a job processes this data in bulk.\n",
    "* The output (features, embeddings, predictions) is **stored** for later use.\n",
    "\n",
    "#### **Learning and Model Behavior**\n",
    "\n",
    "* Models in batch mode are usually **trained on the full dataset at once**.\n",
    "* When new data arrives, the model is typically **retrained from scratch** or retrained on a large historical dataset.\n",
    "* This makes batch systems **simple but slow to adapt** to new information.\n",
    "\n",
    "#### **Batch Predictions (Offline Prediction)**\n",
    "\n",
    "* Predictions are computed **ahead of time**, not when the user requests them.\n",
    "* These predictions are stored (e.g., in a DB or cache) and **reused** later.\n",
    "* This improves efficiency and **reduces compute costs**.\n",
    "\n",
    "#### **Common Use Cases**\n",
    "\n",
    "| Example                             | Why Batch Works                                   |\n",
    "| ----------------------------------- | ------------------------------------------------- |\n",
    "| Nightly recommender system updates  | User preference doesn’t need updates every second |\n",
    "| Analytics dashboards and BI reports | Data is summarized at regular intervals           |\n",
    "| Knowledge base re-indexing for RAG  | Documents are not changing constantly             |\n",
    "\n",
    "---\n",
    "\n",
    "### Streaming Ingestion (Real-Time Processing)\n",
    "\n",
    "Streaming ingestion means data is processed **immediately** as it arrives. The system reacts to events **in real time**.\n",
    "\n",
    "#### **How it Works**\n",
    "\n",
    "* Data flows continuously through systems like **Kafka** or **Kinesis**.\n",
    "* The pipeline processes and updates downstream systems instantly.\n",
    "* Results are available with **very low latency**.\n",
    "\n",
    "#### **Learning and Model Behavior**\n",
    "\n",
    "* Supports **online or incremental learning**:\n",
    "\n",
    "  * The model updates itself using small bits of new data (mini-batches).\n",
    "* Allows the system to **adapt quickly** to new patterns or sudden changes.\n",
    "\n",
    "#### **Online Predictions (Real-Time Inference)**\n",
    "\n",
    "* Predictions are generated **when the user requests them**.\n",
    "* The response must be **fast** — usually under a few milliseconds.\n",
    "\n",
    "#### **Common Use Cases**\n",
    "\n",
    "| Example                                | Why Streaming is Needed          |\n",
    "| -------------------------------------- | -------------------------------- |\n",
    "| Fraud detection (e.g., PayPal, Stripe) | Decisions must be made instantly |\n",
    "| Real-time chatbots and LLM assistants  | Users expect immediate replies   |\n",
    "| TikTok-style content recommendation    | User preferences change quickly  |\n",
    "\n",
    "---\n",
    "\n",
    "#### Comparison Summary\n",
    "\n",
    "| Aspect            | Batch Ingestion                    | Streaming Ingestion                 |\n",
    "| ----------------- | ---------------------------------- | ----------------------------------- |\n",
    "| Processing Timing | Scheduled (periodic)               | Continuous (real-time)              |\n",
    "| Data Freshness    | Data may be slightly old           | Always up-to-date                   |\n",
    "| Model Updating    | Re-training in large chunks        | Small, incremental updates          |\n",
    "| Prediction Mode   | Precomputed and stored             | Generated on demand                 |\n",
    "| System Complexity | Simpler to implement               | More complex and resource-intensive |\n",
    "| Ideal Use Cases   | Offline analytics, nightly updates | Real-time apps, dynamic behavior    |\n",
    "\n",
    "---\n",
    "\n",
    "#### How This Relates to RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "| RAG Component        | Batch Pipeline                       | Streaming Pipeline                     |\n",
    "| -------------------- | ------------------------------------ | -------------------------------------- |\n",
    "| Embedding Documents  | Periodically re-embed entire dataset | Embed new data instantly as it appears |\n",
    "| Vector Store Updates | Scheduled refreshes                  | Continuous updates                     |\n",
    "| Use Case Fit         | Corporate knowledge bases            | Live customer feedback or chat logs    |\n",
    "\n",
    "---\n",
    "\n",
    "**Key Insight**\n",
    "\n",
    "Many companies **start with batch pipelines because they are easier**.\n",
    "As the system grows and the need for freshness increases, they **add or transition to streaming**.\n",
    "\n",
    "\n",
    "### **Demonstration**\n",
    "\n",
    "Below is a **simple, practical demonstration** of both **Batch** and **Streaming** ingestion using **plain Python** so you can see the difference clearly.\n",
    "(No extra infrastructure required to understand the concept.)\n",
    "\n",
    "---\n",
    "\n",
    "#### Batch Ingestion (Periodic Bulk Processing)\n",
    "\n",
    "Assume we have a folder where logs accumulate throughout the day.\n",
    "At **midnight**, we run a batch job to process all logs at once.\n",
    "\n",
    "##### **Example Folder Structure**\n",
    "\n",
    "```\n",
    "/logs/\n",
    "   log_01.csv\n",
    "   log_02.csv\n",
    "   log_03.csv\n",
    "```\n",
    "\n",
    "#### **Batch Ingestion Demo (Python)**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "def batch_ingest_logs():\n",
    "    files = glob.glob(\"logs/*.csv\")  # get all CSV files\n",
    "    batch_data = []\n",
    "\n",
    "    for file in files:\n",
    "        df = pd.read_csv(file)\n",
    "        batch_data.append(df)\n",
    "\n",
    "    combined = pd.concat(batch_data)\n",
    "    combined.to_csv(\"processed/batch_output.csv\", index=False)\n",
    "    print(\"Batch ingestion finished. Combined data saved.\")\n",
    "\n",
    "# This is typically scheduled using Airflow / Cron\n",
    "batch_ingest_logs()\n",
    "```\n",
    "\n",
    "**What happened:**\n",
    "\n",
    "* We waited until enough data accumulated.\n",
    "* Processed everything together.\n",
    "* Stored results for later use.\n",
    "\n",
    "**This is Batch.**\n",
    "\n",
    "---\n",
    "\n",
    "### Streaming Ingestion (Real-Time Processing)\n",
    "\n",
    "Here, data arrives **continuously**, so we process each record **immediately**.\n",
    "\n",
    "We’ll simulate streaming by reading one event at a time.\n",
    "\n",
    "#### Streaming Data Example\n",
    "\n",
    "```\n",
    "incoming_stream = [\n",
    "  {\"user\": \"A\", \"action\": \"login\"},\n",
    "  {\"user\": \"B\", \"action\": \"purchase\"},\n",
    "  {\"user\": \"C\", \"action\": \"logout\"}\n",
    "]\n",
    "```\n",
    "\n",
    "#### Streaming Ingestion Demo (Python)\n",
    "\n",
    "```python\n",
    "import time\n",
    "\n",
    "def process_event(event):\n",
    "    print(f\"Processing event: {event}\")\n",
    "\n",
    "def streaming_ingest(events):\n",
    "    for event in events:\n",
    "        process_event(event)\n",
    "        time.sleep(1)  # simulate real-time arrival\n",
    "\n",
    "incoming_stream = [\n",
    "  {\"user\": \"A\", \"action\": \"login\"},\n",
    "  {\"user\": \"B\", \"action\": \"purchase\"},\n",
    "  {\"user\": \"C\", \"action\": \"logout\"}\n",
    "]\n",
    "\n",
    "streaming_ingest(incoming_stream)\n",
    "```\n",
    "\n",
    "**What happens:**\n",
    "\n",
    "* Each event is processed as soon as it arrives.\n",
    "* No waiting for a full batch.\n",
    "* Output is immediate.\n",
    "\n",
    "**This is Streaming.**\n",
    "\n",
    "---\n",
    "\n",
    "**Key Difference Shown Clearly**\n",
    "\n",
    "| Aspect                 | Batch Example                            | Streaming Example                               |\n",
    "| ---------------------- | ---------------------------------------- | ----------------------------------------------- |\n",
    "| When Data is Processed | After accumulation (e.g., nightly job)   | Immediately as events occur                     |\n",
    "| Code Pattern           | Read many files → process → write output | Read one event → process → repeat               |\n",
    "| Latency                | Higher                                   | Very low                                        |\n",
    "| Use Case               | ETL, offline reporting                   | Chatbots, fraud detection, real-time dashboards |\n",
    "\n",
    "---\n",
    "\n",
    "**How This Maps to RAG (Vector Database Ingestion)**\n",
    "\n",
    "| Step              | Batch RAG                              | Streaming RAG                                |\n",
    "| ----------------- | -------------------------------------- | -------------------------------------------- |\n",
    "| Embedding Docs    | Run nightly to embed all new documents | Embed each document as soon as it is created |\n",
    "| Vector DB updates | Periodic bulk indexing                 | Continuous incremental updates               |\n",
    "| Usage             | Internal knowledge bases               | Live chat, customer support feeds            |\n",
    "\n",
    "\n",
    "\n",
    "### **Batch Pipeline (Airflow + Spark)**\n",
    "\n",
    "```\n",
    "Data Source (CSV / DB / S3)\n",
    "        ↓\n",
    "Airflow Scheduler (runs daily)\n",
    "        ↓\n",
    "Spark Batch Job (ETL / Embeddings / Aggregations)\n",
    "        ↓\n",
    "Data Lake / Warehouse / Vector DB\n",
    "```\n",
    "\n",
    "### **Streaming Pipeline (Kafka + Spark Structured Streaming)**\n",
    "\n",
    "```\n",
    "Producers (apps / microservices / sensors)\n",
    "        ↓\n",
    "Kafka Topic (real-time event buffer)\n",
    "        ↓\n",
    "Spark Structured Streaming Job (transform & write)\n",
    "        ↓\n",
    "Target System (DB / Dashboard / Vector Store)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Batch Ingestion Demo using Airflow + Spark\n",
    "\n",
    "##### **Airflow DAG (scheduled daily batch job)\n",
    "\n",
    "Save as: `dags/batch_ingest_spark.py`\n",
    "\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from datetime import datetime\n",
    "\n",
    "default_args = {\n",
    "    \"start_date\": datetime(2025, 1, 1),\n",
    "    \"retries\": 1\n",
    "}\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"batch_ingestion_spark\",\n",
    "    schedule_interval=\"@daily\",\n",
    "    default_args=default_args,\n",
    "    catchup=False\n",
    "):\n",
    "    run_spark_job = BashOperator(\n",
    "        task_id=\"run_spark_batch_job\",\n",
    "        bash_command=\"spark-submit /opt/jobs/batch_etl.py\"\n",
    "    )\n",
    "\n",
    "    run_spark_job\n",
    "```\n",
    "\n",
    "##### **Spark Batch ETL Job**\n",
    "\n",
    "Save as: `/opt/jobs/batch_etl.py`\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"BatchETL\").getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"/data/raw/*.csv\", header=True, inferSchema=True)\n",
    "\n",
    "df_clean = df.dropna()\n",
    "\n",
    "df_clean.write.mode(\"overwrite\").parquet(\"/data/processed/batch_output/\")\n",
    "\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "**What happens:**\n",
    "\n",
    "* Airflow **triggers Spark job everyday**\n",
    "* Spark reads **all accumulated files**\n",
    "* Cleans and writes output once per schedule\n",
    "  → **This is Batch**\n",
    "\n",
    "---\n",
    "\n",
    "#### Streaming Ingestion Demo using Kafka + Spark Structured Streaming\n",
    "\n",
    "##### Kafka Setup\n",
    "\n",
    "Create Kafka topic:\n",
    "\n",
    "```bash\n",
    "kafka-topics.sh --create --topic user_events --bootstrap-server localhost:9092\n",
    "```\n",
    "\n",
    "##### Streaming Producer (simulating real-time events)\n",
    "\n",
    "```python\n",
    "from kafka import KafkaProducer\n",
    "import json, time\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=\"localhost:9092\",\n",
    "    value_serializer=lambda v: json.dumps(v).encode(\"utf-8\")\n",
    ")\n",
    "\n",
    "events = [\n",
    "    {\"user\": \"A\", \"action\": \"login\"},\n",
    "    {\"user\": \"B\", \"action\": \"purchase\"},\n",
    "    {\"user\": \"C\", \"action\": \"logout\"}\n",
    "]\n",
    "\n",
    "for event in events:\n",
    "    producer.send(\"user_events\", event)\n",
    "    print(\"Sent:\", event)\n",
    "    time.sleep(1)  # simulate real-time arrival\n",
    "```\n",
    "\n",
    "##### Spark Streaming Consumer\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"StreamingETL\").getOrCreate()\n",
    "\n",
    "df = (spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    "    .option(\"subscribe\", \"user_events\")\n",
    "    .load())\n",
    "\n",
    "json_df = df.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "query = (json_df\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"append\")\n",
    "    .start())\n",
    "\n",
    "query.awaitTermination()\n",
    "```\n",
    "\n",
    "What happens:\n",
    "\n",
    "* Events are **pushed to Kafka one-by-one**\n",
    "* Spark Structured Streaming **reads and processes instantly**\n",
    "  → **This is Streaming**\n",
    "\n",
    "---\n",
    "\n",
    "**Comparison in This Demo**\n",
    "\n",
    "| Feature      | Batch (Airflow + Spark) | Streaming (Kafka + Spark Streaming) |\n",
    "| ------------ | ----------------------- | ----------------------------------- |\n",
    "| Data Arrival | Accumulated             | Continuous                          |\n",
    "| Trigger      | Airflow schedule        | Event-driven                        |\n",
    "| Spark Mode   | Batch job               | Structured Streaming                |\n",
    "| Output       | Periodic data refresh   | Real-time updates                   |\n",
    "\n",
    "---\n",
    "\n",
    "**Optional Upgrade: RAG Integration**\n",
    "\n",
    "| Step           | Batch RAG                                             | Streaming RAG                             |\n",
    "| -------------- | ----------------------------------------------------- | ----------------------------------------- |\n",
    "| Embedding Docs | Trigger `spark-submit generate_embeddings.py` nightly | Embed documents as soon as they hit Kafka |\n",
    "| Vector DB      | Bulk upsert into FAISS / Pinecone                     | Incremental upsert                        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4795f1d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
