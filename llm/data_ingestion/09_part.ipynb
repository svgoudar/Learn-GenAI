{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d153109e",
   "metadata": {},
   "source": [
    "\n",
    "```{contents}\n",
    "```\n",
    "\n",
    "## Orchestration and Workflow Scheduling\n",
    "\n",
    "Orchestration ensures that **all ingestion, cleaning, chunking, embedding, validation, indexing, and monitoring steps** run in the correct order with retries, parallelism, error handling, and observability.\n",
    "It is the *backbone* of a production-grade RAG / Generative-AI data platform.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Why Orchestration Is Needed\n",
    "\n",
    "Generative-AI ingestion pipelines are multi-stage:\n",
    "\n",
    "```\n",
    "Extract → Parse → Validate → Clean → Chunk → Dedup → Embed → Store → Monitor\n",
    "```\n",
    "\n",
    "Without orchestration, failures break downstream steps, ingestion stalls, and pipelines cannot scale.\n",
    "\n",
    "Orchestration ensures:\n",
    "\n",
    "* automatic scheduling (hourly, daily, near-real-time)\n",
    "* dependency management\n",
    "* retries & error handling\n",
    "* parallel execution (GPU-heavy steps)\n",
    "* monitoring & alerts\n",
    "* versioned pipelines (schema changes)\n",
    "* scalable ingestion from multiple sources\n",
    "\n",
    "Popular orchestrators:\n",
    "\n",
    "* **Apache Airflow**\n",
    "* **Prefect**\n",
    "* **Dagster**\n",
    "* **Kubeflow Pipelines**\n",
    "* **AWS Step Functions / GCP Cloud Composer / Azure Data Factory**\n",
    "\n",
    "---\n",
    "\n",
    "### 2. What Orchestration Controls in GenAI Ingestion\n",
    "\n",
    "#### **A. Data Extraction Workflows**\n",
    "\n",
    "* Pull from S3, GCS, Azure Blob\n",
    "* Crawl websites\n",
    "* Read PDFs, HTML, text, Docx\n",
    "* Consume Kafka messages\n",
    "* Pull from APIs\n",
    "\n",
    "Scheduler ensures:\n",
    "\n",
    "* periodic runs\n",
    "* backfill of missing days\n",
    "* metadata tracking (version, timestamp)\n",
    "\n",
    "---\n",
    "\n",
    "#### **B. Parsing and Validation Jobs**\n",
    "\n",
    "Orchestrator triggers:\n",
    "\n",
    "* PDF → text\n",
    "* OCR jobs for scanned documents\n",
    "* Running DocumentValidator (schema, safety, quality)\n",
    "\n",
    "If validation fails:\n",
    "\n",
    "* route to **dead-letter queue (DLQ)**\n",
    "* notify via Slack/Email\n",
    "* log error reason\n",
    "\n",
    "---\n",
    "\n",
    "#### **C. Cleaning, Dedup, and Chunking**\n",
    "\n",
    "Often CPU-heavy. Orchestrator:\n",
    "\n",
    "* runs cleaning tasks in parallel\n",
    "* performs exact & near-duplicate checks\n",
    "* standardizes metadata\n",
    "* writes intermediate outputs to a staging zone\n",
    "\n",
    "---\n",
    "\n",
    "#### **D. Embedding Generation Jobs**\n",
    "\n",
    "Embedding creation is the **heaviest** step:\n",
    "\n",
    "* high GPU usage (local embedding models)\n",
    "* API rate limits (OpenAI embeddings)\n",
    "* batch processing required\n",
    "\n",
    "Orchestrator:\n",
    "\n",
    "* handles batching\n",
    "* auto-retries API failures\n",
    "* scales out via KubernetesPodOperator or GPU workers\n",
    "\n",
    "---\n",
    "\n",
    "#### **E. Vector Store & Document Store Loading**\n",
    "\n",
    "Handles:\n",
    "\n",
    "* upsert into Pinecone / Chroma / FAISS\n",
    "* writing chunk metadata\n",
    "* tracking index versions\n",
    "* periodic re-indexing\n",
    "\n",
    "---\n",
    "\n",
    "#### **F. Monitoring & Alerts**\n",
    "\n",
    "Orchestration hooks send:\n",
    "\n",
    "* ingestion success rate\n",
    "* validation failures\n",
    "* embedding latency metrics\n",
    "* vector-store insert failures\n",
    "* quality score distributions\n",
    "\n",
    "Stored in Prometheus + Grafana / ELK / Datadog.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Typical Orchestration Workflow for GenAI Ingestion\n",
    "\n",
    "```\n",
    "                       (Scheduler)\n",
    "                           |\n",
    "                +----------+----------+\n",
    "                |                     |\n",
    "        Extract / Crawl         Kafka Consumer\n",
    "                |                     |\n",
    "                v                     v\n",
    "           Raw Landing Zone (S3/GCS)\n",
    "                |\n",
    "                v\n",
    "        Parsing & Normalization\n",
    "                |\n",
    "                v\n",
    "     Validation & Safety Filtering\n",
    "                |\n",
    "                +--------------+\n",
    "                |              |\n",
    "           Clean Pass      Fail → DLQ\n",
    "                |\n",
    "                v\n",
    "     Noise Removal & Deduplication\n",
    "                |\n",
    "                v\n",
    "            Chunking Stage\n",
    "                |\n",
    "                v\n",
    "      Embedding Generation Workers\n",
    "                |\n",
    "                v\n",
    "     Embedding Validation (dimension, NaN)\n",
    "                |\n",
    "                v\n",
    "          Vector DB Upsert\n",
    "                |\n",
    "                v\n",
    "       Metadata Storage (SQL/NoSQL)\n",
    "                |\n",
    "                v\n",
    "             Monitoring\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Example: Airflow-Based Orchestration (Conceptual)\n",
    "\n",
    "#### DAG Flow\n",
    "\n",
    "```\n",
    "t1_extract_raw_files\n",
    "    → t2_parse_and_clean\n",
    "    → t3_validate_and_filter\n",
    "    → t4_chunk_documents\n",
    "    → t5_generate_embeddings (parallel across GPUs)\n",
    "    → t6_embedding_validation\n",
    "    → t7_load_to_vector_store\n",
    "    → t8_update_catalog\n",
    "    → t9_quality_monitoring\n",
    "```\n",
    "\n",
    "#### Scheduling\n",
    "\n",
    "* Hourly for support articles\n",
    "* Daily for PDF manuals\n",
    "* Real-time streaming for support tickets (Kafka → Airflow API triggers)\n",
    "\n",
    "#### Retry logic\n",
    "\n",
    "* t5 (embedding) → retry 5 times (transient failures)\n",
    "* t2 (parsing) → fallback to OCR route if HTML/PDF parse fails\n",
    "* t7 (load to vector DB) → retry with exponential backoff\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Example Orchestration Features for GenAI Workflows\n",
    "\n",
    "#### **A. Backfill Support**\n",
    "\n",
    "When you add new chunking strategy or new embedding model:\n",
    "\n",
    "* orchestrator automatically replays ingestion for past 7 days / month\n",
    "* writes new vectors under model_version = “embed_v2”\n",
    "\n",
    "#### **B. Parallelism**\n",
    "\n",
    "* Parallel chunk processing per document\n",
    "* Parallel embedding jobs per batch\n",
    "* Parallel vector stores writes\n",
    "\n",
    "#### **C. Versioned Pipelines**\n",
    "\n",
    "* dag_version = 3.1\n",
    "* index_version = 2025-02\n",
    "* embedding_model_version = openai-embed-v3\n",
    "\n",
    "Allows rollback and A/B testing.\n",
    "\n",
    "#### **D. Conditional Branching**\n",
    "\n",
    "Example:\n",
    "\n",
    "* If content_language != “en” → send to translation step\n",
    "* If PDF is scanned → send to OCR pipeline\n",
    "* If document type = “policy” → apply custom chunking rules\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Comparison: Orchestration for RAG vs Fine-tuning\n",
    "\n",
    "| Component         | RAG Ingestion Orchestration                     | LLM Fine-tuning Orchestration            |\n",
    "| ----------------- | ----------------------------------------------- | ---------------------------------------- |\n",
    "| Scheduling        | Frequent (hourly/daily)                         | Less frequent (days/weeks)               |\n",
    "| Steps             | extract → parse → clean → chunk → embed → index | clean → sample → tokenize → pack → train |\n",
    "| State mgmt        | vector indexes & metadata                       | checkpoints, loss curves                 |\n",
    "| Parallelism       | embed-chunk parallelism                         | distributed GPU training                 |\n",
    "| Backfill          | required for re-embedding                       | full retrain or LoRA update              |\n",
    "| Failure tolerance | must be high (always ingesting)                 | offline batch, more forgiving            |\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**\n",
    "\n",
    "Workflow orchestration ensures that **every ingestion step is automated, reliable, parallel, observable, and versioned**.\n",
    "For Generative AI, this is critical because ingestion involves heavy transformations, embeddings, dedup, safety filtering, and continuous updates.\n",
    "\n",
    "If needed, I can also provide:\n",
    "\n",
    "* a complete production Airflow DAG file\n",
    "* a Prefect or Dagster version\n",
    "* a GPU orchestration pattern (KubernetesPodOperator)\n",
    "* a scalable Kafka → Airflow → Pinecone workflow diagram\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98cd036",
   "metadata": {},
   "source": [
    "### Demonstration\n",
    "\n",
    "### 1. Airflow DAG (PDF → Text → Clean → Chunk → Embed → Vector Store)\n",
    "\n",
    "#### File: `rag_ingest_dag.py`\n",
    "\n",
    "```python\n",
    "from datetime import datetime, timedelta\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "# ---------------------------\n",
    "# Task implementations\n",
    "# ---------------------------\n",
    "\n",
    "def extract_documents(**context):\n",
    "    # Example: read from S3 or DB\n",
    "    docs = [\n",
    "        {\n",
    "            \"doc_id\": \"doc_001\",\n",
    "            \"text\": \"Acme manual text...\",\n",
    "            \"metadata\": {\"source\": \"s3://bucket/acme.pdf\"}\n",
    "        }\n",
    "    ]\n",
    "    context[\"ti\"].xcom_push(key=\"raw_docs\", value=docs)\n",
    "\n",
    "\n",
    "def clean_documents(**context):\n",
    "    docs = context[\"ti\"].xcom_pull(key=\"raw_docs\")\n",
    "    cleaned = []\n",
    "    for d in docs:\n",
    "        t = d[\"text\"].replace(\"Page 1 of 10\", \"\").strip()\n",
    "        d[\"clean_text\"] = t\n",
    "        cleaned.append(d)\n",
    "    context[\"ti\"].xcom_push(key=\"clean_docs\", value=cleaned)\n",
    "\n",
    "\n",
    "def chunk_documents(**context):\n",
    "    docs = context[\"ti\"].xcom_pull(key=\"clean_docs\")\n",
    "    for d in docs:\n",
    "        text = d[\"clean_text\"]\n",
    "        d[\"chunks\"] = [text[i:i+300] for i in range(0, len(text), 300)]\n",
    "    context[\"ti\"].xcom_push(key=\"chunked_docs\", value=docs)\n",
    "\n",
    "\n",
    "def embed_chunks(**context):\n",
    "    import numpy as np\n",
    "    docs = context[\"ti\"].xcom_pull(key=\"chunked_docs\")\n",
    "    for d in docs:\n",
    "        embeddings = []\n",
    "        for ch in d[\"chunks\"]:\n",
    "            emb = np.random.rand(1536).tolist()\n",
    "            embeddings.append(emb)\n",
    "        d[\"embeddings\"] = embeddings\n",
    "    context[\"ti\"].xcom_push(key=\"embedded_docs\", value=docs)\n",
    "\n",
    "\n",
    "def load_to_vector_store(**context):\n",
    "    docs = context[\"ti\"].xcom_pull(key=\"embedded_docs\")\n",
    "    # Example: write to FAISS / Pinecone\n",
    "    for d in docs:\n",
    "        print(f\"Upserting {len(d['embeddings'])} vectors for {d['doc_id']}\")\n",
    "    return \"completed\"\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# DAG definition\n",
    "# ---------------------------\n",
    "\n",
    "default_args = {\n",
    "    \"owner\": \"genai-team\",\n",
    "    \"retries\": 2,\n",
    "    \"retry_delay\": timedelta(minutes=3),\n",
    "}\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"rag_data_ingestion\",\n",
    "    default_args=default_args,\n",
    "    start_date=datetime(2025, 1, 1),\n",
    "    schedule_interval=\"@hourly\",\n",
    "    catchup=False,\n",
    ") as dag:\n",
    "\n",
    "    t1 = PythonOperator(task_id=\"extract\", python_callable=extract_documents)\n",
    "    t2 = PythonOperator(task_id=\"clean\", python_callable=clean_documents)\n",
    "    t3 = PythonOperator(task_id=\"chunk\", python_callable=chunk_documents)\n",
    "    t4 = PythonOperator(task_id=\"embed\", python_callable=embed_chunks)\n",
    "    t5 = PythonOperator(task_id=\"load\", python_callable=load_to_vector_store)\n",
    "\n",
    "    t1 >> t2 >> t3 >> t4 >> t5\n",
    "```\n",
    "\n",
    "**Key attributes**\n",
    "\n",
    "* Simple and extendable\n",
    "* XCom between tasks\n",
    "* Retry with exponential backoff\n",
    "* Hourly ingestion schedule\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Dagster Version (Same Workflow)\n",
    "\n",
    "Dagster uses *ops + graphs + jobs*, not tasks.\n",
    "\n",
    "#### File: `rag_ingest_job.py`\n",
    "\n",
    "```python\n",
    "from dagster import op, job, In, Out\n",
    "import numpy as np\n",
    "\n",
    "# ---------------------------\n",
    "# Ops (equivalent to tasks)\n",
    "# ---------------------------\n",
    "\n",
    "@op(out=Out(list))\n",
    "def extract_documents():\n",
    "    docs = [\n",
    "        {\n",
    "            \"doc_id\": \"doc_001\",\n",
    "            \"text\": \"Acme manual text...\",\n",
    "            \"metadata\": {\"source\": \"s3://bucket/acme.pdf\"}\n",
    "        }\n",
    "    ]\n",
    "    return docs\n",
    "\n",
    "\n",
    "@op(out=Out(list))\n",
    "def clean_documents(docs):\n",
    "    cleaned = []\n",
    "    for d in docs:\n",
    "        t = d[\"text\"].replace(\"Page 1 of 10\", \"\").strip()\n",
    "        d[\"clean_text\"] = t\n",
    "        cleaned.append(d)\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "@op(out=Out(list))\n",
    "def chunk_documents(docs):\n",
    "    out = []\n",
    "    for d in docs:\n",
    "        text = d[\"clean_text\"]\n",
    "        d[\"chunks\"] = [text[i:i+300] for i in range(0, len(text), 300)]\n",
    "        out.append(d)\n",
    "    return out\n",
    "\n",
    "\n",
    "@op(out=Out(list))\n",
    "def embed_chunks(docs):\n",
    "    for d in docs:\n",
    "        embeddings = []\n",
    "        for ch in d[\"chunks\"]:\n",
    "            emb = np.random.rand(1536).tolist()\n",
    "            embeddings.append(emb)\n",
    "        d[\"embeddings\"] = embeddings\n",
    "    return docs\n",
    "\n",
    "\n",
    "@op\n",
    "def load_to_vector_store(docs):\n",
    "    for d in docs:\n",
    "        print(f\"Upserting {len(d['embeddings'])} vectors for {d['doc_id']}\")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Job (orchestrates workflow)\n",
    "# ---------------------------\n",
    "\n",
    "@job\n",
    "def rag_ingestion_job():\n",
    "    docs = extract_documents()\n",
    "    cleaned = clean_documents(docs)\n",
    "    chunked = chunk_documents(cleaned)\n",
    "    embedded = embed_chunks(chunked)\n",
    "    load_to_vector_store(embedded)\n",
    "```\n",
    "\n",
    "**Dagster strengths**\n",
    "\n",
    "* Type-safe pipelines\n",
    "* Rich asset-based lineage tracking\n",
    "* Native observability and materialization\n",
    "* Supports sensors, schedules, and partitions\n",
    "\n",
    "---\n",
    "\n",
    "# 3. Scheduling in Dagster\n",
    "\n",
    "Add schedule:\n",
    "\n",
    "```python\n",
    "from dagster import ScheduleDefinition\n",
    "\n",
    "rag_ingestion_schedule = ScheduleDefinition(\n",
    "    job=rag_ingestion_job,\n",
    "    cron_schedule=\"0 * * * *\"  # hourly\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Airflow vs. Dagster (Generative AI ingestion comparison)\n",
    "\n",
    "| Feature                             | Airflow                 | Dagster                                  |\n",
    "| ----------------------------------- | ----------------------- | ---------------------------------------- |\n",
    "| Paradigm                            | Task-based DAG          | Software-defined assets                  |\n",
    "| Observability                       | Basic                   | Strong (lineage, materialization graphs) |\n",
    "| Retry/Recovery                      | Mature                  | Good                                     |\n",
    "| Distributed execution               | Celery/K8s              | K8s/agent                                |\n",
    "| Suitability for embeddings          | Good                    | Excellent (asset partitions)             |\n",
    "| Backfill support                    | Strong                  | Very strong                              |\n",
    "| Code-first                          | Partial                 | Full                                     |\n",
    "| Integrating GPU embedding workloads | Requires K8sPodOperator | Native Kubernetes job launches           |\n",
    "\n",
    "**Use case**\n",
    "\n",
    "* Airflow: enterprise batch scheduling, ETL-first setups\n",
    "* Dagster: ML/GenAI pipelines with strong lineage and asset semantics\n",
    "\n",
    "---\n",
    "\n",
    "If needed, I can also provide:\n",
    "\n",
    "* Dagster **asset-based** implementation\n",
    "* Airflow **KubernetesPodOperator** version for GPU embeddings\n",
    "* CI/CD structure for deploying both\n",
    "* Monitoring dashboards (Prometheus/Grafana) for ingestion metrics\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
