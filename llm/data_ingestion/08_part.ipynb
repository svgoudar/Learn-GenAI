{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e80d1b4b",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## Data Transformation\n",
    "\n",
    "---\n",
    "\n",
    "### 1 — High-level ETL / ELT Architecture\n",
    "\n",
    "**ETL (strict):**\n",
    "\n",
    "```\n",
    "Source Systems (S3, Web, DB, APIs)\n",
    "    ↓ Extract\n",
    "Preprocessing / Validation (clean, schema validate)\n",
    "    ↓ Transform (chunk, dedup, enrich, embed)\n",
    "    ↓ Load (Vector DB, Document Store, Data Lake)\n",
    "```\n",
    "\n",
    "**ELT (hybrid common in GenAI):**\n",
    "\n",
    "```\n",
    "Source Systems\n",
    "    ↓ Extract\n",
    "Raw Landing Zone (S3 / GCS / Data Lake)\n",
    "    ↓ Load\n",
    "Transformation Jobs (Spark / Python / DBT / Airflow)\n",
    "    ↓ Transformed Zone\n",
    "    ↓ Load into Vector DB / Feature Store / Catalog\n",
    "```\n",
    "\n",
    "**Key components**\n",
    "\n",
    "* Ingestors: connectors, scrapers, API fetchers\n",
    "* Parsers: PDF/OCR/HTML → text\n",
    "* Validator: schema, language, safety checks\n",
    "* Cleaner: boilerplate removal, normalization\n",
    "* Chunker: semantic chunking + metadata\n",
    "* Deduper: exact + near-duplicate logic\n",
    "* Embedder: model wrapper (OpenAI/local)\n",
    "* Vector store: Pinecone/FAISS/Chroma/Weaviate wrapper\n",
    "* Orchestrator: Airflow / Prefect / Dagster\n",
    "* Monitoring: metrics, logs, data-quality dashboard\n",
    "\n",
    "---\n",
    "\n",
    "### 2 — Python transformation pipeline\n",
    "\n",
    "**Files / modules**\n",
    "\n",
    "* `models.py` — pydantic schemas\n",
    "* `validator.py` — DocumentValidator (from earlier)\n",
    "* `cleaner.py` — boilerplate & normalization\n",
    "* `chunker.py` — semantic chunking\n",
    "* `dedup.py` — exact + near-duplicate logic\n",
    "* `embedder.py` — embedding wrapper (pluggable)\n",
    "* `storage.py` — vector store interface\n",
    "* `pipeline.py` — orchestrates ETL/ELT flow\n",
    "\n",
    "Below is a single-file example that includes the main modules for clarity. Replace embedding and storage implementations with production drivers.\n",
    "\n",
    "```python\n",
    "# pipeline_full.py\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from pydantic import BaseModel, Field\n",
    "import re, hashlib, json, time\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------\n",
    "# models.py\n",
    "# -------------------------\n",
    "class Metadata(BaseModel):\n",
    "    source: str\n",
    "    language: Optional[str] = \"en\"\n",
    "    timestamp: Optional[str] = None\n",
    "    tags: List[str] = []\n",
    "\n",
    "class Document(BaseModel):\n",
    "    doc_id: str\n",
    "    text: str\n",
    "    metadata: Metadata\n",
    "\n",
    "class Chunk(BaseModel):\n",
    "    chunk_id: str\n",
    "    doc_id: str\n",
    "    chunk_text: str\n",
    "    chunk_index: int\n",
    "    metadata: Dict = {}\n",
    "\n",
    "# -------------------------\n",
    "# validator.py (simplified)\n",
    "# -------------------------\n",
    "class DocumentValidator:\n",
    "    def __init__(self, embedding_dim=1536, allowed_langs=None):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.allowed_langs = allowed_langs or [\"en\"]\n",
    "\n",
    "    def validate_schema(self, doc: Document) -> Tuple[bool, str]:\n",
    "        if not doc.doc_id:\n",
    "            return False, \"Missing doc_id\"\n",
    "        if not doc.text or len(doc.text.strip()) < 10:\n",
    "            return False, \"Text too short\"\n",
    "        return True, \"ok\"\n",
    "\n",
    "    def validate_embedding(self, emb: np.ndarray) -> Tuple[bool, str]:\n",
    "        if emb is None:\n",
    "            return False, \"No embedding\"\n",
    "        if emb.ndim != 1:\n",
    "            return False, \"Embedding must be 1D\"\n",
    "        if emb.shape[0] != self.embedding_dim:\n",
    "            return False, f\"Embedding dimension mismatch {emb.shape[0]} != {self.embedding_dim}\"\n",
    "        if np.isnan(emb).any() or np.isinf(emb).any():\n",
    "            return False, \"Invalid embedding values\"\n",
    "        return True, \"ok\"\n",
    "\n",
    "# -------------------------\n",
    "# cleaner.py\n",
    "# -------------------------\n",
    "BOILERPLATE_PATTERNS = [\n",
    "    r\"Page \\d+ of \\d+\",\n",
    "    r\"Copyright.*\",\n",
    "    r\"Back to top\",\n",
    "    r\"^\\s*[-=]{3,}\\s*$\",  # separators\n",
    "]\n",
    "\n",
    "def remove_boilerplate(text: str) -> str:\n",
    "    out = text\n",
    "    for p in BOILERPLATE_PATTERNS:\n",
    "        out = re.sub(p, \" \", out, flags=re.IGNORECASE | re.MULTILINE)\n",
    "    out = re.sub(r\"\\s+\", \" \", out).strip()\n",
    "    return out\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    text = text.replace(\"\\u00A0\", \" \")\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    t = remove_boilerplate(text)\n",
    "    t = normalize_text(t)\n",
    "    return t\n",
    "\n",
    "# -------------------------\n",
    "# chunker.py\n",
    "# -------------------------\n",
    "def chunk_text_semantic(text: str, max_tokens: int = 256) -> List[str]:\n",
    "    # Simple heuristic chunker by sentences grouped to approx max_tokens words\n",
    "    sentences = re.split(r'(?<=[\\.\\?\\!])\\s+', text)\n",
    "    chunks = []\n",
    "    current = []\n",
    "    current_len = 0\n",
    "    for s in sentences:\n",
    "        words = s.split()\n",
    "        if current_len + len(words) > max_tokens and current:\n",
    "            chunks.append(\" \".join(current))\n",
    "            current, current_len = [], 0\n",
    "        current.append(s)\n",
    "        current_len += len(words)\n",
    "    if current:\n",
    "        chunks.append(\" \".join(current))\n",
    "    return chunks\n",
    "\n",
    "# -------------------------\n",
    "# dedup.py\n",
    "# -------------------------\n",
    "def sha256_hash(text: str) -> str:\n",
    "    return hashlib.sha256(text.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def cosine_sim(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))\n",
    "\n",
    "def exact_dedup(hashes_seen: set, text: str) -> bool:\n",
    "    h = sha256_hash(\" \".join(text.split()))\n",
    "    if h in hashes_seen:\n",
    "        return True\n",
    "    hashes_seen.add(h)\n",
    "    return False\n",
    "\n",
    "# -------------------------\n",
    "# embedder.py (stub)\n",
    "# -------------------------\n",
    "class Embedder:\n",
    "    def __init__(self, dim=1536):\n",
    "        self.dim = dim\n",
    "\n",
    "    def embed(self, texts: List[str]) -> List[np.ndarray]:\n",
    "        # Production: call OpenAI/Local model; here we use deterministic pseudo-embeds\n",
    "        out = []\n",
    "        for t in texts:\n",
    "            h = int(hashlib.md5(t.encode()).hexdigest()[:8], 16)\n",
    "            rng = np.random.RandomState(h)\n",
    "            v = rng.randn(self.dim).astype(np.float32)\n",
    "            v = v / (np.linalg.norm(v) + 1e-9)\n",
    "            out.append(v)\n",
    "        return out\n",
    "\n",
    "# -------------------------\n",
    "# storage.py (vector store stub)\n",
    "# -------------------------\n",
    "class InMemoryVectorStore:\n",
    "    def __init__(self, dim=1536):\n",
    "        self.dim = dim\n",
    "        self.vectors = []  # list of (id, vector, metadata)\n",
    "\n",
    "    def upsert(self, id: str, vector: np.ndarray, metadata: Dict):\n",
    "        self.vectors.append((id, vector, metadata))\n",
    "\n",
    "    def query(self, vector: np.ndarray, top_k=5):\n",
    "        sims = [(cid, cosine_sim(vector, v), meta) for cid, v, meta in self.vectors]\n",
    "        sims.sort(key=lambda x: x[1], reverse=True)\n",
    "        return sims[:top_k]\n",
    "\n",
    "# -------------------------\n",
    "# pipeline.py\n",
    "# -------------------------\n",
    "class RAGIngestionPipeline:\n",
    "    def __init__(self, validator: DocumentValidator, embedder: Embedder, store: InMemoryVectorStore):\n",
    "        self.validator = validator\n",
    "        self.embedder = embedder\n",
    "        self.store = store\n",
    "        self.hashes_seen = set()\n",
    "\n",
    "    def ingest_document(self, doc: Document):\n",
    "        ok, msg = self.validator.validate_schema(doc)\n",
    "        if not ok:\n",
    "            return False, msg\n",
    "\n",
    "        text = clean_text(doc.text)\n",
    "        chunks = chunk_text_semantic(text, max_tokens=200)\n",
    "\n",
    "        # exact dedup at chunk level\n",
    "        unique_chunks = []\n",
    "        for i, c in enumerate(chunks):\n",
    "            if exact_dedup(self.hashes_seen, c):\n",
    "                continue\n",
    "            chunk_id = f\"{doc.doc_id}#chunk{i}\"\n",
    "            unique_chunks.append(Chunk(chunk_id=chunk_id, doc_id=doc.doc_id, chunk_text=c, chunk_index=i, metadata=doc.metadata.dict()))\n",
    "\n",
    "        if not unique_chunks:\n",
    "            return False, \"No unique chunks after dedup\"\n",
    "\n",
    "        texts = [c.chunk_text for c in unique_chunks]\n",
    "        embeddings = self.embedder.embed(texts)\n",
    "\n",
    "        # near-duplicate check (naive pairwise vs store)\n",
    "        for ch, emb in zip(unique_chunks, embeddings):\n",
    "            v_ok, v_msg = self.validator.validate_embedding(emb)\n",
    "            if not v_ok:\n",
    "                return False, f\"Embedding invalid: {v_msg}\"\n",
    "            # naive: ensure not too similar to any stored vector\n",
    "            too_similar = False\n",
    "            for cid, v, meta in self.store.vectors:\n",
    "                if cosine_sim(emb, v) > 0.995:\n",
    "                    too_similar = True\n",
    "                    break\n",
    "            if too_similar:\n",
    "                continue\n",
    "            self.store.upsert(ch.chunk_id, emb, ch.metadata)\n",
    "\n",
    "        return True, f\"Ingested {len(self.store.vectors)} vectors\"\n",
    "\n",
    "# -------------------------\n",
    "# Example usage\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    doc = Document(\n",
    "        doc_id=\"doc_123\",\n",
    "        text=\"\"\"\n",
    "            Acme Product Manual – Version 4.2\n",
    "            Copyright © 2024 Acme Corp.\n",
    "            Page 1 of 3\n",
    "\n",
    "            How to install the Acme Widget?\n",
    "            Step 1: Unpack the device.\n",
    "            Step 2: Connect the power cable.\n",
    "            Step 3: Download the Acme setup app.\n",
    "        \"\"\",\n",
    "        metadata=Metadata(source=\"s3://bucket/manual.pdf\", timestamp=time.strftime(\"%Y-%m-%dT%H:%M:%SZ\"))\n",
    "    )\n",
    "\n",
    "    validator = DocumentValidator(embedding_dim=1536)\n",
    "    embedder = Embedder(dim=1536)\n",
    "    store = InMemoryVectorStore(dim=1536)\n",
    "    pipeline = RAGIngestionPipeline(validator, embedder, store)\n",
    "\n",
    "    ok, msg = pipeline.ingest_document(doc)\n",
    "    print(ok, msg)\n",
    "    # Query example\n",
    "    q = embedder.embed([\"How to install the Acme Widget?\"])[0]\n",
    "    print(\"Query results:\", store.query(q, top_k=3))\n",
    "```\n",
    "\n",
    "**Notes**\n",
    "\n",
    "* Swap `Embedder.embed` with real model calls (OpenAI / local) and `InMemoryVectorStore` with FAISS/Pinecone/Chroma.\n",
    "* Add async support for throughput.\n",
    "* Add logging, retry, DLQ (dead-letter queue) for failed docs.\n",
    "\n",
    "---\n",
    "\n",
    "### 3 — Airflow DAG for ingestion + transformation\n",
    "\n",
    "Below is an Airflow DAG skeleton using `PythonOperator`. It orchestrates extract → transform → load. Replace the task functions with the pipeline methods above.\n",
    "\n",
    "```python\n",
    "# airflow_dag.py\n",
    "from datetime import datetime, timedelta\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "# Import your pipeline modules (assume available in PYTHONPATH)\n",
    "from pipeline_full import Document, Metadata, RAGIngestionPipeline, DocumentValidator, Embedder, InMemoryVectorStore\n",
    "import json, time\n",
    "\n",
    "DEFAULT_ARGS = {\n",
    "    \"owner\": \"data-team\",\n",
    "    \"depends_on_past\": False,\n",
    "    \"email_on_failure\": False,\n",
    "    \"retries\": 1,\n",
    "    \"retry_delay\": timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    dag_id=\"rag_ingest_pipeline\",\n",
    "    default_args=DEFAULT_ARGS,\n",
    "    start_date=datetime(2025, 1, 1),\n",
    "    schedule_interval=\"@hourly\",\n",
    "    catchup=False,\n",
    "    max_active_runs=1,\n",
    ")\n",
    "\n",
    "# Mocked extract step\n",
    "def extract(**context):\n",
    "    # In production: list objects from S3, read DB rows, subscribe to Kafka topic, etc.\n",
    "    sample_doc = {\n",
    "        \"doc_id\": f\"doc-{int(time.time())}\",\n",
    "        \"text\": \"Acme manual ...\",\n",
    "        \"metadata\": {\"source\": \"s3://bucket/sample.pdf\", \"timestamp\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\")}\n",
    "    }\n",
    "    # push list of documents to XCom (or push references)\n",
    "    context['ti'].xcom_push(key=\"raw_docs\", value=json.dumps([sample_doc]))\n",
    "\n",
    "def transform(**context):\n",
    "    raw = context['ti'].xcom_pull(key=\"raw_docs\")\n",
    "    docs = json.loads(raw)\n",
    "    # instantiate pipeline components (or import from shared module)\n",
    "    validator = DocumentValidator(embedding_dim=1536)\n",
    "    embedder = Embedder(dim=1536)\n",
    "    store = InMemoryVectorStore(dim=1536)\n",
    "    pipeline = RAGIngestionPipeline(validator, embedder, store)\n",
    "    results = []\n",
    "    for d in docs:\n",
    "        doc = Document(doc_id=d[\"doc_id\"], text=d[\"text\"], metadata=Metadata(**d[\"metadata\"]))\n",
    "        ok, msg = pipeline.ingest_document(doc)\n",
    "        results.append({\"doc_id\": d[\"doc_id\"], \"ok\": ok, \"msg\": msg})\n",
    "    context['ti'].xcom_push(key=\"ingest_results\", value=json.dumps(results))\n",
    "\n",
    "def load(**context):\n",
    "    # load could push to production vector DB, update catalog, metrics, etc.\n",
    "    results = json.loads(context['ti'].xcom_pull(key=\"ingest_results\"))\n",
    "    # For demo, just print or send to monitoring\n",
    "    for r in results:\n",
    "        print(\"Ingest result:\", r)\n",
    "\n",
    "t1 = PythonOperator(task_id=\"extract\", python_callable=extract, dag=dag)\n",
    "t2 = PythonOperator(task_id=\"transform\", python_callable=transform, dag=dag)\n",
    "t3 = PythonOperator(task_id=\"load\", python_callable=load, dag=dag)\n",
    "\n",
    "t1 >> t2 >> t3\n",
    "```\n",
    "\n",
    "**Production considerations**\n",
    "\n",
    "* Use sensors/operators for S3/Kafka.\n",
    "* Use XCom sparingly — prefer storing references in S3/DB and passing keys.\n",
    "* Make tasks idempotent.\n",
    "* Add DLQ handling for failed docs.\n",
    "* Parameterize resources and concurrency (pooling, task slots).\n",
    "* Use KubernetesPodOperator or DockerOperator for heavy embedding tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### 4 — How transformation differs: RAG vs LLM fine-tuning\n",
    "\n",
    "| Aspect                   |                                            RAG (Retrieval-Augmented Generation) | LLM Fine-tuning                                                                                       |\n",
    "| ------------------------ | ------------------------------------------------------------------------------: | ----------------------------------------------------------------------------------------------------- |\n",
    "| Primary goal             |                   Fast, accurate retrieval of supporting context for generation | Update model weights to change behavior or knowledge                                                  |\n",
    "| Output of transform      |                                  Clean, coherent chunks + embeddings + metadata | Paired training examples (input → target), instruction–response pairs, tokenized datasets             |\n",
    "| Chunking strategy        | Semantic chunks (200–800 tokens) preserving context; include citations/metadata | Examples often shorter; preserve full dialogue/turns if training conversational behavior              |\n",
    "| Deduplication importance |                  Critical — duplicate chunks waste vector DB and bias retrieval | Important but may be allowed if duplicates represent important distribution                           |\n",
    "| Annotation needs         |                  Minimal; quality scores, citations, source attribution helpful | High: labels, correct outputs, multi-step reasoning, prompt formatting, instruction templates         |\n",
    "| Safety transformations   |      Remove unsafe content before indexing; mask PII to avoid retrieval leakage | Strict removal of harmful content; curated to avoid introducing unsafe behavior during weight updates |\n",
    "| Embedding generation     |    Required; multiple embedding versions possible; embedding validation crucial | Not required (unless training retrieval-augmented finetuning)                                         |\n",
    "| Format for storage       |                           Vector DB + document store (JSON lines with metadata) | JSONL or TFRecords: `{prompt:..., completion:...}` or conversational formats                          |\n",
    "| Update cadence           |                              Frequent incremental ingestion; live index updates | Less frequent; expensive retraining or LoRA/adapter updates                                           |\n",
    "| Evaluation               |            Retrieval metrics (recall@k), end-to-end generation with R-precision | Perplexity, BLEU, ROUGE, human eval, instruction-following benchmarks                                 |\n",
    "| Handling updates         |                     Dual-write, lazy migration of embeddings, versioned indexes | Re-train / fine-tune or use adapters; more costly to update knowledge                                 |\n",
    "\n",
    "**Practical differences in transformation steps**\n",
    "\n",
    "* **RAG**\n",
    "\n",
    "  * Emphasize: boilerplate removal, chunk coherence, citation extraction, embedding generation, dedup.\n",
    "  * Produce: `(chunk_id, chunk_text, embedding, metadata)` records.\n",
    "  * Post-process: index vectors and keep provenance.\n",
    "\n",
    "* **Fine-tuning**\n",
    "\n",
    "  * Emphasize: consistent formatting, label correctness, instruction style alignment, dedup of erroneous labels, dataset balancing.\n",
    "  * Produce: training records in model-specific format (JSONL, TFRecords) with careful tokenization and padding, often with quality metadata and sampling weights.\n",
    "  * Post-process: calculate dataset statistics, remove low-quality examples, split train/val/test.\n",
    "\n",
    "---\n",
    "\n",
    "## 5 — Additional production considerations\n",
    "\n",
    "* **Observability**: track ingestion success rate, validation failure reasons, duplicate rates, embedder latency, downstream retrieval performance.\n",
    "* **Versioning**: schema_version in documents, embedding_model_version, vector_index_version.\n",
    "* **Backfill / Migration**: support lazy migration (on-read) and batch re-embed pipelines for embedding model upgrades.\n",
    "* **Privacy & Compliance**: PII detection & masking, retention policies, opt-out handling.\n",
    "* **Cost/Throughput**: batch embedding calls, parallelization, GPU/CPU sizing, rate-limiting for remote APIs.\n",
    "* **Testing**: unit tests for cleaning, chunking, dedup; integration tests for upserting and querying.\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I will:\n",
    "\n",
    "* produce a runnable repo scaffold with the files separated, or\n",
    "* convert the pipeline into an Airflow `KubernetesPodOperator` pattern for scalable embedding jobs, or\n",
    "* provide a production-ready `Dockerfile` + `docker-compose` for local testing.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
