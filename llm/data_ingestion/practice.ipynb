{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a7e6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_classic.schema import Document\n",
    "\n",
    "# Custom loader function to preserve context and metadata\n",
    "def load_documents_with_context(dataset_path: str):\n",
    "    \"\"\"\n",
    "    Load documents while preserving file structure and metadata\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    dataset_dir = Path(dataset_path)\n",
    "    \n",
    "    # Walk through all subdirectories to maintain category information\n",
    "    for category_dir in dataset_dir.iterdir():\n",
    "        if category_dir.is_dir():\n",
    "            category_name = category_dir.name\n",
    "            print(f\"Loading category: {category_name}\")\n",
    "            \n",
    "            # Load all .txt files in this category\n",
    "            txt_files = list(category_dir.glob(\"*.txt\"))\n",
    "            print(f\"  Found {len(txt_files)} files\")\n",
    "            \n",
    "            for txt_file in txt_files:\n",
    "                try:\n",
    "                    with open(txt_file, 'r', encoding='utf-8') as f:\n",
    "                        content = f.read()\n",
    "                    \n",
    "                    # Create document with rich metadata\n",
    "                    doc = Document(\n",
    "                        page_content=content,\n",
    "                        metadata={\n",
    "                            'source': str(txt_file),\n",
    "                            'filename': txt_file.name,\n",
    "                            'category': category_name,\n",
    "                            'file_size': len(content),\n",
    "                            'word_count': len(content.split()),\n",
    "                            'file_path': str(txt_file.relative_to(dataset_dir))\n",
    "                        }\n",
    "                    )\n",
    "                    documents.append(doc)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  Error loading {txt_file}: {e}\")\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Load documents with preserved context\n",
    "dataset_path = r\"***\"\n",
    "docs = load_documents_with_context(dataset_path)\n",
    "\n",
    "print(f\"\\nTotal documents loaded: {len(docs)}\")\n",
    "\n",
    "# Display category breakdown\n",
    "if docs:\n",
    "    categories = {}\n",
    "    for doc in docs:\n",
    "        cat = doc.metadata['category']\n",
    "        categories[cat] = categories.get(cat, 0) + 1\n",
    "    \n",
    "    print(f\"\\nDocuments by category:\")\n",
    "    for category, count in sorted(categories.items()):\n",
    "        print(f\"  {category}: {count} documents\")\n",
    "    \n",
    "    # Show sample document metadata\n",
    "    print(f\"\\nSample document metadata:\")\n",
    "    sample_doc = docs[0]\n",
    "    for key, value in sample_doc.metadata.items():\n",
    "        if isinstance(value, str) and len(value) > 50:\n",
    "            print(f\"  {key}: {value[:50]}...\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(f\"\\nSample content preview:\")\n",
    "    print(f\"{sample_doc.page_content[:200]}...\")\n",
    "\n",
    "# Enhanced text splitter that preserves metadata\n",
    "class ContextAwareTextSplitter(RecursiveCharacterTextSplitter):\n",
    "    \"\"\"\n",
    "    Text splitter that preserves document metadata in chunks\n",
    "    \"\"\"\n",
    "    \n",
    "    def split_documents(self, documents):\n",
    "        \"\"\"Split documents while preserving all metadata\"\"\"\n",
    "        all_chunks = []\n",
    "        \n",
    "        for doc in documents:\n",
    "            # Split the document content\n",
    "            chunks = self.split_text(doc.page_content)\n",
    "            \n",
    "            # Create new documents for each chunk with preserved metadata\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                chunk_doc = Document(\n",
    "                    page_content=chunk,\n",
    "                    metadata={\n",
    "                        **doc.metadata,  # Preserve all original metadata\n",
    "                        'chunk_index': i,\n",
    "                        'total_chunks': len(chunks),\n",
    "                        'chunk_size': len(chunk.split())\n",
    "                    }\n",
    "                )\n",
    "                all_chunks.append(chunk_doc)\n",
    "        \n",
    "        return all_chunks\n",
    "\n",
    "# Split into chunks while preserving context\n",
    "splitter = ContextAwareTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "print(f\"\\nTotal chunks created: {len(chunks)}\")\n",
    "\n",
    "# Analyze chunk distribution by category\n",
    "if chunks:\n",
    "    chunk_categories = {}\n",
    "    chunk_sizes = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        cat = chunk.metadata['category']\n",
    "        chunk_categories[cat] = chunk_categories.get(cat, 0) + 1\n",
    "        chunk_sizes.append(chunk.metadata['chunk_size'])\n",
    "    \n",
    "    print(f\"\\nChunks by category:\")\n",
    "    for category, count in sorted(chunk_categories.items()):\n",
    "        print(f\"  {category}: {count} chunks\")\n",
    "    \n",
    "    import statistics\n",
    "    print(f\"\\nChunk size statistics:\")\n",
    "    print(f\"  Average: {statistics.mean(chunk_sizes):.1f} words\")\n",
    "    print(f\"  Median: {statistics.median(chunk_sizes):.1f} words\")\n",
    "    print(f\"  Min: {min(chunk_sizes)} words\")\n",
    "    print(f\"  Max: {max(chunk_sizes)} words\")\n",
    "    \n",
    "    # Show sample chunk with metadata\n",
    "    print(f\"\\nSample chunk metadata:\")\n",
    "    sample_chunk = chunks[0]\n",
    "    for key, value in sample_chunk.metadata.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(f\"\\nSample chunk content:\")\n",
    "    print(f\"{sample_chunk.page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a497a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.embeddings import HuggingFaceEmbeddings\n",
    "emb = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b46aa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.vectorstores import Chroma\n",
    "db = Chroma.from_documents(chunks, emb, collection_name=\"knowledge_base\")\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 4})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba35fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "628d3f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./chroma'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db._persist_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac73acc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "# from langchain.chains import RetrievalQA\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",   # public hosted model\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f37ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain_classic.chains.retrieval_qa.base import RetrievalQA\n",
    "\n",
    "# llm = Ollama(base_url=\"http://localhost:11435\", model=\"llama3\")\n",
    "rag = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "print(rag(\"Explain HIV vaccine trial\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b124c29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yourenv",
   "language": "python",
   "name": "yourenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
