{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20a7e6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading category: business\n",
      "  Found 100 files\n",
      "Loading category: entertainment\n",
      "  Found 100 files\n",
      "Loading category: food\n",
      "  Found 100 files\n",
      "Loading category: graphics\n",
      "  Found 100 files\n",
      "Loading category: food\n",
      "  Found 100 files\n",
      "Loading category: graphics\n",
      "  Found 100 files\n",
      "Loading category: historical\n",
      "  Found 100 files\n",
      "Loading category: medical\n",
      "  Found 100 files\n",
      "Loading category: historical\n",
      "  Found 100 files\n",
      "Loading category: medical\n",
      "  Found 100 files\n",
      "Loading category: politics\n",
      "  Found 100 files\n",
      "Loading category: space\n",
      "  Found 100 files\n",
      "Loading category: politics\n",
      "  Found 100 files\n",
      "Loading category: space\n",
      "  Found 100 files\n",
      "Loading category: sport\n",
      "  Found 100 files\n",
      "Loading category: technologie\n",
      "  Found 100 files\n",
      "Loading category: sport\n",
      "  Found 100 files\n",
      "Loading category: technologie\n",
      "  Found 100 files\n",
      "\n",
      "Total documents loaded: 1000\n",
      "\n",
      "Documents by category:\n",
      "  business: 100 documents\n",
      "  entertainment: 100 documents\n",
      "  food: 100 documents\n",
      "  graphics: 100 documents\n",
      "  historical: 100 documents\n",
      "  medical: 100 documents\n",
      "  politics: 100 documents\n",
      "  space: 100 documents\n",
      "  sport: 100 documents\n",
      "  technologie: 100 documents\n",
      "\n",
      "Sample document metadata:\n",
      "  source: c:\\Github\\Learn-GenAI\\genai_book\\datasets\\txt\\busi...\n",
      "  filename: business_1.txt\n",
      "  category: business\n",
      "  file_size: 848\n",
      "  word_count: 149\n",
      "  file_path: business\\business_1.txt\n",
      "\n",
      "Sample content preview:\n",
      "Lufthansa flies back to profit\n",
      "\n",
      "German airline Lufthansa has returned to profit in 2004 after posting huge losses in 2003.\n",
      "\n",
      "In a preliminary report, the airline announced net profits of 400m euros ($5...\n",
      "\n",
      "Total chunks created: 3674\n",
      "\n",
      "Chunks by category:\n",
      "  business: 288 chunks\n",
      "  entertainment: 263 chunks\n",
      "  food: 211 chunks\n",
      "  graphics: 270 chunks\n",
      "  historical: 800 chunks\n",
      "  medical: 380 chunks\n",
      "  politics: 370 chunks\n",
      "  space: 394 chunks\n",
      "  sport: 290 chunks\n",
      "  technologie: 408 chunks\n",
      "\n",
      "Chunk size statistics:\n",
      "  Average: 114.3 words\n",
      "  Median: 125.0 words\n",
      "  Min: 1 words\n",
      "  Max: 191 words\n",
      "\n",
      "Sample chunk metadata:\n",
      "  source: c:\\Github\\Learn-GenAI\\genai_book\\datasets\\txt\\business\\business_1.txt\n",
      "  filename: business_1.txt\n",
      "  category: business\n",
      "  file_size: 848\n",
      "  word_count: 149\n",
      "  file_path: business\\business_1.txt\n",
      "  chunk_index: 0\n",
      "  total_chunks: 1\n",
      "  chunk_size: 149\n",
      "\n",
      "Sample chunk content:\n",
      "Lufthansa flies back to profit\n",
      "\n",
      "German airline Lufthansa has returned to profit in 2004 after posting huge losses in 2003.\n",
      "\n",
      "In a preliminary report, t...\n",
      "\n",
      "Total documents loaded: 1000\n",
      "\n",
      "Documents by category:\n",
      "  business: 100 documents\n",
      "  entertainment: 100 documents\n",
      "  food: 100 documents\n",
      "  graphics: 100 documents\n",
      "  historical: 100 documents\n",
      "  medical: 100 documents\n",
      "  politics: 100 documents\n",
      "  space: 100 documents\n",
      "  sport: 100 documents\n",
      "  technologie: 100 documents\n",
      "\n",
      "Sample document metadata:\n",
      "  source: c:\\Github\\Learn-GenAI\\genai_book\\datasets\\txt\\busi...\n",
      "  filename: business_1.txt\n",
      "  category: business\n",
      "  file_size: 848\n",
      "  word_count: 149\n",
      "  file_path: business\\business_1.txt\n",
      "\n",
      "Sample content preview:\n",
      "Lufthansa flies back to profit\n",
      "\n",
      "German airline Lufthansa has returned to profit in 2004 after posting huge losses in 2003.\n",
      "\n",
      "In a preliminary report, the airline announced net profits of 400m euros ($5...\n",
      "\n",
      "Total chunks created: 3674\n",
      "\n",
      "Chunks by category:\n",
      "  business: 288 chunks\n",
      "  entertainment: 263 chunks\n",
      "  food: 211 chunks\n",
      "  graphics: 270 chunks\n",
      "  historical: 800 chunks\n",
      "  medical: 380 chunks\n",
      "  politics: 370 chunks\n",
      "  space: 394 chunks\n",
      "  sport: 290 chunks\n",
      "  technologie: 408 chunks\n",
      "\n",
      "Chunk size statistics:\n",
      "  Average: 114.3 words\n",
      "  Median: 125.0 words\n",
      "  Min: 1 words\n",
      "  Max: 191 words\n",
      "\n",
      "Sample chunk metadata:\n",
      "  source: c:\\Github\\Learn-GenAI\\genai_book\\datasets\\txt\\business\\business_1.txt\n",
      "  filename: business_1.txt\n",
      "  category: business\n",
      "  file_size: 848\n",
      "  word_count: 149\n",
      "  file_path: business\\business_1.txt\n",
      "  chunk_index: 0\n",
      "  total_chunks: 1\n",
      "  chunk_size: 149\n",
      "\n",
      "Sample chunk content:\n",
      "Lufthansa flies back to profit\n",
      "\n",
      "German airline Lufthansa has returned to profit in 2004 after posting huge losses in 2003.\n",
      "\n",
      "In a preliminary report, t...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_classic.schema import Document\n",
    "\n",
    "# Custom loader function to preserve context and metadata\n",
    "def load_documents_with_context(dataset_path: str):\n",
    "    \"\"\"\n",
    "    Load documents while preserving file structure and metadata\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    dataset_dir = Path(dataset_path)\n",
    "    \n",
    "    # Walk through all subdirectories to maintain category information\n",
    "    for category_dir in dataset_dir.iterdir():\n",
    "        if category_dir.is_dir():\n",
    "            category_name = category_dir.name\n",
    "            print(f\"Loading category: {category_name}\")\n",
    "            \n",
    "            # Load all .txt files in this category\n",
    "            txt_files = list(category_dir.glob(\"*.txt\"))\n",
    "            print(f\"  Found {len(txt_files)} files\")\n",
    "            \n",
    "            for txt_file in txt_files:\n",
    "                try:\n",
    "                    with open(txt_file, 'r', encoding='utf-8') as f:\n",
    "                        content = f.read()\n",
    "                    \n",
    "                    # Create document with rich metadata\n",
    "                    doc = Document(\n",
    "                        page_content=content,\n",
    "                        metadata={\n",
    "                            'source': str(txt_file),\n",
    "                            'filename': txt_file.name,\n",
    "                            'category': category_name,\n",
    "                            'file_size': len(content),\n",
    "                            'word_count': len(content.split()),\n",
    "                            'file_path': str(txt_file.relative_to(dataset_dir))\n",
    "                        }\n",
    "                    )\n",
    "                    documents.append(doc)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  Error loading {txt_file}: {e}\")\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Load documents with preserved context\n",
    "dataset_path = r\"c:\\Github\\Learn-GenAI\\genai_book\\datasets\\txt\"\n",
    "docs = load_documents_with_context(dataset_path)\n",
    "\n",
    "print(f\"\\nTotal documents loaded: {len(docs)}\")\n",
    "\n",
    "# Display category breakdown\n",
    "if docs:\n",
    "    categories = {}\n",
    "    for doc in docs:\n",
    "        cat = doc.metadata['category']\n",
    "        categories[cat] = categories.get(cat, 0) + 1\n",
    "    \n",
    "    print(f\"\\nDocuments by category:\")\n",
    "    for category, count in sorted(categories.items()):\n",
    "        print(f\"  {category}: {count} documents\")\n",
    "    \n",
    "    # Show sample document metadata\n",
    "    print(f\"\\nSample document metadata:\")\n",
    "    sample_doc = docs[0]\n",
    "    for key, value in sample_doc.metadata.items():\n",
    "        if isinstance(value, str) and len(value) > 50:\n",
    "            print(f\"  {key}: {value[:50]}...\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(f\"\\nSample content preview:\")\n",
    "    print(f\"{sample_doc.page_content[:200]}...\")\n",
    "\n",
    "# Enhanced text splitter that preserves metadata\n",
    "class ContextAwareTextSplitter(RecursiveCharacterTextSplitter):\n",
    "    \"\"\"\n",
    "    Text splitter that preserves document metadata in chunks\n",
    "    \"\"\"\n",
    "    \n",
    "    def split_documents(self, documents):\n",
    "        \"\"\"Split documents while preserving all metadata\"\"\"\n",
    "        all_chunks = []\n",
    "        \n",
    "        for doc in documents:\n",
    "            # Split the document content\n",
    "            chunks = self.split_text(doc.page_content)\n",
    "            \n",
    "            # Create new documents for each chunk with preserved metadata\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                chunk_doc = Document(\n",
    "                    page_content=chunk,\n",
    "                    metadata={\n",
    "                        **doc.metadata,  # Preserve all original metadata\n",
    "                        'chunk_index': i,\n",
    "                        'total_chunks': len(chunks),\n",
    "                        'chunk_size': len(chunk.split())\n",
    "                    }\n",
    "                )\n",
    "                all_chunks.append(chunk_doc)\n",
    "        \n",
    "        return all_chunks\n",
    "\n",
    "# Split into chunks while preserving context\n",
    "splitter = ContextAwareTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "print(f\"\\nTotal chunks created: {len(chunks)}\")\n",
    "\n",
    "# Analyze chunk distribution by category\n",
    "if chunks:\n",
    "    chunk_categories = {}\n",
    "    chunk_sizes = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        cat = chunk.metadata['category']\n",
    "        chunk_categories[cat] = chunk_categories.get(cat, 0) + 1\n",
    "        chunk_sizes.append(chunk.metadata['chunk_size'])\n",
    "    \n",
    "    print(f\"\\nChunks by category:\")\n",
    "    for category, count in sorted(chunk_categories.items()):\n",
    "        print(f\"  {category}: {count} chunks\")\n",
    "    \n",
    "    import statistics\n",
    "    print(f\"\\nChunk size statistics:\")\n",
    "    print(f\"  Average: {statistics.mean(chunk_sizes):.1f} words\")\n",
    "    print(f\"  Median: {statistics.median(chunk_sizes):.1f} words\")\n",
    "    print(f\"  Min: {min(chunk_sizes)} words\")\n",
    "    print(f\"  Max: {max(chunk_sizes)} words\")\n",
    "    \n",
    "    # Show sample chunk with metadata\n",
    "    print(f\"\\nSample chunk metadata:\")\n",
    "    sample_chunk = chunks[0]\n",
    "    for key, value in sample_chunk.metadata.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(f\"\\nSample chunk content:\")\n",
    "    print(f\"{sample_chunk.page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a497a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sangouda\\AppData\\Local\\Temp\\ipykernel_36172\\361145277.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  emb = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.embeddings import HuggingFaceEmbeddings\n",
    "emb = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b46aa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.vectorstores import Chroma\n",
    "db = Chroma.from_documents(chunks, emb, collection_name=\"knowledge_base\")\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 4})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ba35fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sangouda\\AppData\\Local\\Temp\\ipykernel_36172\\123899826.py:1: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  db.persist()\n"
     ]
    }
   ],
   "source": [
    "db.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "628d3f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./chroma'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db._persist_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6aed6ebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac73acc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "# from langchain.chains import RetrievalQA\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",   # public hosted model\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02f37ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Explain HIV vaccine trial', 'result': 'The HIV vaccine trial mentioned began in March 1993 and is notable for being the first trial of experimental HIV vaccines in children who are infected with the human immunodeficiency virus (HIV), the virus that causes AIDS. The National Institutes of Health (NIH) conducted the trial to compare the safety of three experimental HIV vaccines in 90 children recruited from at least 12 sites across the United States. \\n\\nParticipants in the trial were required to be HIV-infected but without symptoms of HIV disease. The purpose of the trial was to gather preliminary data on how childrenâ€™s immature immune systems respond to candidate HIV vaccines, an important step in designing future trials aimed at preventing HIV infection in children.\\n\\nThe trial leaders intended to observe any side effects from the vaccines, such as swollen arms or fevers, and to analyze the immune responses generated by different doses of the vaccines. Specifically, the trial tested two doses each of three different subunit vaccines, which are genetically engineered to incorporate only pieces of the virus, showing previous promise in adult trials.\\n\\nAt least half of the children involved in the trial were to be 2 years of age or younger to allow comparisons between younger and older participants. The study was coordinated by a team of medical experts from various institutions. Overall, the trial represented a hopeful step in addressing the challenges faced by HIV-infected children.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain_classic.chains.retrieval_qa.base import RetrievalQA\n",
    "\n",
    "# llm = Ollama(base_url=\"http://localhost:11435\", model=\"llama3\")\n",
    "rag = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "print(rag(\"Explain HIV vaccine trial\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b124c29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yourenv",
   "language": "python",
   "name": "yourenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
