{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a53c0721",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## Scalability & throughput optimization \n",
    "\n",
    "Below is a compact, actionable guide covering where ingestion systems bottleneck, concrete strategies to scale each stage, configuration examples, metrics to track, and operational trade-offs.\n",
    "\n",
    "---\n",
    "\n",
    "### 1 — Common bottlenecks\n",
    "\n",
    "* **Parsing/IO**: reading many files from S3 / networks.\n",
    "* **Parsing/CPU**: PDF/OCR/HTML cleaning.\n",
    "* **Dedup / similarity checks**: expensive similarity ops at scale.\n",
    "* **Embedding generation**: GPU/remote API latency and rate limits.\n",
    "* **Vector DB writes**: upsert throughput and index rebuild costs.\n",
    "* **Orchestration**: task orchestration and task startup latency.\n",
    "* **Backpressure**: saturating downstream services (vector DB, embedder).\n",
    "\n",
    "---\n",
    "\n",
    "### 2 — High-level strategies (principles)\n",
    "\n",
    "* **Push compute down**: do expensive ops (OCR, dedup) with parallel workers close to data.\n",
    "* **Batch work**: group documents/chunks into optimal batches for embeddings and DB writes.\n",
    "* **Streaming + micro-batch**: combine streaming ingestion (Kafka) with timed micro-batches for downstream heavy ops.\n",
    "* **Autoscale resources**: scale GPU workers and CPU workers independently.\n",
    "* **Idempotent operations**: enable safe retries and replays.\n",
    "* **Asynchronous pipelines**: decouple stages with durable queues (Kafka/SQS).\n",
    "* **Cache & reuse**: cache embeddings for identical texts / reuse existing vectors.\n",
    "* **Backpressure / circuit breakers**: prevent overload when external services degrade.\n",
    "\n",
    "---\n",
    "\n",
    "### 3 — Stage-by-stage optimizations\n",
    "\n",
    "#### Extraction & IO\n",
    "\n",
    "* Use **parallel listing & range GETs** for object stores.\n",
    "* Use **S3 Transfer Acceleration / multi-part downloads** where applicable.\n",
    "* Use workers colocated in the same region and VPC endpoints.\n",
    "\n",
    "#### Parsing & Cleaning (CPU)\n",
    "\n",
    "* Parallelize by file; use **multiprocessing** or Kubernetes pods.\n",
    "* Use optimized parsers (pdfminer / tika) with streaming parsing.\n",
    "* Offload OCR to GPU pods only for scanned docs (detect first).\n",
    "\n",
    "#### Deduplication & Noise Removal\n",
    "\n",
    "* **Exact dedup**: fast hash (SHA-256) with bloom filter to reject early.\n",
    "* **Near-dedup**: do an approximate LSH / MinHash prefilter then fine cosine on a small candidate set.\n",
    "* Maintain a **cache of recent chunk signatures** to avoid recomputation.\n",
    "\n",
    "#### Chunking\n",
    "\n",
    "* Chunk in parallel; emit metadata (doc_id, offsets) to preserve lineage.\n",
    "* Choose chunk sizes tuned to embedding model token window.\n",
    "\n",
    "#### Embedding Generation (the primary hotspot)\n",
    "\n",
    "* Use **batching**: group N chunks per call. Typical batch sizes: 16–512 depending on model & GPU memory.\n",
    "* For local models:\n",
    "\n",
    "  * Use **GPU pods** via KubernetesPodOperator / K8s jobs; utilize mixed precision and batch inference.\n",
    "  * Use model servers (TorchServe, Triton) that accept batched requests.\n",
    "* For remote API providers:\n",
    "\n",
    "  * Batch multiple inputs into a single API call if supported.\n",
    "  * Implement **rate limiters** and **global token-based queues**.\n",
    "* Use **quantized / distilled embedding models** for throughput on CPU when acceptable (8-bit weights).\n",
    "\n",
    "#### Embedding Validation & Dedup (after embedding)\n",
    "\n",
    "* Compute candidate similarity against a small **ANN index** (FAISS/HNSW) with tuned recall; avoid full pairwise comparisons.\n",
    "* Use incremental indexing: upsert small batches, merge indices offline.\n",
    "\n",
    "#### Vector DB Writes\n",
    "\n",
    "* Use **bulk upserts** and tuned write parallelism.\n",
    "* Prefer vector stores that support sharding and horizontal scaling (Pinecone, Qdrant, Weaviate).\n",
    "* Use async writes with a retry/DLQ mechanism and ensure idempotent upsert keys.\n",
    "\n",
    "#### Orchestration\n",
    "\n",
    "* Use dynamic task mapping (Airflow) or partitioned assets (Dagster) to fan-out chunk processing.\n",
    "* Use task pools & resource quotas to limit concurrent heavy tasks.\n",
    "* Prefer **KubernetesPodOperator** or launching worker containers for GPU work.\n",
    "\n",
    "---\n",
    "\n",
    "### 4 — Architectural patterns\n",
    "\n",
    "#### Micro-batch streaming (recommended)\n",
    "\n",
    "```\n",
    "Producer (S3 crawler / API) → Kafka topic (raw) →\n",
    "Consumer group 1: parsing/cleaning → emits clean docs to `clean` topic →\n",
    "Consumer group 2: chunker/dedup → emits chunk batches to `embed` topic →\n",
    "Embedding pool (GPU pods) consumes embed topic → emits embeddings →\n",
    "Vector DB writer consumes embedding topic → upsert\n",
    "```\n",
    "\n",
    "Advantages: high throughput, backpressure support, independent scaling per stage.\n",
    "\n",
    "#### Batch + reindex pattern\n",
    "\n",
    "* Bulk ingest raw into landing zone → run parallel map jobs (Spark / Dask) to transform → generate embeddings in parallel on GPU cluster → bulk upsert into vector DB.\n",
    "\n",
    "---\n",
    "\n",
    "### 5 — Autoscaling & resource configs (examples)\n",
    "\n",
    "#### Kubernetes HPA (GPU worker)\n",
    "\n",
    "```yaml\n",
    "apiVersion: autoscaling/v2\n",
    "kind: HorizontalPodAutoscaler\n",
    "metadata:\n",
    "  name: embedder-hpa\n",
    "spec:\n",
    "  scaleTargetRef:\n",
    "    apiVersion: apps/v1\n",
    "    kind: Deployment\n",
    "    name: embedder-deployment\n",
    "  minReplicas: 1\n",
    "  maxReplicas: 20\n",
    "  metrics:\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: nvidia.com/gpu\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 70\n",
    "```\n",
    "\n",
    "(Use custom metrics if you track queue length or GPU utilization.)\n",
    "\n",
    "#### Airflow dynamic task mapping (embedding fan-out)\n",
    "\n",
    "```python\n",
    "# pseudo\n",
    "docs = load_docs()\n",
    "mapped = map(lambda batch: embed_batch(batch), docs_in_batches)\n",
    "```\n",
    "\n",
    "Use `max_active_tasks_per_dag` and pools to avoid overprovisioning.\n",
    "\n",
    "---\n",
    "\n",
    "### 6 — Batching, concurrency & sizing guidance\n",
    "\n",
    "* **Batch size** for embedding API: start 32–64; tune for latency/throughput.\n",
    "* **Parallel workers**: start with #workers = (#docs_per_minute * avg_processing_time) / batch_size and iterate.\n",
    "* **Vector DB upsert**: bulk upsert 256–4096 vectors per request depending on provider.\n",
    "* Use **profiling** and adjust batch size to maximize GPU utilization without causing OOM.\n",
    "\n",
    "---\n",
    "\n",
    "### 7 — Cost & performance tradeoffs\n",
    "\n",
    "* Larger batches → better throughput, higher memory usage and latency.\n",
    "* More parallel pods → lower latency but higher cost.\n",
    "* Local models on GPUs → cheaper at high scale vs cloud API per-call costs, but ops overhead higher.\n",
    "* Quantization reduces cost/latency but may reduce quality.\n",
    "\n",
    "---\n",
    "\n",
    "### 8 — Monitoring & SLOs (what to track)\n",
    "\n",
    "* **Throughput**: docs/sec, chunks/sec, embeddings/sec, vectors/sec.\n",
    "* **Latency**: parse latency, embed latency (per batch), upsert latency.\n",
    "* **Queue depth**: Kafka topic lag, task queue length.\n",
    "* **Resource utilization**: GPU/CPU/memory, pod restarts.\n",
    "* **Error rates**: validation fails, embedding fails, upsert fails.\n",
    "* **Duplicate rate** after dedup (helps tune thresholds).\n",
    "  Set SLOs: e.g., 95% of batches processed within X seconds.\n",
    "\n",
    "---\n",
    "\n",
    "### 9 — Large-scale operational tips (1M+ docs)\n",
    "\n",
    "* **Shard data** by tenant/namespace and process shards independently.\n",
    "* Precompute and store **signatures** to avoid reprocessing duplicates across runs.\n",
    "* Use **tiered storage**: hot index for recent docs, cold snapshot for archival.\n",
    "* Implement **lazy reembedding**: only reembed on-read or for high-impact docs when embedding model upgrades.\n",
    "* Use **backfills** for historical reprocessing with controlled rate to avoid overload.\n",
    "\n",
    "---\n",
    "\n",
    "### 10 — Quick code pattern: async batching worker (pseudo-Python)\n",
    "\n",
    "```python\n",
    "from asyncio import Queue, create_task\n",
    "\n",
    "queue = Queue(maxsize=1000)\n",
    "\n",
    "async def ingest_producer(docs):\n",
    "    for d in docs:\n",
    "        await queue.put(d)\n",
    "\n",
    "async def embed_worker(embed_model, out_queue):\n",
    "    batch = []\n",
    "    while True:\n",
    "        d = await queue.get()\n",
    "        batch.append(d)\n",
    "        if len(batch) >= BATCH_SIZE:\n",
    "            emb = embed_model.embed(batch)  # sync or await\n",
    "            await out_queue.put( (batch, emb) )\n",
    "            batch = []\n",
    "\n",
    "# spawn N embed_worker tasks, tune N by GPU count and batch size\n",
    "```\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
