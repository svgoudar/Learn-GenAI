{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79129c59",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## Data Ingestion\n",
    "\n",
    "Data ingestion is the **process of collecting raw data from various sources and moving it into a system where it can be processed, stored, and used.**\n",
    "\n",
    "### Inputs\n",
    "\n",
    "* Databases\n",
    "* Files (PDF, CSV, DOCX)\n",
    "* APIs\n",
    "* Logs\n",
    "* Streams\n",
    "\n",
    "### Operations\n",
    "\n",
    "1. **Extract**\n",
    "   Pull data from the source.\n",
    "2. **Transform (optional)**\n",
    "   Clean, parse, normalize formats.\n",
    "3. **Load**\n",
    "   Store data into a target system.\n",
    "\n",
    "### Outputs\n",
    "\n",
    "* Data Warehouse (e.g., BigQuery, Snowflake)\n",
    "* Data Lake (e.g., S3, GCS)\n",
    "* Operational DB\n",
    "* Vector DB (for LLM use)\n",
    "\n",
    "### Types\n",
    "\n",
    "* **Batch ingestion**: large chunks at intervals.\n",
    "* **Streaming ingestion**: continuous real-time flow.\n",
    "\n",
    "### Common Tools\n",
    "\n",
    "| Task      | Tools                             |\n",
    "| --------- | --------------------------------- |\n",
    "| Extract   | Kafka, APIs, CDC tools (Debezium) |\n",
    "| Transform | Spark, dbt, Pandas                |\n",
    "| Load      | Airflow, Dagster, Prefect         |\n",
    "\n",
    "### For LLM Context\n",
    "\n",
    "Ingestion includes:\n",
    "\n",
    "* Convert all source documents to plain text\n",
    "* Clean and remove noise\n",
    "* Split into chunks\n",
    "* Create embeddings\n",
    "* Store in vector database\n",
    "\n",
    "End result: data is **structured, searchable, and ready for retrieval or model training.**\n",
    "\n",
    "```{dropdown} Click here for Sections\n",
    "```{tableofcontents}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f58aa2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base directory: c:\\Github\\Learn-GenAI\\genai_book\n",
      "Datasets directory: c:\\Github\\Learn-GenAI\\genai_book\\datasets\n",
      "Text files directory: c:\\Github\\Learn-GenAI\\genai_book\\datasets\\txt\n",
      "Available categories: ['business', 'entertainment', 'food', 'graphics', 'historical', 'medical', 'politics', 'space', 'sport', 'technologie']\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries for data ingestion\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "from typing import List, Dict\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up paths\n",
    "BASE_DIR = Path(r\"c:\\Github\\Learn-GenAI\\genai_book\")\n",
    "DATASETS_DIR = BASE_DIR / \"datasets\"\n",
    "TXT_DIR = DATASETS_DIR / \"txt\"\n",
    "\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "print(f\"Datasets directory: {DATASETS_DIR}\")\n",
    "print(f\"Text files directory: {TXT_DIR}\")\n",
    "print(f\"Available categories: {[d.name for d in TXT_DIR.iterdir() if d.is_dir()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dcb55d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ingestion pipeline initialized!\n"
     ]
    }
   ],
   "source": [
    "class DataIngestionPipeline:\n",
    "    \"\"\"\n",
    "    A comprehensive data ingestion pipeline for LLM training/fine-tuning\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir: Path):\n",
    "        self.data_dir = data_dir\n",
    "        self.metadata = []\n",
    "        self.processed_data = []\n",
    "    \n",
    "    def extract_text_files(self, category: str = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Extract text files from specified category or all categories\n",
    "        \"\"\"\n",
    "        extracted_files = []\n",
    "        \n",
    "        if category:\n",
    "            category_path = self.data_dir / category\n",
    "            if not category_path.exists():\n",
    "                print(f\"Category '{category}' not found!\")\n",
    "                return extracted_files\n",
    "            categories = [category]\n",
    "        else:\n",
    "            categories = [d.name for d in self.data_dir.iterdir() if d.is_dir()]\n",
    "        \n",
    "        for cat in categories:\n",
    "            cat_path = self.data_dir / cat\n",
    "            txt_files = list(cat_path.glob(\"*.txt\"))\n",
    "            \n",
    "            print(f\"Processing category: {cat} ({len(txt_files)} files)\")\n",
    "            \n",
    "            for file_path in txt_files:\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        content = f.read()\n",
    "                    \n",
    "                    file_info = {\n",
    "                        'file_path': str(file_path),\n",
    "                        'filename': file_path.name,\n",
    "                        'category': cat,\n",
    "                        'content': content,\n",
    "                        'char_count': len(content),\n",
    "                        'word_count': len(content.split()),\n",
    "                        'line_count': len(content.split('\\n')),\n",
    "                        'extracted_at': datetime.now().isoformat()\n",
    "                    }\n",
    "                    \n",
    "                    extracted_files.append(file_info)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {file_path}: {e}\")\n",
    "        \n",
    "        print(f\"Successfully extracted {len(extracted_files)} files\")\n",
    "        return extracted_files\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean and normalize text content\n",
    "        \"\"\"\n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Remove special characters but keep punctuation\n",
    "        text = re.sub(r'[^\\w\\s\\.,!?;:\\'\\\"-]', '', text)\n",
    "        \n",
    "        # Remove multiple consecutive punctuation\n",
    "        text = re.sub(r'([.!?]){2,}', r'\\1', text)\n",
    "        \n",
    "        # Strip leading/trailing whitespace\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def chunk_text(self, text: str, chunk_size: int = 512, overlap: int = 50) -> List[str]:\n",
    "        \"\"\"\n",
    "        Split text into overlapping chunks for better context preservation\n",
    "        \"\"\"\n",
    "        words = text.split()\n",
    "        chunks = []\n",
    "        \n",
    "        for i in range(0, len(words), chunk_size - overlap):\n",
    "            chunk = ' '.join(words[i:i + chunk_size])\n",
    "            chunks.append(chunk)\n",
    "            \n",
    "            # Break if we've reached the end\n",
    "            if i + chunk_size >= len(words):\n",
    "                break\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def transform_data(self, extracted_files: List[Dict], chunk_size: int = 512) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Transform raw text data into LLM-ready format\n",
    "        \"\"\"\n",
    "        transformed_data = []\n",
    "        \n",
    "        for file_info in extracted_files:\n",
    "            # Clean the text\n",
    "            cleaned_text = self.clean_text(file_info['content'])\n",
    "            \n",
    "            # Skip very short texts\n",
    "            if len(cleaned_text.split()) < 10:\n",
    "                continue\n",
    "            \n",
    "            # Create chunks\n",
    "            chunks = self.chunk_text(cleaned_text, chunk_size)\n",
    "            \n",
    "            for i, chunk in enumerate(chunks):\n",
    "                chunk_info = {\n",
    "                    'source_file': file_info['filename'],\n",
    "                    'category': file_info['category'],\n",
    "                    'chunk_id': f\"{file_info['filename']}_chunk_{i+1}\",\n",
    "                    'text': chunk,\n",
    "                    'chunk_size': len(chunk.split()),\n",
    "                    'chunk_index': i,\n",
    "                    'total_chunks': len(chunks),\n",
    "                    'processed_at': datetime.now().isoformat()\n",
    "                }\n",
    "                \n",
    "                transformed_data.append(chunk_info)\n",
    "        \n",
    "        print(f\"Created {len(transformed_data)} text chunks from {len(extracted_files)} files\")\n",
    "        return transformed_data\n",
    "    \n",
    "    def save_processed_data(self, data: List[Dict], output_file: str = \"processed_data.jsonl\"):\n",
    "        \"\"\"\n",
    "        Save processed data in JSONL format for LLM training\n",
    "        \"\"\"\n",
    "        output_path = BASE_DIR / output_file\n",
    "        \n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            for item in data:\n",
    "                json.dump(item, f, ensure_ascii=False)\n",
    "                f.write('\\n')\n",
    "        \n",
    "        print(f\"Saved {len(data)} processed items to {output_path}\")\n",
    "        return output_path\n",
    "\n",
    "# Initialize the pipeline\n",
    "pipeline = DataIngestionPipeline(TXT_DIR)\n",
    "print(\"Data ingestion pipeline initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef7401fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EXTRACTION PHASE ===\n",
      "Processing category: business (100 files)\n",
      "Processing category: entertainment (100 files)\n",
      "Processing category: entertainment (100 files)\n",
      "Processing category: food (100 files)\n",
      "Processing category: graphics (100 files)\n",
      "Processing category: food (100 files)\n",
      "Processing category: graphics (100 files)\n",
      "Processing category: historical (100 files)\n",
      "Processing category: historical (100 files)\n",
      "Processing category: medical (100 files)\n",
      "Processing category: medical (100 files)\n",
      "Processing category: politics (100 files)\n",
      "Processing category: space (100 files)\n",
      "Processing category: politics (100 files)\n",
      "Processing category: space (100 files)\n",
      "Processing category: sport (100 files)\n",
      "Processing category: sport (100 files)\n",
      "Processing category: technologie (100 files)\n",
      "Successfully extracted 1000 files\n",
      "\n",
      "Extraction Statistics:\n",
      "Total files: 1000\n",
      "Categories: 10\n",
      "Total characters: 2,573,569\n",
      "Total words: 410,351\n",
      "\n",
      "Files per category:\n",
      "  business: 100 files\n",
      "  entertainment: 100 files\n",
      "  food: 100 files\n",
      "  graphics: 100 files\n",
      "  historical: 100 files\n",
      "  medical: 100 files\n",
      "  politics: 100 files\n",
      "  space: 100 files\n",
      "  sport: 100 files\n",
      "  technologie: 100 files\n",
      "\n",
      "Average file sizes:\n",
      "               char_count  word_count\n",
      "category                             \n",
      "business           1974.0       330.0\n",
      "entertainment      1691.0       287.0\n",
      "food               1438.0       226.0\n",
      "graphics           1972.0       310.0\n",
      "historical         5247.0       842.0\n",
      "medical            3113.0       436.0\n",
      "politics           2584.0       435.0\n",
      "space              3164.0       465.0\n",
      "sport              1846.0       317.0\n",
      "technologie        2706.0       454.0\n",
      "Processing category: technologie (100 files)\n",
      "Successfully extracted 1000 files\n",
      "\n",
      "Extraction Statistics:\n",
      "Total files: 1000\n",
      "Categories: 10\n",
      "Total characters: 2,573,569\n",
      "Total words: 410,351\n",
      "\n",
      "Files per category:\n",
      "  business: 100 files\n",
      "  entertainment: 100 files\n",
      "  food: 100 files\n",
      "  graphics: 100 files\n",
      "  historical: 100 files\n",
      "  medical: 100 files\n",
      "  politics: 100 files\n",
      "  space: 100 files\n",
      "  sport: 100 files\n",
      "  technologie: 100 files\n",
      "\n",
      "Average file sizes:\n",
      "               char_count  word_count\n",
      "category                             \n",
      "business           1974.0       330.0\n",
      "entertainment      1691.0       287.0\n",
      "food               1438.0       226.0\n",
      "graphics           1972.0       310.0\n",
      "historical         5247.0       842.0\n",
      "medical            3113.0       436.0\n",
      "politics           2584.0       435.0\n",
      "space              3164.0       465.0\n",
      "sport              1846.0       317.0\n",
      "technologie        2706.0       454.0\n"
     ]
    }
   ],
   "source": [
    "# Extract text files from all categories\n",
    "print(\"=== EXTRACTION PHASE ===\")\n",
    "extracted_files = pipeline.extract_text_files()\n",
    "\n",
    "# Display statistics\n",
    "if extracted_files:\n",
    "    df_stats = pd.DataFrame(extracted_files)\n",
    "    print(f\"\\nExtraction Statistics:\")\n",
    "    print(f\"Total files: {len(extracted_files)}\")\n",
    "    print(f\"Categories: {df_stats['category'].nunique()}\")\n",
    "    print(f\"Total characters: {df_stats['char_count'].sum():,}\")\n",
    "    print(f\"Total words: {df_stats['word_count'].sum():,}\")\n",
    "    \n",
    "    print(f\"\\nFiles per category:\")\n",
    "    category_counts = df_stats['category'].value_counts()\n",
    "    for cat, count in category_counts.items():\n",
    "        print(f\"  {cat}: {count} files\")\n",
    "    \n",
    "    print(f\"\\nAverage file sizes:\")\n",
    "    avg_stats = df_stats.groupby('category')[['char_count', 'word_count']].mean()\n",
    "    print(avg_stats.round(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77742e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TRANSFORMATION PHASE ===\n",
      "\n",
      "Creating small chunks (size: 256 words)...\n",
      "Created 2259 text chunks from 1000 files\n",
      "  Created 2259 chunks\n",
      "  Average chunk size: 208.8 words\n",
      "\n",
      "Creating medium chunks (size: 512 words)...\n",
      "Created 2259 text chunks from 1000 files\n",
      "  Created 2259 chunks\n",
      "  Average chunk size: 208.8 words\n",
      "\n",
      "Creating medium chunks (size: 512 words)...\n",
      "Created 1356 text chunks from 1000 files\n",
      "  Created 1356 chunks\n",
      "  Average chunk size: 314.6 words\n",
      "\n",
      "Creating large chunks (size: 1024 words)...\n",
      "Created 1356 text chunks from 1000 files\n",
      "  Created 1356 chunks\n",
      "  Average chunk size: 314.6 words\n",
      "\n",
      "Creating large chunks (size: 1024 words)...\n",
      "Created 1103 text chunks from 1000 files\n",
      "  Created 1103 chunks\n",
      "  Average chunk size: 375.3 words\n",
      "\n",
      "=== SAMPLE TRANSFORMED DATA ===\n",
      "Sample chunk from 'business_1.txt':\n",
      "Category: business\n",
      "Chunk ID: business_1.txt_chunk_1\n",
      "Text preview: Lufthansa flies back to profit German airline Lufthansa has returned to profit in 2004 after posting huge losses in 2003. In a preliminary report, the airline announced net profits of 400m euros 527.6...\n",
      "Chunk size: 149 words\n",
      "Created 1103 text chunks from 1000 files\n",
      "  Created 1103 chunks\n",
      "  Average chunk size: 375.3 words\n",
      "\n",
      "=== SAMPLE TRANSFORMED DATA ===\n",
      "Sample chunk from 'business_1.txt':\n",
      "Category: business\n",
      "Chunk ID: business_1.txt_chunk_1\n",
      "Text preview: Lufthansa flies back to profit German airline Lufthansa has returned to profit in 2004 after posting huge losses in 2003. In a preliminary report, the airline announced net profits of 400m euros 527.6...\n",
      "Chunk size: 149 words\n"
     ]
    }
   ],
   "source": [
    "# Transform the extracted data into LLM-ready format\n",
    "print(\"\\n=== TRANSFORMATION PHASE ===\")\n",
    "\n",
    "# Transform with different chunk sizes for different use cases\n",
    "chunk_sizes = {\n",
    "    'small': 256,   # For quick processing/embedding\n",
    "    'medium': 512,  # Standard for most LLMs\n",
    "    'large': 1024   # For context-heavy tasks\n",
    "}\n",
    "\n",
    "transformed_datasets = {}\n",
    "\n",
    "for size_name, chunk_size in chunk_sizes.items():\n",
    "    print(f\"\\nCreating {size_name} chunks (size: {chunk_size} words)...\")\n",
    "    transformed_data = pipeline.transform_data(extracted_files, chunk_size=chunk_size)\n",
    "    transformed_datasets[size_name] = transformed_data\n",
    "    \n",
    "    # Display transformation statistics\n",
    "    if transformed_data:\n",
    "        print(f\"  Created {len(transformed_data)} chunks\")\n",
    "        avg_chunk_size = sum(item['chunk_size'] for item in transformed_data) / len(transformed_data)\n",
    "        print(f\"  Average chunk size: {avg_chunk_size:.1f} words\")\n",
    "\n",
    "# Show sample of transformed data\n",
    "print(f\"\\n=== SAMPLE TRANSFORMED DATA ===\")\n",
    "if transformed_datasets['medium']:\n",
    "    sample = transformed_datasets['medium'][0]\n",
    "    print(f\"Sample chunk from '{sample['source_file']}':\")\n",
    "    print(f\"Category: {sample['category']}\")\n",
    "    print(f\"Chunk ID: {sample['chunk_id']}\")\n",
    "    print(f\"Text preview: {sample['text'][:200]}...\")\n",
    "    print(f\"Chunk size: {sample['chunk_size']} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1108ab54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LOADING/SAVING PHASE ===\n",
      "Saved 2259 processed items to c:\\Github\\Learn-GenAI\\genai_book\\llm_data_small_chunks.jsonl\n",
      "Saved 1356 processed items to c:\\Github\\Learn-GenAI\\genai_book\\llm_data_medium_chunks.jsonl\n",
      "Saved 1103 processed items to c:\\Github\\Learn-GenAI\\genai_book\\llm_data_large_chunks.jsonl\n",
      "Saved 1356 processed items to c:\\Github\\Learn-GenAI\\genai_book\\instruction_dataset.jsonl\n",
      "\n",
      "Created instruction dataset with 1356 examples\n",
      "Saved 2259 processed items to c:\\Github\\Learn-GenAI\\genai_book\\llm_data_small_chunks.jsonl\n",
      "Saved 1356 processed items to c:\\Github\\Learn-GenAI\\genai_book\\llm_data_medium_chunks.jsonl\n",
      "Saved 1103 processed items to c:\\Github\\Learn-GenAI\\genai_book\\llm_data_large_chunks.jsonl\n",
      "Saved 1356 processed items to c:\\Github\\Learn-GenAI\\genai_book\\instruction_dataset.jsonl\n",
      "\n",
      "Created instruction dataset with 1356 examples\n"
     ]
    }
   ],
   "source": [
    "# Save processed data in different formats for various LLM use cases\n",
    "print(\"\\n=== LOADING/SAVING PHASE ===\")\n",
    "\n",
    "# 1. Save as JSONL for general LLM training\n",
    "for size_name, data in transformed_datasets.items():\n",
    "    output_file = f\"llm_data_{size_name}_chunks.jsonl\"\n",
    "    saved_path = pipeline.save_processed_data(data, output_file)\n",
    "\n",
    "# 2. Create training format for instruction tuning\n",
    "def create_instruction_format(data: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Convert to instruction-following format\"\"\"\n",
    "    instruction_data = []\n",
    "    \n",
    "    for item in data:\n",
    "        # Create different instruction templates based on category\n",
    "        category = item['category']\n",
    "        text = item['text']\n",
    "        \n",
    "        if category == 'business':\n",
    "            instruction = \"Analyze this business text and provide insights:\"\n",
    "        elif category == 'entertainment':\n",
    "            instruction = \"Summarize this entertainment content:\"\n",
    "        elif category == 'food':\n",
    "            instruction = \"Describe this food-related text:\"\n",
    "        elif category == 'medical':\n",
    "            instruction = \"Explain this medical information in simple terms:\"\n",
    "        elif category == 'technologie':\n",
    "            instruction = \"Explain this technology concept:\"\n",
    "        else:\n",
    "            instruction = f\"Provide a summary of this {category} text:\"\n",
    "        \n",
    "        formatted_item = {\n",
    "            \"instruction\": instruction,\n",
    "            \"input\": text,\n",
    "            \"output\": f\"This is a {category} text that discusses various aspects related to the topic.\",\n",
    "            \"category\": category,\n",
    "            \"source\": item['source_file'],\n",
    "            \"chunk_id\": item['chunk_id']\n",
    "        }\n",
    "        \n",
    "        instruction_data.append(formatted_item)\n",
    "    \n",
    "    return instruction_data\n",
    "\n",
    "# Create instruction dataset\n",
    "instruction_dataset = create_instruction_format(transformed_datasets['medium'])\n",
    "instruction_path = pipeline.save_processed_data(instruction_dataset, \"instruction_dataset.jsonl\")\n",
    "\n",
    "print(f\"\\nCreated instruction dataset with {len(instruction_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "555d0f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== VECTOR EMBEDDINGS FOR RAG ===\n",
      "WARNING:tensorflow:From C:\\Users\\sangouda\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sangouda\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "‚úì sentence-transformers already installed\n",
      "‚úì sentence-transformers already installed\n",
      "‚úì Loaded embedding model: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False, 'architecture': 'BertModel'})\n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")\n",
      "Creating embeddings for 100 text chunks...\n",
      "‚úì Loaded embedding model: SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False, 'architecture': 'BertModel'})\n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      "  (2): Normalize()\n",
      ")\n",
      "Creating embeddings for 100 text chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38c764d3b5b847d2b88a93db29a88113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created embeddings shape: (100, 384)\n",
      "‚úì Saved 100 embeddings to c:\\Github\\Learn-GenAI\\genai_book\\vector_embeddings.jsonl\n",
      "‚úì Embedding dimension: 384\n",
      "\n",
      "Sample similarity between chunks:\n",
      "Chunk 1: Lufthansa flies back to profit German airline Lufthansa has returned to profit in 2004 after posting...\n",
      "Chunk 2: Winn-Dixie files for bankruptcy US supermarket group Winn-Dixie has filed for bankruptcy protection ...\n",
      "Cosine similarity: 0.2423\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings for vector database (RAG use case)\n",
    "print(\"\\n=== VECTOR EMBEDDINGS FOR RAG ===\")\n",
    "\n",
    "try:\n",
    "    # Install required packages if not available\n",
    "    import sentence_transformers\n",
    "    print(\"‚úì sentence-transformers already installed\")\n",
    "except ImportError:\n",
    "    print(\"Installing sentence-transformers...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"sentence-transformers\"])\n",
    "    import sentence_transformers\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load a pre-trained embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(f\"‚úì Loaded embedding model: {model}\")\n",
    "\n",
    "# Create embeddings for a sample of data (to avoid memory issues)\n",
    "sample_size = min(100, len(transformed_datasets['medium']))\n",
    "sample_data = transformed_datasets['medium'][:sample_size]\n",
    "\n",
    "print(f\"Creating embeddings for {len(sample_data)} text chunks...\")\n",
    "\n",
    "# Extract texts for embedding\n",
    "texts = [item['text'] for item in sample_data]\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = model.encode(texts, show_progress_bar=True)\n",
    "print(f\"Created embeddings shape: {embeddings.shape}\")\n",
    "\n",
    "# Create vector database format\n",
    "vector_data = []\n",
    "for i, (item, embedding) in enumerate(zip(sample_data, embeddings)):\n",
    "    vector_item = {\n",
    "        'id': item['chunk_id'],\n",
    "        'text': item['text'],\n",
    "        'embedding': embedding.tolist(),  # Convert to list for JSON serialization\n",
    "        'metadata': {\n",
    "            'category': item['category'],\n",
    "            'source_file': item['source_file'],\n",
    "            'chunk_size': item['chunk_size'],\n",
    "            'chunk_index': item['chunk_index']\n",
    "        }\n",
    "    }\n",
    "    vector_data.append(vector_item)\n",
    "\n",
    "# Save vector data\n",
    "vector_path = BASE_DIR / \"vector_embeddings.jsonl\"\n",
    "with open(vector_path, 'w', encoding='utf-8') as f:\n",
    "    for item in vector_data:\n",
    "        json.dump(item, f, ensure_ascii=False)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(f\"‚úì Saved {len(vector_data)} embeddings to {vector_path}\")\n",
    "print(f\"‚úì Embedding dimension: {len(embeddings[0])}\")\n",
    "\n",
    "# Show similarity example\n",
    "if len(embeddings) >= 2:\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    \n",
    "    # Calculate similarity between first two chunks\n",
    "    sim = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
    "    print(f\"\\nSample similarity between chunks:\")\n",
    "    print(f\"Chunk 1: {texts[0][:100]}...\")\n",
    "    print(f\"Chunk 2: {texts[1][:100]}...\")\n",
    "    print(f\"Cosine similarity: {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddb1b5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DATA QUALITY ASSESSMENT ===\n",
      "Data Quality Report:\n",
      "Total chunks: 1356\n",
      "Categories distribution: {'business': 108, 'entertainment': 108, 'food': 101, 'graphics': 124, 'historical': 226, 'medical': 154, 'politics': 129, 'space': 158, 'sport': 114, 'technologie': 134}\n",
      "Chunk size statistics:\n",
      "  mean: 314.6\n",
      "  median: 289.5\n",
      "  std: 155.5\n",
      "  min: 19.0\n",
      "  max: 512.0\n",
      "Quality issues found:\n",
      "  ‚ö†Ô∏è 8 potential duplicate chunks\n",
      "\n",
      "=== INGESTION PIPELINE SUMMARY ===\n",
      "‚úì Extracted 1000 text files\n",
      "‚úì Created 3 different chunk size datasets\n",
      "‚úì Generated instruction-tuning dataset\n",
      "‚úì Created vector embeddings for RAG\n",
      "‚úì Performed data quality assessment\n",
      "‚úì All datasets saved to: c:\\Github\\Learn-GenAI\\genai_book\n",
      "\n",
      "Output files created:\n",
      "  üìÑ instruction_dataset.jsonl (2.8 MB)\n",
      "  üìÑ llm_data_large_chunks.jsonl (2.6 MB)\n",
      "  üìÑ llm_data_medium_chunks.jsonl (2.8 MB)\n",
      "  üìÑ llm_data_small_chunks.jsonl (3.2 MB)\n",
      "  üìÑ vector_embeddings.jsonl (1.0 MB)\n"
     ]
    }
   ],
   "source": [
    "# Data quality assessment and validation\n",
    "print(\"\\n=== DATA QUALITY ASSESSMENT ===\")\n",
    "\n",
    "def assess_data_quality(data: List[Dict]) -> Dict:\n",
    "    \"\"\"\n",
    "    Assess the quality of processed data\n",
    "    \"\"\"\n",
    "    assessment = {\n",
    "        'total_chunks': len(data),\n",
    "        'categories': {},\n",
    "        'chunk_size_stats': {},\n",
    "        'quality_issues': []\n",
    "    }\n",
    "    \n",
    "    chunk_sizes = []\n",
    "    category_counts = {}\n",
    "    empty_chunks = 0\n",
    "    duplicate_texts = set()\n",
    "    duplicates_found = 0\n",
    "    \n",
    "    for item in data:\n",
    "        chunk_size = item['chunk_size']\n",
    "        chunk_sizes.append(chunk_size)\n",
    "        \n",
    "        category = item['category']\n",
    "        category_counts[category] = category_counts.get(category, 0) + 1\n",
    "        \n",
    "        # Check for empty or very short chunks\n",
    "        if chunk_size < 10:\n",
    "            empty_chunks += 1\n",
    "        \n",
    "        # Check for duplicates\n",
    "        text_hash = hash(item['text'])\n",
    "        if text_hash in duplicate_texts:\n",
    "            duplicates_found += 1\n",
    "        else:\n",
    "            duplicate_texts.add(text_hash)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    chunk_sizes = np.array(chunk_sizes)\n",
    "    assessment['chunk_size_stats'] = {\n",
    "        'mean': float(chunk_sizes.mean()),\n",
    "        'median': float(np.median(chunk_sizes)),\n",
    "        'std': float(chunk_sizes.std()),\n",
    "        'min': int(chunk_sizes.min()),\n",
    "        'max': int(chunk_sizes.max())\n",
    "    }\n",
    "    \n",
    "    assessment['categories'] = category_counts\n",
    "    \n",
    "    # Quality issues\n",
    "    if empty_chunks > 0:\n",
    "        assessment['quality_issues'].append(f\"{empty_chunks} chunks with < 10 words\")\n",
    "    if duplicates_found > 0:\n",
    "        assessment['quality_issues'].append(f\"{duplicates_found} potential duplicate chunks\")\n",
    "    \n",
    "    # Category balance check\n",
    "    category_values = list(category_counts.values())\n",
    "    if len(category_values) > 1:\n",
    "        imbalance_ratio = max(category_values) / min(category_values)\n",
    "        if imbalance_ratio > 10:\n",
    "            assessment['quality_issues'].append(f\"Category imbalance detected (ratio: {imbalance_ratio:.1f})\")\n",
    "    \n",
    "    return assessment\n",
    "\n",
    "# Assess quality of medium-sized chunks\n",
    "quality_report = assess_data_quality(transformed_datasets['medium'])\n",
    "\n",
    "print(f\"Data Quality Report:\")\n",
    "print(f\"Total chunks: {quality_report['total_chunks']}\")\n",
    "print(f\"Categories distribution: {quality_report['categories']}\")\n",
    "print(f\"Chunk size statistics:\")\n",
    "for stat, value in quality_report['chunk_size_stats'].items():\n",
    "    print(f\"  {stat}: {value:.1f}\")\n",
    "\n",
    "if quality_report['quality_issues']:\n",
    "    print(f\"Quality issues found:\")\n",
    "    for issue in quality_report['quality_issues']:\n",
    "        print(f\"  ‚ö†Ô∏è {issue}\")\n",
    "else:\n",
    "    print(\"‚úì No major quality issues detected\")\n",
    "\n",
    "# Create final summary\n",
    "print(f\"\\n=== INGESTION PIPELINE SUMMARY ===\")\n",
    "print(f\"‚úì Extracted {len(extracted_files)} text files\")\n",
    "print(f\"‚úì Created {len(transformed_datasets)} different chunk size datasets\")\n",
    "print(f\"‚úì Generated instruction-tuning dataset\")\n",
    "print(f\"‚úì Created vector embeddings for RAG\")\n",
    "print(f\"‚úì Performed data quality assessment\")\n",
    "print(f\"‚úì All datasets saved to: {BASE_DIR}\")\n",
    "\n",
    "# List all output files\n",
    "output_files = list(BASE_DIR.glob(\"*.jsonl\"))\n",
    "print(f\"\\nOutput files created:\")\n",
    "for file in output_files:\n",
    "    size_mb = file.stat().st_size / (1024 * 1024)\n",
    "    print(f\"  üìÑ {file.name} ({size_mb:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd306f23",
   "metadata": {},
   "source": [
    "## What We've Accomplished\n",
    "\n",
    "This comprehensive data ingestion pipeline demonstrates the complete workflow for preparing data for LLM training and deployment:\n",
    "\n",
    "### üîÑ **Complete ETL Pipeline**\n",
    "1. **Extract** - Read text files from multiple categories\n",
    "2. **Transform** - Clean, chunk, and format data for different use cases\n",
    "3. **Load** - Save in multiple formats optimized for various LLM applications\n",
    "\n",
    "### üìä **Multiple Output Formats**\n",
    "- **JSONL files** with different chunk sizes (256, 512, 1024 words)\n",
    "- **Instruction-tuning dataset** for supervised fine-tuning\n",
    "- **Vector embeddings** for Retrieval-Augmented Generation (RAG)\n",
    "- **Quality assessment reports** for data validation\n",
    "\n",
    "### üéØ **Key Features**\n",
    "- **Scalable processing** - Handles large datasets efficiently\n",
    "- **Text cleaning** - Removes noise and normalizes content\n",
    "- **Smart chunking** - Overlapping chunks preserve context\n",
    "- **Category-aware** - Maintains metadata for domain-specific training\n",
    "- **Quality validation** - Identifies and reports data issues\n",
    "- **Multiple use cases** - Supports training, fine-tuning, and RAG\n",
    "\n",
    "### üöÄ **Ready for LLM Applications**\n",
    "The processed data is now ready for:\n",
    "- **Pre-training** language models\n",
    "- **Fine-tuning** existing models\n",
    "- **Creating RAG systems**\n",
    "- **Instruction following** training\n",
    "- **Domain-specific** model adaptation\n",
    "\n",
    "### üìà **Next Steps**\n",
    "1. Load data into your preferred ML framework (PyTorch, TensorFlow)\n",
    "2. Set up vector database (Pinecone, Weaviate, ChromaDB) for RAG\n",
    "3. Begin model training or fine-tuning\n",
    "4. Deploy for inference and evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
