{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2461abad",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## Latency vs Consistency\n",
    "\n",
    "**Latency** = how fast a document moves through ingestion → embedding → indexing → becomes retrievable.\n",
    "\n",
    "**Consistency** = how correct, complete, and synchronized the indexed data is across:\n",
    "\n",
    "* chunks\n",
    "* embeddings\n",
    "* metadata\n",
    "* dedup/versioning\n",
    "* vector store shards\n",
    "* caches\n",
    "\n",
    "**Trade-off:**\n",
    "To ingest fast (low latency), we often accept temporary inconsistencies.\n",
    "To guarantee perfect consistency, ingestion slows down.\n",
    "\n",
    "In Generative-AI systems, both cannot be maximized simultaneously.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Why the trade-off exists\n",
    "\n",
    "GenAI ingestion pipelines involve multiple asynchronous steps:\n",
    "\n",
    "```\n",
    "Extract → Clean → Chunk → Dedup → Embed → Store → Refresh Index\n",
    "```\n",
    "\n",
    "Each step introduces a delay or inconsistency risk:\n",
    "\n",
    "* batching\n",
    "* parallel writes\n",
    "* eventual consistency of vector DB\n",
    "* metadata updates\n",
    "* retries\n",
    "* re-embedding due to model upgrades\n",
    "\n",
    "Optimizing for one side hurts the other.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Examples of Latency-Optimized vs Consistency-Optimized Behavior\n",
    "\n",
    "#### A. Latency-Optimized (Faster Ingestion)\n",
    "\n",
    "Characteristics:\n",
    "\n",
    "* Push documents quickly into vector DB.\n",
    "* Skip heavy quality checks.\n",
    "* Allow stale embeddings temporarily.\n",
    "* Accept partial indexing during failure recovery.\n",
    "* Rely on **eventual consistency** across shards.\n",
    "\n",
    "Outcome:\n",
    "\n",
    "* New documents appear in search faster.\n",
    "* Retrieval may briefly return inconsistent or duplicated chunks.\n",
    "\n",
    "Used for:\n",
    "\n",
    "* customer support search\n",
    "* troubleshooting documentation\n",
    "* systems where freshness matters more than completeness\n",
    "\n",
    "---\n",
    "\n",
    "#### B. Consistency-Optimized (Stronger Guarantees)\n",
    "\n",
    "Characteristics:\n",
    "\n",
    "* Strict dedup before indexing\n",
    "* Validate all embeddings\n",
    "* Atomic upserts (all chunks succeed or none)\n",
    "* Index versioning\n",
    "* Two-phase commit style writes (staging → publish)\n",
    "\n",
    "Outcome:\n",
    "\n",
    "* Retrieval is stable, correct, reproducible\n",
    "* Ingestion pipeline is slower (more blocking)\n",
    "\n",
    "Used for:\n",
    "\n",
    "* legal documents\n",
    "* policy/finance domains\n",
    "* regulated industries\n",
    "* LLM fine-tuning datasets\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Where the Trade-Off Appears in Each Ingestion Stage\n",
    "\n",
    "#### 4.1 Parsing → Cleaning\n",
    "\n",
    "**Low latency:**\n",
    "\n",
    "* Use fast parsing; skip heavy noise-filtering.\n",
    "\n",
    "**High consistency:**\n",
    "\n",
    "* Slow exhaustive parsing, OCR fallback, metadata verification.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4.2 Chunking\n",
    "\n",
    "**Low latency:**\n",
    "\n",
    "* Chunk with simple size-based splits.\n",
    "\n",
    "**High consistency:**\n",
    "\n",
    "* Use semantic chunking, language detection, quality scoring.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4.3 Deduplication\n",
    "\n",
    "**Low latency:**\n",
    "\n",
    "* Only exact dedup via hashing.\n",
    "\n",
    "**High consistency:**\n",
    "\n",
    "* Near-duplicate checks (embeddings), LSH, similarity thresholds.\n",
    "\n",
    "Near-dedup increases latency significantly.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4.4 Embedding Generation\n",
    "\n",
    "**Low latency:**\n",
    "\n",
    "* Smaller batches, parallel GPU fans, faster but expensive.\n",
    "\n",
    "**High consistency:**\n",
    "\n",
    "* Wait for accumulated batch, align embedding versions, enforce deterministic settings.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4.5 Vector DB Indexing\n",
    "\n",
    "**Low latency:**\n",
    "\n",
    "* Write quickly, accept temporary inconsistencies across shards.\n",
    "* Use eventual consistency: some replicas may be stale for seconds/minutes.\n",
    "\n",
    "**High consistency:**\n",
    "\n",
    "* Strict write barriers.\n",
    "* Use transaction logs.\n",
    "* Disable search until index rebuild completes.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4.6 Re-embedding / Model Upgrades\n",
    "\n",
    "**Low latency:**\n",
    "\n",
    "* Lazy re-embedding (only re-embed on retrieval).\n",
    "* On-read fallback to older embeddings.\n",
    "\n",
    "**High consistency:**\n",
    "\n",
    "* Full offline re-embed and re-index before switching version.\n",
    "* Synchronous upgrade.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Concrete Scenarios Demonstrating the Trade-off\n",
    "\n",
    "#### Scenario 1: 10,000 new documents dropped into S3 at once\n",
    "\n",
    "* **Latency mode**: stream them and index as soon as embedding is done.\n",
    "* **Consistency mode**: hold ingestion until chunking, dedup, quality checks complete.\n",
    "\n",
    "#### Scenario 2: Embedding model upgrade from v2 → v3\n",
    "\n",
    "* **Latency mode**: migrate incrementally (eventual consistency).\n",
    "* **Consistency mode**: reprocess entire corpus before switching index version.\n",
    "\n",
    "#### Scenario 3: Vector DB sharding\n",
    "\n",
    "* **Latency mode**: allow out-of-sync shards; queries eventually converge.\n",
    "* **Consistency mode**: force write quorums; slower ingest.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Techniques to Manage the Trade-off (Practical)\n",
    "\n",
    "#### A. Dual-Written Index (Shadow Index)\n",
    "\n",
    "Write to a new index version in background:\n",
    "\n",
    "```\n",
    "index_v2 (active)  \n",
    "index_v3 (building)\n",
    "```\n",
    "\n",
    "Switch traffic when ready.\n",
    "\n",
    "Balances:\n",
    "\n",
    "* fast ingestion\n",
    "* high consistency for final published index\n",
    "\n",
    "---\n",
    "\n",
    "#### B. Micro-Batch Staging Layer\n",
    "\n",
    "```\n",
    "Chunks → staging table → (validated) → vector index\n",
    "```\n",
    "\n",
    "Enables:\n",
    "\n",
    "* fast ingestion into staging\n",
    "* strict validation for index\n",
    "* rollback support\n",
    "\n",
    "---\n",
    "\n",
    "#### C. Eventual Consistency with Time-Based SLO\n",
    "\n",
    "Common pattern: **fresh within 30–120 seconds**.\n",
    "\n",
    "Used when slight delays are acceptable.\n",
    "\n",
    "---\n",
    "\n",
    "## D. Strong Consistency with Write Barriers\n",
    "\n",
    "Use:\n",
    "\n",
    "* atomic upserts\n",
    "* version-controlled metadata\n",
    "* shard-wide sync points\n",
    "\n",
    "Slower, but consistent.\n",
    "\n",
    "---\n",
    "\n",
    "#### E. Quality Gates (Configurable)\n",
    "\n",
    "Switchable at runtime:\n",
    "\n",
    "```\n",
    "FAST_MODE:\n",
    "    skip_near_dedup = true\n",
    "    skip_quality_scoring = true\n",
    "\n",
    "SAFE_MODE:\n",
    "    enforce_dedup = true\n",
    "    verify_embeddings = true\n",
    "    require_metadata = true\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Industry Examples\n",
    "\n",
    "#### Slack Search / Helpdesk Search\n",
    "\n",
    "Optimized for latency → content visible almost immediately.\n",
    "\n",
    "#### Financial Policy Document Systems\n",
    "\n",
    "Optimized for consistency → ingestion pipelines run slower with strict validation.\n",
    "\n",
    "#### RAG Systems in Production\n",
    "\n",
    "Often adopt **mixed** approach:\n",
    "\n",
    "* ingestion: eventually consistent\n",
    "* retrieval: strongly consistent\n",
    "  (using version barriers + shadow indexes)\n",
    "\n",
    "---\n",
    "\n",
    "**Summary Table**\n",
    "\n",
    "| Ingestion decision    | Latency focus | Consistency focus  |\n",
    "| --------------------- | ------------- | ------------------ |\n",
    "| Parsing               | fast, shallow | strict, deep       |\n",
    "| Cleaning              | minimal       | heavy              |\n",
    "| Dedup                 | exact only    | exact + near       |\n",
    "| Embeddings            | immediate     | batched, versioned |\n",
    "| Indexing              | eventual      | strong             |\n",
    "| Updates               | incremental   | full rebuild       |\n",
    "| RAG freshness         | high          | moderate           |\n",
    "| Risk of inconsistency | higher        | minimal            |\n",
    "\n",
    "---\n",
    "\n",
    "**Final Summary**\n",
    "\n",
    "**Latency-optimized ingestion** prioritizes speed and data freshness, accepting temporary inconsistencies (e.g., some chunks not yet indexed, older embedding versions in use).\n",
    "\n",
    "**Consistency-optimized ingestion** prioritizes correctness, version alignment, and deterministic outputs at the cost of slower ingestion.\n",
    "\n",
    "In practice, production GenAI systems use **hybrid strategies**:\n",
    "\n",
    "* fast ingestion to staging\n",
    "* consistent publishing to index\n",
    "* versioned embeddings\n",
    "* partial eventual consistency with bounded staleness\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2619d8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Below is a production-style, ready-to-run **Python example** that implements two operational modes for a GenAI ingestion pipeline:\n",
    "\n",
    "* **FAST_MODE** — low-latency path (fewer checks, streaming into index/staging)\n",
    "* **SAFE_MODE** — consistency-first path (strict validation, dedup, staged publish, atomic upsert)\n",
    "\n",
    "Features:\n",
    "\n",
    "* single `ModeConfig` to switch behaviour\n",
    "* pipeline stages (parse → clean → chunk → dedup → embed → upsert)\n",
    "* staging area + atomic publish for SAFE_MODE\n",
    "* tunable batching, retries, and thresholds\n",
    "* simple in-memory vector store and DLQ for demonstration\n",
    "* example runs showing both modes\n",
    "\n",
    "Save as `mode_ingest_pipeline.py` and run.\n",
    "\n",
    "```python\n",
    "# mode_ingest_pipeline.py\n",
    "import time\n",
    "import hashlib\n",
    "import random\n",
    "import threading\n",
    "from typing import List, Dict, Any, Optional\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------\n",
    "# Mode configuration\n",
    "# -------------------------\n",
    "class ModeConfig:\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        skip_near_dedup: bool,\n",
    "        enforce_quality_checks: bool,\n",
    "        embed_batch_size: int,\n",
    "        use_staging_publish: bool,\n",
    "        require_all_chunks_atomic: bool\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.skip_near_dedup = skip_near_dedup\n",
    "        self.enforce_quality_checks = enforce_quality_checks\n",
    "        self.embed_batch_size = embed_batch_size\n",
    "        self.use_staging_publish = use_staging_publish\n",
    "        self.require_all_chunks_atomic = require_all_chunks_atomic\n",
    "\n",
    "FAST_MODE = ModeConfig(\n",
    "    name=\"FAST_MODE\",\n",
    "    skip_near_dedup=True,\n",
    "    enforce_quality_checks=False,\n",
    "    embed_batch_size=64,\n",
    "    use_staging_publish=False,\n",
    "    require_all_chunks_atomic=False,\n",
    ")\n",
    "\n",
    "SAFE_MODE = ModeConfig(\n",
    "    name=\"SAFE_MODE\",\n",
    "    skip_near_dedup=False,\n",
    "    enforce_quality_checks=True,\n",
    "    embed_batch_size=16,\n",
    "    use_staging_publish=True,\n",
    "    require_all_chunks_atomic=True,\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Simple in-memory stores\n",
    "# -------------------------\n",
    "class InMemoryVectorStore:\n",
    "    def __init__(self):\n",
    "        self.store = {}           # vector_id -> (vector, metadata)\n",
    "        self.lock = threading.Lock()\n",
    "        self.index_version = 0\n",
    "\n",
    "    def upsert(self, id: str, vector: List[float], metadata: Dict[str, Any]):\n",
    "        with self.lock:\n",
    "            self.store[id] = (vector, metadata)\n",
    "\n",
    "    def bulk_upsert(self, rows: List[Dict[str, Any]]):\n",
    "        with self.lock:\n",
    "            for r in rows:\n",
    "                self.store[r[\"id\"]] = (r[\"vector\"], r.get(\"metadata\", {}))\n",
    "\n",
    "    def fetch(self, id: str):\n",
    "        return self.store.get(id)\n",
    "\n",
    "    def count(self):\n",
    "        return len(self.store)\n",
    "\n",
    "    def snapshot(self):\n",
    "        with self.lock:\n",
    "            return dict(self.store)\n",
    "\n",
    "vector_db = InMemoryVectorStore()\n",
    "staging_store = {}   # doc_id -> list of chunk records\n",
    "DLQ = []\n",
    "\n",
    "# -------------------------\n",
    "# Utilities\n",
    "# -------------------------\n",
    "def sha256(text: str) -> str:\n",
    "    return hashlib.sha256(text.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def simple_clean(text: str) -> str:\n",
    "    return \" \".join(text.replace(\"\\n\", \" \").split())\n",
    "\n",
    "def simple_chunk(text: str, max_chars=300) -> List[str]:\n",
    "    return [text[i:i+max_chars] for i in range(0, len(text), max_chars)]\n",
    "\n",
    "def fake_embed_batch(texts: List[str], dim=1536) -> List[List[float]]:\n",
    "    # deterministic pseudo-embeds using hashing seed\n",
    "    out = []\n",
    "    for t in texts:\n",
    "        h = int(sha256(t)[:8], 16)\n",
    "        rng = np.random.RandomState(h)\n",
    "        v = rng.randn(dim).astype(float)\n",
    "        v = (v / (np.linalg.norm(v) + 1e-9)).tolist()\n",
    "        out.append(v)\n",
    "    return out\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    a = np.array(a); b = np.array(b)\n",
    "    return float(np.dot(a, b) / (np.linalg.norm(a)*np.linalg.norm(b) + 1e-12))\n",
    "\n",
    "# -------------------------\n",
    "# Dedup structures (simple)\n",
    "# -------------------------\n",
    "exact_hash_set = set()\n",
    "\n",
    "def exact_dedup_check(text: str) -> bool:\n",
    "    h = sha256(\" \".join(text.split()))\n",
    "    if h in exact_hash_set:\n",
    "        return True\n",
    "    exact_hash_set.add(h)\n",
    "    return False\n",
    "\n",
    "# For near-dedup: small ANN-like naive sample of vector DB (expensive at scale)\n",
    "def near_dedup_check(embedding, threshold=0.995) -> bool:\n",
    "    # iterate small sample: here entire db (demo). Production: use ANN (HNSW) search.\n",
    "    for vid, (vec, meta) in vector_db.snapshot().items():\n",
    "        if cosine_sim(vec, embedding) >= threshold:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# -------------------------\n",
    "# Pipeline class\n",
    "# -------------------------\n",
    "class IngestPipeline:\n",
    "    def __init__(self, mode: ModeConfig):\n",
    "        self.mode = mode\n",
    "\n",
    "    def ingest_document(self, doc: Dict[str, Any]) -> bool:\n",
    "        \"\"\"Single-document ingestion orchestration, returns True on (attempted) success.\"\"\"\n",
    "        doc_id = doc.get(\"doc_id\") or f\"doc_{int(time.time()*1000)}\"\n",
    "        text = doc.get(\"text\", \"\")\n",
    "        if not text.strip():\n",
    "            DLQ.append((doc, \"EMPTY_TEXT\"))\n",
    "            return False\n",
    "\n",
    "        # Parse & clean\n",
    "        clean_text = simple_clean(text)\n",
    "        if self.mode.enforce_quality_checks and len(clean_text) < 20:\n",
    "            DLQ.append((doc, \"TOO_SHORT\"))\n",
    "            return False\n",
    "\n",
    "        # Chunk\n",
    "        chunks = simple_chunk(clean_text, max_chars=400)   # tune per model\n",
    "        chunk_records = []\n",
    "        for idx, ch in enumerate(chunks):\n",
    "            if exact_dedup_check(ch):\n",
    "                # exact duplicate skip\n",
    "                continue\n",
    "            chunk_id = f\"{doc_id}#c{idx}\"\n",
    "            chunk_records.append({\n",
    "                \"doc_id\": doc_id,\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"chunk_text\": ch,\n",
    "                \"chunk_index\": idx,\n",
    "                \"metadata\": doc.get(\"metadata\", {})\n",
    "            })\n",
    "\n",
    "        if not chunk_records:\n",
    "            # nothing to ingest after dedup\n",
    "            return True\n",
    "\n",
    "        # If SAFE_MODE and require atomic all-chunks, stage first\n",
    "        if self.mode.use_staging_publish:\n",
    "            staging_store[doc_id] = {\"chunks\": chunk_records, \"status\": \"staged\", \"created\": time.time()}\n",
    "\n",
    "        # Embedding in batches (mode controls batch sizes)\n",
    "        texts = [c[\"chunk_text\"] for c in chunk_records]\n",
    "        batch_size = self.mode.embed_batch_size\n",
    "        embeddings = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            emb = fake_embed_batch(batch_texts)\n",
    "            embeddings.extend(emb)\n",
    "\n",
    "        # Near-dedup (optional; expensive)\n",
    "        rows_to_upsert = []\n",
    "        for c, emb in zip(chunk_records, embeddings):\n",
    "            if not self.mode.skip_near_dedup:\n",
    "                if near_dedup_check(emb, threshold=0.995):\n",
    "                    # treat as duplicate; skip indexing\n",
    "                    continue\n",
    "            rows_to_upsert.append({\"id\": c[\"chunk_id\"], \"vector\": emb, \"metadata\": c[\"metadata\"]})\n",
    "\n",
    "        # Upsert strategy:\n",
    "        if self.mode.use_staging_publish:\n",
    "            # store into staging area first\n",
    "            staging_store[doc_id][\"embeddings\"] = rows_to_upsert\n",
    "            staging_store[doc_id][\"status\"] = \"ready\"\n",
    "            # Publish atomically if policy requires\n",
    "            if self.mode.require_all_chunks_atomic:\n",
    "                return self._publish_staging_atomic(doc_id)\n",
    "            else:\n",
    "                # best-effort publish: push immediately but keep staging record\n",
    "                vector_db.bulk_upsert(rows_to_upsert)\n",
    "                return True\n",
    "        else:\n",
    "            # FAST_MODE: upsert directly (streaming)\n",
    "            try:\n",
    "                vector_db.bulk_upsert(rows_to_upsert)\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                DLQ.append((doc, f\"UPsertFail:{str(e)}\"))\n",
    "                return False\n",
    "\n",
    "    def _publish_staging_atomic(self, doc_id: str) -> bool:\n",
    "        \"\"\"Publish staged doc atomically: all-or-none semantics (demo).\"\"\"\n",
    "        record = staging_store.get(doc_id)\n",
    "        if not record or record.get(\"status\") != \"ready\":\n",
    "            DLQ.append(({\"doc_id\": doc_id}, \"STAGING_NOT_READY\"))\n",
    "            return False\n",
    "        rows = record.get(\"embeddings\", [])\n",
    "        # Basic atomicity: validate vectors then perform one bulk upsert\n",
    "        # If validation fails, do not upsert and leave staged for manual inspection\n",
    "        for r in rows:\n",
    "            v = r[\"vector\"]\n",
    "            if not self._validate_vector(v):\n",
    "                DLQ.append(({\"doc_id\": doc_id}, \"INVALID_EMBEDDING\"))\n",
    "                return False\n",
    "        try:\n",
    "            vector_db.bulk_upsert(rows)\n",
    "            record[\"status\"] = \"published\"\n",
    "            # increment index_version to indicate consistent publish (demo)\n",
    "            vector_db.index_version += 1\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            DLQ.append(({\"doc_id\": doc_id}, f\"PUBLISH_FAIL:{str(e)}\"))\n",
    "            return False\n",
    "\n",
    "    def _validate_vector(self, v):\n",
    "        arr = np.array(v)\n",
    "        if arr.ndim != 1:\n",
    "            return False\n",
    "        if np.isnan(arr).any() or np.isinf(arr).any():\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "# -------------------------\n",
    "# Demo usage\n",
    "# -------------------------\n",
    "def demo_run():\n",
    "    docs = [\n",
    "        {\"doc_id\": \"doc_fast_1\", \"text\": \"How to install the Acme Widget? Unpack device, connect cable, run setup.\"},\n",
    "        {\"doc_id\": \"doc_fast_2\", \"text\": \"Short\"},  # will fail in SAFE_MODE (quality)\n",
    "        {\"doc_id\": \"doc_fast_3\", \"text\": \"Repeated content. \" * 50}\n",
    "    ]\n",
    "\n",
    "    print(\"=== Running FAST_MODE ===\")\n",
    "    p_fast = IngestPipeline(FAST_MODE)\n",
    "    for d in docs:\n",
    "        ok = p_fast.ingest_document(d)\n",
    "        print(\"FAST ingest\", d[\"doc_id\"], \"ok=\", ok)\n",
    "    print(\"Vector DB count (FAST):\", vector_db.count())\n",
    "    print(\"DLQ:\", DLQ)\n",
    "\n",
    "    # reset small state for SAFE_MODE demonstration\n",
    "    print(\"\\nClearing exact_hash_set and DLQ for safe demo\")\n",
    "    exact_hash_set.clear()\n",
    "    DLQ.clear()\n",
    "\n",
    "    print(\"\\n=== Running SAFE_MODE ===\")\n",
    "    p_safe = IngestPipeline(SAFE_MODE)\n",
    "    for d in docs:\n",
    "        ok = p_safe.ingest_document(d)\n",
    "        print(\"SAFE ingest\", d[\"doc_id\"], \"ok=\", ok)\n",
    "    print(\"Vector DB count (SAFE):\", vector_db.count())\n",
    "    print(\"Staging store keys:\", list(staging_store.keys()))\n",
    "    print(\"Index version:\", vector_db.index_version)\n",
    "    print(\"DLQ:\", DLQ)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo_run()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### How the code maps FAST vs SAFE decisions\n",
    "\n",
    "* **Deduplication**\n",
    "\n",
    "  * FAST: exact dedup only (fast hash check)\n",
    "  * SAFE: exact + near-dedup using similarity (slower; uses ANN in production)\n",
    "\n",
    "* **Quality checks**\n",
    "\n",
    "  * FAST: skip heavy quality checks\n",
    "  * SAFE: enforce minimum length and any other validators\n",
    "\n",
    "* **Embedding batch size**\n",
    "\n",
    "  * FAST: larger batch (higher throughput)\n",
    "  * SAFE: smaller batch (more deterministic, easier validation)\n",
    "\n",
    "* **Publishing**\n",
    "\n",
    "  * FAST: streaming direct upsert to vector DB\n",
    "  * SAFE: stage then atomic publish (all-or-none) with index version increment\n",
    "\n",
    "* **Error handling**\n",
    "\n",
    "  * FAST: best-effort; failures go to DLQ but do not always block\n",
    "  * SAFE: strict validation, failures kept in staging until resolved\n",
    "\n",
    "---\n",
    "\n",
    "### Production notes and improvements to adopt\n",
    "\n",
    "* Replace `fake_embed_batch` with real embedding service; batch aggregation logic remains the same.\n",
    "* Replace naive `near_dedup_check` with an ANN index (HNSW/FAISS/Chroma) for speed.\n",
    "* Staging should be durable (S3 / DB) and support transactional publish (e.g., swap index alias).\n",
    "* Use queues (Kafka/SQS) between stages to decouple latency and scale each stage independently.\n",
    "* Add observability: metrics for docs/sec, chunks/sec, queue lag, embedding latency, DLQ rates.\n",
    "* Implement configurable time-bounded staging (expire stale staged entries).\n",
    "* Add per-tenant access controls and encryption on metadata before upsert.\n",
    "* For SAFE_MODE atomic publish at scale, use shadow index pattern: build new index/version and switch alias atomically.\n",
    "\n",
    "---\n",
    "\n",
    "### Quick run explanation\n",
    "\n",
    "* Run the file: `python mode_ingest_pipeline.py`\n",
    "* It prints behavior differences:\n",
    "\n",
    "  * FAST_MODE ingests quickly and upserts directly.\n",
    "  * SAFE_MODE stages items, validates embeddings, and publishes atomically (or fails to DLQ if validation fails).\n",
    "\n",
    "This demonstrates practical switches you can flip at runtime to trade **latency** for **consistency**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709357d4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
