{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ac6178d",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## Error Handling and Retry Policies\n",
    "\n",
    "Data ingestion for Generative AI involves **parsing, cleaning, chunking, dedup, embeddings, storage, and orchestration**.\n",
    "Each step can fail due to schema issues, bad documents, API errors, GPU failures, or rate limits.\n",
    "Robust **error handling + retry strategy** ensures that ingestion is *reliable, fault-tolerant, and recoverable* at scale.\n",
    "\n",
    "Below is a clear, structured explanation.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Why Error Handling Matters in GenAI Ingestion\n",
    "\n",
    "Generative-AI pipelines process:\n",
    "\n",
    "* PDFs, HTML, DOCX, OCR\n",
    "* Logs, tickets, emails\n",
    "* Large batches of heterogeneous documents\n",
    "* External APIs (OpenAI, inference servers)\n",
    "* Vector stores (Pinecone, FAISS, Qdrant)\n",
    "\n",
    "Errors are inevitable:\n",
    "\n",
    "* Parsing failures (bad PDFs, OCR corruptions)\n",
    "* Chunking/text errors (unicode issues)\n",
    "* Validation failures (missing fields, unsafe content)\n",
    "* Embedding generation outages\n",
    "* API rate limits\n",
    "* Vector DB write failures\n",
    "* Network timeouts\n",
    "\n",
    "Without structured handling:\n",
    "\n",
    "* Broken pipeline\n",
    "* Partial ingestion\n",
    "* Orphaned embeddings\n",
    "* Inconsistent vector index\n",
    "* Repeated work / wasted cost\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Types of Errors in Generative-AI Ingestion\n",
    "\n",
    "#### **A. Document-level errors**\n",
    "\n",
    "Affect only one document.\n",
    "\n",
    "* corrupted PDF\n",
    "* empty text\n",
    "* missing metadata\n",
    "* invalid format\n",
    "\n",
    "**Action**: send to Dead-Letter Queue (DLQ), mark failed, continue pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "#### **B. Pipeline-stage errors**\n",
    "\n",
    "Specific step fails.\n",
    "\n",
    "Examples:\n",
    "\n",
    "* cleaning function crashes\n",
    "* chunking produces empty results\n",
    "* dedup hash failure\n",
    "\n",
    "**Action**: retry step, fallback to alternative method (e.g., OCR fallback).\n",
    "\n",
    "---\n",
    "\n",
    "#### **C. External service errors**\n",
    "\n",
    "* Embedding API rate limit\n",
    "* OpenAI 429 / 5xx\n",
    "* Vector database unresponsive\n",
    "\n",
    "**Action**: exponential retries, circuit breaker, queue for delayed retry.\n",
    "\n",
    "---\n",
    "\n",
    "#### **D. System-level errors**\n",
    "\n",
    "* Out of memory\n",
    "* GPU crash\n",
    "* Disk full\n",
    "* Worker node failure\n",
    "\n",
    "**Action**: automated retry on new worker, checkpoint restart.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Retry Policies (Core Patterns)\n",
    "\n",
    "#### **1. Fixed Delay Retry**\n",
    "\n",
    "Retry after fixed interval.\n",
    "\n",
    "```\n",
    "Retry every 30 seconds, up to 3 attempts.\n",
    "```\n",
    "\n",
    "Used for predictable transient issues.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Exponential Backoff**\n",
    "\n",
    "Wait time increases exponentially.\n",
    "\n",
    "```\n",
    "Wait: 10s → 30s → 90s → 180s\n",
    "```\n",
    "\n",
    "Used for:\n",
    "\n",
    "* rate limits\n",
    "* external API overload\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Jitter (Randomized Backoff)**\n",
    "\n",
    "Adds randomness to avoid thundering herd.\n",
    "\n",
    "```\n",
    "Wait = base * 2^retry + random(0,1)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Max Retry With Fallback**\n",
    "\n",
    "After N failures, try fallback logic.\n",
    "\n",
    "Example:\n",
    "\n",
    "* If PDF parser fails 3 times → use OCR parser\n",
    "* If embedding API fails → use backup embedding model\n",
    "\n",
    "---\n",
    "\n",
    "#### **5. Dead-Letter Queue (DLQ)**\n",
    "\n",
    "Documents that repeatedly fail are routed to DLQ for manual inspection.\n",
    "\n",
    "---\n",
    "\n",
    "#### **6. Idempotency**\n",
    "\n",
    "Ensures the same document can be retried **without corrupting vector-storage**.\n",
    "\n",
    "Example idempotent rule:\n",
    "\n",
    "* Upsert by chunk_id\n",
    "* Delete previous version before re-embedding\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Error Handling in Each Step (Detailed)\n",
    "\n",
    "#### **A. Extraction**\n",
    "\n",
    "Errors:\n",
    "\n",
    "* network timeouts\n",
    "* corrupted downloads\n",
    "\n",
    "Handling:\n",
    "\n",
    "* retries (3–5)\n",
    "* fallback mirror locations\n",
    "* hash integrity check\n",
    "\n",
    "---\n",
    "\n",
    "#### **B. Parsing**\n",
    "\n",
    "Errors:\n",
    "\n",
    "* PDF unparseable\n",
    "* HTML malformed\n",
    "\n",
    "Handling:\n",
    "\n",
    "* try alternate parser\n",
    "* fallback to OCR\n",
    "* move to DLQ if both fail\n",
    "\n",
    "---\n",
    "\n",
    "#### **C. Cleaning / Noise Removal**\n",
    "\n",
    "Errors:\n",
    "\n",
    "* regex crashes\n",
    "* Unicode decode issues\n",
    "\n",
    "Handling:\n",
    "\n",
    "* catch exceptions\n",
    "* run normalization (replace invalid chars)\n",
    "* skip faulty sections\n",
    "\n",
    "---\n",
    "\n",
    "#### **D. Validation**\n",
    "\n",
    "Errors:\n",
    "\n",
    "* missing fields\n",
    "* unsupported language\n",
    "* unsafe content\n",
    "\n",
    "Handling:\n",
    "\n",
    "* mark invalid → DLQ\n",
    "* don’t retry (not transient)\n",
    "\n",
    "---\n",
    "\n",
    "#### **E. Chunking**\n",
    "\n",
    "Errors:\n",
    "\n",
    "* empty chunks\n",
    "* chunk too large\n",
    "\n",
    "Handling:\n",
    "\n",
    "* retry with default chunk size\n",
    "* fallback to sentence-based chunker\n",
    "\n",
    "---\n",
    "\n",
    "#### **F. Dedup**\n",
    "\n",
    "Errors:\n",
    "\n",
    "* hashing failure\n",
    "* similarity computation errors\n",
    "\n",
    "Handling:\n",
    "\n",
    "* skip chunk\n",
    "* record logs\n",
    "\n",
    "---\n",
    "\n",
    "#### **G. Embedding Generation**\n",
    "\n",
    "Errors:\n",
    "\n",
    "* API rate limit\n",
    "* internal model server error\n",
    "* GPU OOM\n",
    "\n",
    "Handling:\n",
    "\n",
    "* exponential backoff\n",
    "* retry on different worker\n",
    "* batch smaller inputs\n",
    "\n",
    "---\n",
    "\n",
    "#### **H. Vector DB Write**\n",
    "\n",
    "Errors:\n",
    "\n",
    "* network timeout\n",
    "* index not ready\n",
    "* dimension mismatch\n",
    "\n",
    "Handling:\n",
    "\n",
    "* retry\n",
    "* validate embedding before write\n",
    "* auto-delete bad entries\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Error Handling in Orchestration (Airflow / Dagster)\n",
    "\n",
    "#### **Airflow**\n",
    "\n",
    "* `retries=5`\n",
    "* `retry_delay=timedelta(minutes=2)`\n",
    "* Task marked as failed moves to alerting\n",
    "* XCom stores intermediary checkpoints\n",
    "* Use `on_failure_callback` → send Slack/email\n",
    "* Dead-Letter Queue via S3 or DB table\n",
    "\n",
    "---\n",
    "\n",
    "#### **Dagster**\n",
    "\n",
    "* Retries with `RetryPolicy(max_retries=3)`\n",
    "* Automatic logging / materialization of failing ops\n",
    "* Sensors for failure detection\n",
    "* Assets keep lineage so partial ingestion is safe\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Common Retry Policy for Embedding Services\n",
    "\n",
    "```\n",
    "max_retries = 6\n",
    "backoff = exponential (5s, 10s, 20s, 40s, 80s)\n",
    "jitter = random(0–2s)\n",
    "retry_on = [429, 500, 502, 503, connection errors]\n",
    "fallback = switch to alternate embedding endpoint\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Dead-Letter Queue (DLQ)\n",
    "\n",
    "DLQ stores permanent failures in:\n",
    "\n",
    "* S3 bucket: `dlq/ingestion/<timestamp>/<doc_id>.json`\n",
    "* DynamoDB/Firestore\n",
    "* Kafka DLQ topic\n",
    "\n",
    "Contains:\n",
    "\n",
    "* document ID\n",
    "* raw content\n",
    "* failure reason\n",
    "* stack trace\n",
    "* timestamps\n",
    "\n",
    "This avoids stopping the pipeline for bad data.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Example: Simple Retry Logic (Python)\n",
    "\n",
    "```python\n",
    "import time\n",
    "import random\n",
    "\n",
    "def retry_with_exponential_backoff(func, retries=5):\n",
    "    delay = 2\n",
    "    for attempt in range(1, retries+1):\n",
    "        try:\n",
    "            return func()\n",
    "        except Exception as e:\n",
    "            if attempt == retries:\n",
    "                raise\n",
    "            wait = delay * attempt + random.uniform(0, 1)\n",
    "            time.sleep(wait)\n",
    "```\n",
    "\n",
    "Used for:\n",
    "\n",
    "* embedding calls\n",
    "* vector DB writes\n",
    "* network fetch\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Summary\n",
    "\n",
    "#### Error Handling\n",
    "\n",
    "* Detect and classify failures\n",
    "* Recover when possible\n",
    "* Route irrecoverable data to DLQ\n",
    "* Preserve pipeline consistency\n",
    "* Log everything for debugging\n",
    "\n",
    "#### Retry Policies\n",
    "\n",
    "* Fixed retry for predictable issues\n",
    "* Exponential backoff for API/rate limits\n",
    "* Fallback logic for alternate parsers or embedding models\n",
    "* Idempotent writes to ensure safe replays\n",
    "\n",
    "Together, they make the GenAI ingestion pipeline **resilient, scalable, and production-grade**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8455a3",
   "metadata": {},
   "source": [
    "### 1. Airflow Example (with retries, backoff, DLQ, fallback)\n",
    "\n",
    "#### File: `genai_ingest_dag.py`\n",
    "\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "DLQ_BUCKET = \"genai-dlq-bucket\"\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "# -------------------------\n",
    "# Helper: Write to DLQ\n",
    "# -------------------------\n",
    "def send_to_dlq(doc, reason):\n",
    "    key = f\"dlq/{doc['doc_id']}_{int(time.time())}.json\"\n",
    "    payload = {\"doc\": doc, \"reason\": reason}\n",
    "    s3.put_object(Bucket=DLQ_BUCKET, Key=key, Body=json.dumps(payload))\n",
    "\n",
    "# -------------------------\n",
    "# Extract Task\n",
    "# -------------------------\n",
    "def extract(**context):\n",
    "    docs = [\n",
    "        {\"doc_id\": \"doc_001\", \"text\": \"Acme manual...\", \"metadata\": {}},\n",
    "        {\"doc_id\": \"doc_bad\", \"text\": \"\", \"metadata\": {}},  # bad doc\n",
    "    ]\n",
    "    context[\"ti\"].xcom_push(key=\"docs\", value=docs)\n",
    "\n",
    "# -------------------------\n",
    "# Clean Task With Fallback + DLQ\n",
    "# -------------------------\n",
    "def clean(**context):\n",
    "    docs = context[\"ti\"].xcom_pull(key=\"docs\")\n",
    "    cleaned = []\n",
    "    for d in docs:\n",
    "        if not d[\"text\"].strip():     # invalid content\n",
    "            send_to_dlq(d, \"EMPTY_TEXT\")\n",
    "            continue\n",
    "\n",
    "        # cleaning with retry wrapper\n",
    "        try:\n",
    "            cleaned_text = d[\"text\"].replace(\"Page 1 of 10\", \"\").strip()\n",
    "        except Exception as e:\n",
    "            send_to_dlq(d, f\"CLEANING_FAILED: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "        d[\"clean_text\"] = cleaned_text\n",
    "        cleaned.append(d)\n",
    "\n",
    "    context[\"ti\"].xcom_push(key=\"clean_docs\", value=cleaned)\n",
    "\n",
    "# -------------------------\n",
    "# Embedding Task With Exponential Backoff\n",
    "# -------------------------\n",
    "def embed(**context):\n",
    "    docs = context[\"ti\"].xcom_pull(key=\"clean_docs\")\n",
    "    out = []\n",
    "\n",
    "    for d in docs:\n",
    "        retries = 5\n",
    "        delay = 2\n",
    "\n",
    "        for attempt in range(1, retries+1):\n",
    "            try:\n",
    "                # simulate API failure\n",
    "                if random.random() < 0.3:\n",
    "                    raise ConnectionError(\"API timeout\")\n",
    "\n",
    "                emb = [0.1] * 1536   # Fake embedding\n",
    "                d[\"embedding\"] = emb\n",
    "                out.append(d)\n",
    "                break\n",
    "\n",
    "            except Exception as e:\n",
    "                if attempt == retries:\n",
    "                    send_to_dlq(d, f\"EMBED_FAIL: {str(e)}\")\n",
    "                else:\n",
    "                    wait = delay * attempt\n",
    "                    time.sleep(wait)\n",
    "\n",
    "    context[\"ti\"].xcom_push(key=\"embedded_docs\", value=out)\n",
    "\n",
    "# -------------------------\n",
    "# Load Task\n",
    "# -------------------------\n",
    "def load(**context):\n",
    "    docs = context[\"ti\"].xcom_pull(key=\"embedded_docs\")\n",
    "    for d in docs:\n",
    "        print(f\"Upsert vector for {d['doc_id']}\")\n",
    "    return \"done\"\n",
    "\n",
    "# -------------------------\n",
    "# DAG Definition\n",
    "# -------------------------\n",
    "default_args = {\n",
    "    \"owner\": \"genai-team\",\n",
    "    \"retries\": 3,\n",
    "    \"retry_delay\": timedelta(seconds=10),\n",
    "}\n",
    "\n",
    "with DAG(\n",
    "    \"genai_ingestion_with_retry\",\n",
    "    default_args=default_args,\n",
    "    start_date=datetime(2025, 1, 1),\n",
    "    schedule_interval=\"@hourly\",\n",
    "    catchup=False,\n",
    ") as dag:\n",
    "\n",
    "    t1 = PythonOperator(task_id=\"extract\", python_callable=extract)\n",
    "    t2 = PythonOperator(task_id=\"clean\", python_callable=clean)\n",
    "    t3 = PythonOperator(task_id=\"embed\", python_callable=embed)\n",
    "    t4 = PythonOperator(task_id=\"load\", python_callable=load)\n",
    "\n",
    "    t1 >> t2 >> t3 >> t4\n",
    "```\n",
    "\n",
    "#### What this Airflow DAG demonstrates\n",
    "\n",
    "* Document-level validation\n",
    "* DLQ routing\n",
    "* Exponential backoff (manual implementation)\n",
    "* Automatic Airflow retries per-task\n",
    "* Partial success ingestion\n",
    "* No pipeline crash due to a bad document\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Dagster Example (RetryPolicy + failure sensors + DLQ)\n",
    "\n",
    "#### File: `genai_ingest_job.py`\n",
    "\n",
    "```python\n",
    "from dagster import op, job, RetryPolicy, Failure, fs_io_manager\n",
    "import time, random, json\n",
    "\n",
    "DLQ_FILE = \"dlq.json\"\n",
    "\n",
    "# -------------------------\n",
    "# DLQ Writer\n",
    "# -------------------------\n",
    "def write_dlq(doc, reason):\n",
    "    with open(DLQ_FILE, \"a\") as f:\n",
    "        f.write(json.dumps({\"doc\": doc, \"reason\": reason}) + \"\\n\")\n",
    "\n",
    "# -------------------------\n",
    "# Ops\n",
    "# -------------------------\n",
    "\n",
    "@op\n",
    "def extract_docs():\n",
    "    return [\n",
    "        {\"doc_id\": \"doc_001\", \"text\": \"Acme manual...\", \"metadata\": {}},\n",
    "        {\"doc_id\": \"doc_bad\", \"text\": \"\", \"metadata\": {}},\n",
    "    ]\n",
    "\n",
    "@op\n",
    "def clean_docs(docs):\n",
    "    out = []\n",
    "    for d in docs:\n",
    "        if not d[\"text\"].strip():\n",
    "            write_dlq(d, \"EMPTY_TEXT\")\n",
    "            continue\n",
    "        d[\"clean_text\"] = d[\"text\"].replace(\"Page 1 of 10\", \"\").strip()\n",
    "        out.append(d)\n",
    "    return out\n",
    "\n",
    "@op(\n",
    "    retry_policy=RetryPolicy(max_retries=5, delay=2)\n",
    ")\n",
    "def embed_chunks(docs):\n",
    "    out = []\n",
    "    for d in docs:\n",
    "        if random.random() < 0.3:\n",
    "            raise Failure(description=f\"Embedding API failed for {d['doc_id']}\")\n",
    "        d[\"embedding\"] = [0.1] * 1536\n",
    "        out.append(d)\n",
    "    return out\n",
    "\n",
    "@op\n",
    "def load_vectors(docs):\n",
    "    for d in docs:\n",
    "        print(f\"Upsert vector for {d['doc_id']}\")\n",
    "\n",
    "# -------------------------\n",
    "# Job\n",
    "# -------------------------\n",
    "@job(resource_defs={\"io_manager\": fs_io_manager})\n",
    "def genai_ingestion_job():\n",
    "    docs = extract_docs()\n",
    "    cleaned = clean_docs(docs)\n",
    "    embedded = embed_chunks(cleaned)\n",
    "    load_vectors(embedded)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### What the Dagster job demonstrates\n",
    "\n",
    "#### A. RetryPolicy with built-in retry handling\n",
    "\n",
    "```\n",
    "RetryPolicy(max_retries=5, delay=2)\n",
    "```\n",
    "\n",
    "Dagster automatically:\n",
    "\n",
    "* retries failed ops\n",
    "* logs failure metadata\n",
    "* visualizes backoff in UI\n",
    "\n",
    "#### B. Failure handling via `Failure()`\n",
    "\n",
    "You can classify failures and provide messages.\n",
    "\n",
    "#### C. DLQ support\n",
    "\n",
    "Manual DLQ writing for failed ops:\n",
    "\n",
    "```\n",
    "write_dlq(doc, \"EMPTY_TEXT\")\n",
    "```\n",
    "\n",
    "#### D. Strong lineage tracking\n",
    "\n",
    "Dagster automatically tracks:\n",
    "\n",
    "* asset materialization\n",
    "* partial runs\n",
    "* upstream/downstream dependencies\n",
    "\n",
    "---\n",
    "\n",
    "**Summary Comparison**\n",
    "\n",
    "| Concern            | Airflow                                         | Dagster                                    |\n",
    "| ------------------ | ----------------------------------------------- | ------------------------------------------ |\n",
    "| Retry policies     | Built-in + custom code                          | Built-in `RetryPolicy`                     |\n",
    "| Backoff            | Manual or built-in `ExponentialBackoff` plugins | Automatic exponential retry via user logic |\n",
    "| DLQ handling       | S3/DB/Kafka in tasks                            | file/DB/kafka via ops                      |\n",
    "| Failure visibility | Task logs                                       | Rich UI + event logs                       |\n",
    "| Best for           | ETL-heavy GenAI ingestion                       | ML/GenAI pipelines with assets             |\n",
    "\n",
    "---\n",
    "\n",
    "### KubernetesPodOperator Example for GenAI Embedding\n",
    "\n",
    "#### File: `genai_k8s_ingest_dag.py`\n",
    "\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.providers.cncf.kubernetes.operators.kubernetes_pod import KubernetesPodOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Airflow DAG: CPU tasks → GPU embedding pod → load\n",
    "# --------------------------------------------------\n",
    "\n",
    "default_args = {\n",
    "    \"owner\": \"genai-team\",\n",
    "    \"retries\": 2,\n",
    "    \"retry_delay\": timedelta(seconds=20)\n",
    "}\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"genai_ingestion_k8s_gpu\",\n",
    "    start_date=datetime(2025, 1, 1),\n",
    "    default_args=default_args,\n",
    "    schedule_interval=\"@hourly\",\n",
    "    catchup=False,\n",
    ") as dag:\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # CPU task: extract documents from S3/DB/API\n",
    "    # ---------------------------------------------\n",
    "    def extract(**context):\n",
    "        docs = [\n",
    "            {\"doc_id\": \"doc_001\", \"text\": \"Acme manual text...\", \"metadata\": {}}\n",
    "        ]\n",
    "        context[\"ti\"].xcom_push(key=\"docs\", value=docs)\n",
    "\n",
    "    t1_extract = PythonOperator(\n",
    "        task_id=\"extract_docs\",\n",
    "        python_callable=extract\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # GPU task: run embedding inside KubernetesPod\n",
    "    # ---------------------------------------------\n",
    "    t2_embed_gpu = KubernetesPodOperator(\n",
    "        task_id=\"embed_gpu\",\n",
    "        name=\"embed-gpu-job\",\n",
    "        namespace=\"genai\",\n",
    "        image=\"myrepo/genai-embedder:latest\",         # custom embedding image\n",
    "        cmds=[\"python3\", \"embed.py\"],                # entrypoint in the container\n",
    "        arguments=[\n",
    "            \"--input-xcom\", \"{{ ti.xcom_pull(key='docs') }}\",\n",
    "            \"--output-path\", \"/output/embeddings.json\"\n",
    "        ],\n",
    "        env_vars={\n",
    "            \"EMBED_MODEL\": \"my-embedding-v3\",\n",
    "            \"BATCH_SIZE\": \"32\"\n",
    "        },\n",
    "        volumes=[\n",
    "            # example: mount persistent volume for output\n",
    "        ],\n",
    "        volume_mounts=[\n",
    "        ],\n",
    "        container_resources={\n",
    "            \"limits\": {\"nvidia.com/gpu\": \"1\"},        # request 1 GPU\n",
    "            \"requests\": {\"memory\": \"4Gi\", \"cpu\": \"1\"}\n",
    "        },\n",
    "        get_logs=True,\n",
    "        is_delete_operator_pod=True,                  # auto-clean pod\n",
    "        in_cluster=True,                              # running inside cluster\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------\n",
    "    # CPU task: load embeddings into vector DB\n",
    "    # ---------------------------------------------\n",
    "    def load_embeddings():\n",
    "        print(\"loading embeddings into vector DB (FAISS / Pinecone / Chroma)\")\n",
    "\n",
    "    t3_load = PythonOperator(\n",
    "        task_id=\"load_embeddings\",\n",
    "        python_callable=load_embeddings\n",
    "    )\n",
    "\n",
    "    t1_extract >> t2_embed_gpu >> t3_load\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. What This KubernetesPodOperator Setup Demonstrates\n",
    "\n",
    "#### A. GPU Execution\n",
    "\n",
    "```\n",
    "container_resources = {\n",
    "    \"limits\": {\"nvidia.com/gpu\": \"1\"}\n",
    "}\n",
    "```\n",
    "\n",
    "This ensures the embedding workload runs on a GPU-enabled node.\n",
    "\n",
    "---\n",
    "\n",
    "#### B. Pod runs *only* the heavy step\n",
    "\n",
    "Airflow handles lightweight orchestrating tasks; heavy work (embeddings, OCR, vectorization) is passed to a dedicated K8s pod.\n",
    "\n",
    "---\n",
    "\n",
    "#### C. Fully isolated environment\n",
    "\n",
    "Embedding container contains:\n",
    "\n",
    "* embedding model\n",
    "* tokenizer\n",
    "* dependencies\n",
    "* CUDA drivers (if needed)\n",
    "\n",
    "You avoid overloading Airflow workers.\n",
    "\n",
    "---\n",
    "\n",
    "#### D. Auto-cleanup\n",
    "\n",
    "```\n",
    "is_delete_operator_pod=True\n",
    "```\n",
    "\n",
    "Pods are deleted after completion.\n",
    "\n",
    "---\n",
    "\n",
    "#### E. XCom passing → Pod arguments\n",
    "\n",
    "You can pass documents from Airflow to the pod as arguments.\n",
    "\n",
    "---\n",
    "\n",
    "#### F. Retries (Airflow-level)\n",
    "\n",
    "`retries=2` ensures Airflow re-runs the pod if it fails.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Example: embed.py\n",
    "\n",
    "This script runs *inside* the Kubernetes pod.\n",
    "\n",
    "```python\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--input-xcom\")\n",
    "parser.add_argument(\"--output-path\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "docs = json.loads(args.input_xcom)\n",
    "\n",
    "# Fake GPU embedding (replace with real model)\n",
    "embeddings = []\n",
    "for d in docs:\n",
    "    emb = np.random.rand(1536).tolist()\n",
    "    embeddings.append({\"doc_id\": d[\"doc_id\"], \"embedding\": emb})\n",
    "\n",
    "with open(args.output_path, \"w\") as f:\n",
    "    json.dump(embeddings, f)\n",
    "\n",
    "print(\"Embedding completed inside GPU pod\")\n",
    "```\n",
    "\n",
    "In reality, replace this with:\n",
    "\n",
    "* OpenAI embedding calls\n",
    "* Local embedding model (Jina, SentenceTransformers, LlamaIndex)\n",
    "* TensorRT optimized encoder\n",
    "* Custom PyTorch model\n",
    "\n",
    "---\n",
    "\n",
    "### Typical Use Cases in GenAI Ingestion\n",
    "\n",
    "| Task                      | Should run in CPU? | Should run in GPU pod? |\n",
    "| ------------------------- | ------------------ | ---------------------- |\n",
    "| PDF parsing               | CPU                | No                     |\n",
    "| OCR                       | CPU or GPU         | Yes (GPU recommended)  |\n",
    "| Noise removal             | CPU                | No                     |\n",
    "| Chunking                  | CPU                | No                     |\n",
    "| Dedup & similarity checks | CPU                | Sometimes              |\n",
    "| Embedding generation      | No                 | **Yes**                |\n",
    "| Vector DB writes          | CPU                | No                     |\n",
    "| Reranking                 | No                 | Optional               |\n",
    "\n",
    "Embedding is typically the heavy GPU-bound stage → KubernetesPodOperator is ideal.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Production Considerations\n",
    "\n",
    "#### A. Node selectors for GPU nodes\n",
    "\n",
    "```\n",
    "node_selector={\"gpu\": \"true\"}\n",
    "```\n",
    "\n",
    "#### B. Tolerations for GPU node pools\n",
    "\n",
    "Allows pods to run only on GPU nodes.\n",
    "\n",
    "#### C. Use Kubernetes Secrets\n",
    "\n",
    "Inject OpenAI keys or auth tokens securely.\n",
    "\n",
    "#### D. Artifact storage\n",
    "\n",
    "Use S3/GCS for passing embeddings back to Airflow.\n",
    "\n",
    "#### E. Logging\n",
    "\n",
    "`get_logs=True` streams pod logs to Airflow UI.\n",
    "\n",
    "#### F. Batch splitting\n",
    "\n",
    "Large ingestion → multiple embedding pods launched in parallel.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
