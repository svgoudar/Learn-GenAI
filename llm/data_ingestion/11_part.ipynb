{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34217c24",
   "metadata": {},
   "source": [
    "\n",
    "## Metadata\n",
    "\n",
    "Metadata is **descriptive information attached to documents, chunks, embeddings, or model outputs**.\n",
    "It enables traceability, quality control, searchability, and reproducibility.\n",
    "\n",
    "### Types of metadata\n",
    "\n",
    "#### **A. Document Metadata**\n",
    "\n",
    "* `doc_id`\n",
    "* `source` (URL, S3 path, SharePoint path)\n",
    "* `filename`\n",
    "* `ingestion_timestamp`\n",
    "* `language`\n",
    "* `document_type` (FAQ, manual, ticket)\n",
    "* `author` (if available)\n",
    "* `version`\n",
    "* `hash` (raw document checksum)\n",
    "\n",
    "#### **B. Chunk Metadata**\n",
    "\n",
    "* `chunk_id`\n",
    "* `doc_id`\n",
    "* `chunk_index`\n",
    "* `section_title`\n",
    "* `page_number`\n",
    "* `char_offset`\n",
    "* `token_count`\n",
    "* `quality_score`\n",
    "* `chunk_version` (if chunking strategy changes)\n",
    "\n",
    "#### **C. Embedding Metadata**\n",
    "\n",
    "* `embedding_model_version`\n",
    "* `dimensions`\n",
    "* `vector_id`\n",
    "* `embedding_timestamp`\n",
    "* `similarity_threshold_used`\n",
    "\n",
    "#### **D. Processing Metadata**\n",
    "\n",
    "* `parser_used` (pdfminer, tika, html cleaner)\n",
    "* `ocr_engine` (tesseract, paddleocr)\n",
    "* `cleaning_version`\n",
    "* `chunking_version`\n",
    "* `dedup_strategy_version`\n",
    "* `safety_filter_version`\n",
    "\n",
    "#### **E. Operational Metadata**\n",
    "\n",
    "* `ingestion_batch_id`\n",
    "* `retry_count`\n",
    "* `worker_id`\n",
    "* `pipeline_id` (Airflow/Dagster run ID)\n",
    "* `latency` per stage\n",
    "\n",
    "This metadata lets you reconstruct *exactly how* the data was produced.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. What Lineage Tracking Means\n",
    "\n",
    "**Lineage tracking** is the process of recording every transformation step that data undergoes from raw → cleaned → chunked → embedded → indexed.\n",
    "\n",
    "It answers:\n",
    "\n",
    "* Where did this chunk come from?\n",
    "* What raw PDF created this embedding?\n",
    "* Which chunking strategy was used?\n",
    "* Which model created this embedding?\n",
    "* When was it processed?\n",
    "* Can we reproduce it exactly if needed?\n",
    "\n",
    "Lineage is mandatory for:\n",
    "\n",
    "* debugging\n",
    "* audits\n",
    "* model drift analysis\n",
    "* re-ingesting with new embedding models\n",
    "* ensuring reproducibility\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Lineage Tracking Model (Simple Practical Graph)\n",
    "\n",
    "```\n",
    "RAW_DOCUMENT\n",
    "  |\n",
    "  ├── parsed_by = pdf_parser_v2\n",
    "  ↓\n",
    "PARSED_TEXT\n",
    "  |\n",
    "  ├── cleaned_by = cleaner_v3\n",
    "  ↓\n",
    "CLEAN_TEXT\n",
    "  |\n",
    "  ├── chunked_by = chunker_v4\n",
    "  ↓\n",
    "CHUNKS\n",
    "  |\n",
    "  ├── embedded_by = embedding_model_v3\n",
    "  ├── vector_id = x123\n",
    "  ↓\n",
    "EMBEDDINGS\n",
    "  |\n",
    "  └── indexed_in = pinecone_index_v7\n",
    "```\n",
    "\n",
    "Each arrow represents a **lineage edge** with metadata.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Why Metadata + Lineage Are Critical in Generative AI\n",
    "\n",
    "#### A. Embedding Model Upgrades\n",
    "\n",
    "If you switch embedding model from `v1` → `v2`, you must know:\n",
    "\n",
    "* which chunks were embedded with old model\n",
    "* which need re-embedding\n",
    "* which vector store index version was used\n",
    "\n",
    "Without lineage, re-indexing becomes impossible.\n",
    "\n",
    "---\n",
    "\n",
    "#### B. Chunking Strategy Changes\n",
    "\n",
    "If you update chunk size from **200 tokens → 500 tokens**, lineage lets you:\n",
    "\n",
    "* rollback\n",
    "* regenerate\n",
    "* compare retrieval metrics between versions\n",
    "\n",
    "---\n",
    "\n",
    "#### C. Safety / Compliance\n",
    "\n",
    "Lineage provides:\n",
    "\n",
    "* proof of document origin\n",
    "* GDPR/PII deletion tracking\n",
    "* dataset auditability for regulations\n",
    "\n",
    "---\n",
    "\n",
    "#### D. Debugging RAG Failures\n",
    "\n",
    "If retrieval produces irrelevant results, lineage tells you:\n",
    "\n",
    "* which chunk\n",
    "* which embedding model\n",
    "* which preprocessing\n",
    "* which version of dedup strategy\n",
    "  caused the issue.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. How to Store Metadata and Lineage\n",
    "\n",
    "#### A. Vector Databases\n",
    "\n",
    "Add metadata per vector:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"doc_id\": \"doc_001\",\n",
    "  \"chunk_id\": \"doc_001_2\",\n",
    "  \"source\": \"s3://bucket/manual.pdf\",\n",
    "  \"page\": 12,\n",
    "  \"section\": \"Installation\",\n",
    "  \"chunk_version\": 4,\n",
    "  \"embedding_version\": \"embed-v3\",\n",
    "  \"ingested_at\": \"2025-02-10T10:22:00Z\"\n",
    "}\n",
    "```\n",
    "\n",
    "#### B. Document Store (SQL / NoSQL)\n",
    "\n",
    "Use tables:\n",
    "\n",
    "##### `documents`\n",
    "\n",
    "```\n",
    "doc_id | source | hash | language | version | parser_version\n",
    "```\n",
    "\n",
    "##### `chunks`\n",
    "\n",
    "```\n",
    "chunk_id | doc_id | chunk_index | text | chunk_version | cleaning_version\n",
    "```\n",
    "\n",
    "##### `embeddings`\n",
    "\n",
    "```\n",
    "vector_id | chunk_id | model_version | dim | timestamp | index_version\n",
    "```\n",
    "\n",
    "#### C. Data Lake (S3/GCS)\n",
    "\n",
    "Store intermediate states:\n",
    "\n",
    "```\n",
    "raw/       doc_001.pdf\n",
    "clean/     doc_001.clean.json\n",
    "chunks/    doc_001.chunks.json\n",
    "embed/     doc_001.embed.json\n",
    "```\n",
    "\n",
    "#### D. Orchestrator Metadata (Airflow / Dagster)\n",
    "\n",
    "* Airflow: `run_id`, task logs, XCom\n",
    "* Dagster: materializations + asset lineage view\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Lineage Tracking in Airflow (Simplified)\n",
    "\n",
    "Airflow stores lineage metadata with each task:\n",
    "\n",
    "```\n",
    "context[\"ti\"].xcom_push(\"lineage\", {\n",
    "   \"doc_id\": d[\"doc_id\"],\n",
    "   \"cleaner_version\": \"3.1\",\n",
    "   \"embedder_model\": \"openai-embed-v3\",\n",
    "   \"run_id\": context['run_id']\n",
    "})\n",
    "```\n",
    "\n",
    "Can be written to:\n",
    "\n",
    "* S3 JSON lineage file\n",
    "* DynamoDB lineage table\n",
    "* Elasticsearch index for search\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Lineage Tracking in Dagster (Native Support)\n",
    "\n",
    "Dagster provides **software-defined assets** with built-in lineage.\n",
    "\n",
    "```\n",
    "@asset\n",
    "def clean_text(raw_document):\n",
    "    ...\n",
    "    return cleaned_text    # Dagster tracks lineage automatically\n",
    "\n",
    "@asset\n",
    "def chunks(clean_text):\n",
    "    ...\n",
    "    return chunk_list\n",
    "\n",
    "@asset\n",
    "def embeddings(chunks):\n",
    "    ...\n",
    "```\n",
    "\n",
    "Dagster UI automatically shows:\n",
    "\n",
    "* dependencies\n",
    "* materialization history\n",
    "* metadata for each step\n",
    "\n",
    "This is ideal for GenAI ingestion.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Metadata + Lineage Best Practices for Generative-AI Data Pipelines\n",
    "\n",
    "#### Mandatory fields to store for each chunk\n",
    "\n",
    "* `doc_id`\n",
    "* `chunk_id`\n",
    "* `chunk_index`\n",
    "* `cleaning_version`\n",
    "* `chunking_version`\n",
    "* `embedding_model_version`\n",
    "* `ingestion_timestamp`\n",
    "* `source_uri`\n",
    "* `hash_raw`\n",
    "* `hash_clean`\n",
    "\n",
    "#### Recommended workflows\n",
    "\n",
    "* Store hashes at each stage to detect unintended changes\n",
    "* Use consistent versioning (clean_v3, chunk_v4, embed_v3)\n",
    "* Maintain index_version for vector DB rebuild\n",
    "* Keep full lineage in S3 as JSON for audits\n",
    "* Use Dagster for asset lineage if possible\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**\n",
    "\n",
    "**Metadata** gives context to data at every step: source, document type, chunk info, model versions, timestamps.\n",
    "\n",
    "**Lineage** tracks how data moves and transforms through the ingestion pipeline: raw → parsed → cleaned → chunked → embedded → indexed.\n",
    "\n",
    "Together, they ensure:\n",
    "\n",
    "* reproducibility\n",
    "* debuggability\n",
    "* compliance\n",
    "* rollbacks\n",
    "* quality control\n",
    "* scalable ingestion\n",
    "\n",
    "\n",
    "This is the **recommended Dagster pattern** for RAG / LLM data pipelines because assets give you:\n",
    "\n",
    "* automatic lineage\n",
    "* versioning\n",
    "* metadata tracking\n",
    "* materialization history\n",
    "* orchestration\n",
    "* retries\n",
    "* partitions\n",
    "* observability\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Folder Structure (recommended)\n",
    "\n",
    "```\n",
    "genai_ingest/\n",
    "    ├── assets/\n",
    "    │     ├── raw_docs.py\n",
    "    │     ├── clean_docs.py\n",
    "    │     ├── chunks.py\n",
    "    │     ├── embeddings.py\n",
    "    │     └── vector_store.py\n",
    "    ├── resources/\n",
    "    │     ├── s3.py\n",
    "    │     ├── embedder.py\n",
    "    │     └── vector_db.py\n",
    "    ├── __init__.py\n",
    "    └── workspace.yaml\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Dagster Asset-Based Pipeline (Full Example)\n",
    "\n",
    "#### A. **raw_docs asset**\n",
    "\n",
    "Extracts documents from S3 / DB / API.\n",
    "\n",
    "```python\n",
    "# raw_docs.py\n",
    "from dagster import asset, Output, MetadataValue\n",
    "import json\n",
    "\n",
    "@asset\n",
    "def raw_documents():\n",
    "    docs = [\n",
    "        {\"doc_id\": \"doc_001\", \"text\": \"Acme manual text...\", \"metadata\": {\"source\":\"s3://bucket/acme.pdf\"}},\n",
    "        {\"doc_id\": \"doc_002\", \"text\": \"Install guide steps...\", \"metadata\": {\"source\":\"s3://bucket/install.pdf\"}}\n",
    "    ]\n",
    "\n",
    "    return Output(\n",
    "        docs,\n",
    "        metadata={\"count\": len(docs), \"source\": MetadataValue.text(\"S3 bucket\")}\n",
    "    )\n",
    "```\n",
    "\n",
    "Dagster automatically tracks lineage and metadata here.\n",
    "\n",
    "---\n",
    "\n",
    "#### B. **clean_docs asset**\n",
    "\n",
    "Performs cleaning, normalization, boilerplate removal.\n",
    "\n",
    "```python\n",
    "# clean_docs.py\n",
    "from dagster import asset, Output, MetadataValue\n",
    "import re\n",
    "\n",
    "def clean_text(t: str) -> str:\n",
    "    t = re.sub(r\"Page \\d+ of \\d+\", \"\", t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    return t\n",
    "\n",
    "@asset\n",
    "def clean_documents(raw_documents):\n",
    "    cleaned = []\n",
    "    for d in raw_documents:\n",
    "        if not d[\"text\"].strip():\n",
    "            continue\n",
    "        d[\"clean_text\"] = clean_text(d[\"text\"])\n",
    "        cleaned.append(d)\n",
    "\n",
    "    return Output(\n",
    "        cleaned,\n",
    "        metadata={\"cleaned_count\": len(cleaned), \"cleaning_version\": \"v3\"}\n",
    "    )\n",
    "```\n",
    "\n",
    "Dagster UI now shows:\n",
    "`raw_documents → clean_documents` lineage.\n",
    "\n",
    "---\n",
    "\n",
    "#### C. **chunks asset**\n",
    "\n",
    "Chunk using semantic or size-based splitting.\n",
    "\n",
    "```python\n",
    "# chunks.py\n",
    "from dagster import asset, Output, MetadataValue\n",
    "\n",
    "def chunk_text(text, max_char=300):\n",
    "    return [text[i:i+max_char] for i in range(0, len(text), max_char)]\n",
    "\n",
    "@asset\n",
    "def document_chunks(clean_documents):\n",
    "    all_chunks = []\n",
    "\n",
    "    for d in clean_documents:\n",
    "        chunks = chunk_text(d[\"clean_text\"])\n",
    "        for i, ch in enumerate(chunks):\n",
    "            all_chunks.append({\n",
    "                \"doc_id\": d[\"doc_id\"],\n",
    "                \"chunk_id\": f\"{d['doc_id']}_chunk_{i}\",\n",
    "                \"chunk_text\": ch,\n",
    "                \"chunk_index\": i,\n",
    "                \"metadata\": d[\"metadata\"]\n",
    "            })\n",
    "\n",
    "    return Output(\n",
    "        all_chunks,\n",
    "        metadata={\n",
    "            \"total_chunks\": len(all_chunks),\n",
    "            \"chunking_version\": \"v4\"\n",
    "        }\n",
    "    )\n",
    "```\n",
    "\n",
    "Dagster lineage:\n",
    "`raw_documents → clean_documents → document_chunks`\n",
    "\n",
    "---\n",
    "\n",
    "#### D. **embeddings asset** (GPU or CPU)\n",
    "\n",
    "Connects to embedding model (OpenAI or local).\n",
    "\n",
    "```python\n",
    "# embeddings.py\n",
    "from dagster import asset, Output, MetadataValue\n",
    "import numpy as np\n",
    "\n",
    "def fake_embed(text):\n",
    "    # Placeholder for real model call\n",
    "    return np.random.rand(1536).tolist()\n",
    "\n",
    "@asset\n",
    "def chunk_embeddings(document_chunks):\n",
    "    embedded = []\n",
    "\n",
    "    for ch in document_chunks:\n",
    "        emb = fake_embed(ch[\"chunk_text\"])\n",
    "        ch[\"embedding\"] = emb\n",
    "        embedded.append(ch)\n",
    "\n",
    "    return Output(\n",
    "        embedded,\n",
    "        metadata={\n",
    "            \"embedding_model\": \"embed-v3\",\n",
    "            \"count\": len(embedded),\n",
    "            \"dim\": 1536\n",
    "        }\n",
    "    )\n",
    "```\n",
    "\n",
    "Dagster lineage:\n",
    "`raw_documents → clean_documents → document_chunks → chunk_embeddings`\n",
    "\n",
    "---\n",
    "\n",
    "#### E. **vector_store asset**\n",
    "\n",
    "Writes to FAISS / Pinecone / Chroma / Weaviate.\n",
    "\n",
    "```python\n",
    "# vector_store.py\n",
    "from dagster import asset, Output, MetadataValue\n",
    "\n",
    "@asset\n",
    "def upsert_to_vector_store(chunk_embeddings):\n",
    "    for rec in chunk_embeddings:\n",
    "        # In production: pinecone.upsert(...)\n",
    "        pass\n",
    "\n",
    "    return Output(\n",
    "        \"completed\",\n",
    "        metadata={\"vector_count\": len(chunk_embeddings), \"index_version\": \"v7\"}\n",
    "    )\n",
    "```\n",
    "\n",
    "Final lineage:\n",
    "\n",
    "```\n",
    "raw_documents\n",
    "    → clean_documents\n",
    "        → document_chunks\n",
    "            → chunk_embeddings\n",
    "                → upsert_to_vector_store\n",
    "```\n",
    "\n",
    "Dagster visualizes this DAG automatically.\n",
    "\n",
    "---\n",
    "\n",
    "# 3. Unified Job (asset group execution)\n",
    "\n",
    "In `__init__.py` or `repository.py`:\n",
    "\n",
    "```python\n",
    "from dagster import Definitions, load_assets_from_package_module\n",
    "import assets\n",
    "\n",
    "all_assets = load_assets_from_package_module(assets)\n",
    "\n",
    "defs = Definitions(assets=all_assets)\n",
    "```\n",
    "\n",
    "Dagster's UI automatically:\n",
    "\n",
    "* maps assets\n",
    "* draws lineage graph\n",
    "* shows metadata\n",
    "* shows materialization history\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Scheduling the Asset Pipeline\n",
    "\n",
    "You can schedule ingestion:\n",
    "\n",
    "```python\n",
    "from dagster import ScheduleDefinition\n",
    "\n",
    "ingest_schedule = ScheduleDefinition(\n",
    "    job=defs.get_software_defined_asset_job(\"all_assets_job\"),\n",
    "    cron_schedule=\"0 * * * *\"   # hourly\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Advantages of Asset-Based Dagster for GenAI\n",
    "\n",
    "#### 1. **Automatic lineage tracking**\n",
    "\n",
    "Dagster visually links:\n",
    "\n",
    "* raw → clean → chunks → embeddings → vector store\n",
    "\n",
    "#### 2. **Version tracking**\n",
    "\n",
    "You can version:\n",
    "\n",
    "* cleaning versions\n",
    "* chunking versions\n",
    "* embedding models\n",
    "* index versions\n",
    "\n",
    "#### 3. **Materialization history**\n",
    "\n",
    "Every run stores metadata such as:\n",
    "\n",
    "* timestamp\n",
    "* data size\n",
    "* model version\n",
    "* quality scores\n",
    "\n",
    "#### 4. **Backfills**\n",
    "\n",
    "You can easily re-chunk or re-embed historical data after upgrading your embedding model.\n",
    "\n",
    "#### 5. **Observability**\n",
    "\n",
    "Dagster UI shows:\n",
    "\n",
    "* asset dependencies\n",
    "* failed ops\n",
    "* retries\n",
    "* per-asset metadata\n",
    "\n",
    "#### 6. **Parallel execution**\n",
    "\n",
    "Hotspots:\n",
    "\n",
    "* chunking\n",
    "* embeddings\n",
    "  can easily scale.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Add Optional Metadata for Governance\n",
    "\n",
    "Dagster supports rich asset metadata:\n",
    "\n",
    "```python\n",
    "return Output(\n",
    "    data,\n",
    "    metadata={\n",
    "        \"record_count\": len(data),\n",
    "        \"hash\": MetadataValue.md5(data_bytes),\n",
    "        \"quality_score\": 0.94,\n",
    "        \"source\": MetadataValue.url(\"https://docs.example.com\")\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**\n",
    "\n",
    "Dagster asset-based pipelines are ideal for GenAI ingestion because they automatically give:\n",
    "\n",
    "* **lineage**\n",
    "* **metadata tracking**\n",
    "* **version control**\n",
    "* **observability**\n",
    "* **modular ingestion stages**\n",
    "* **easy incremental updates**\n",
    "* **support for backfills and model upgrades**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
