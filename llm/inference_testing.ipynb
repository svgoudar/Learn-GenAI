{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64f605ea",
   "metadata": {},
   "source": [
    "# LLM Inference Testing\n",
    "\n",
    "This notebook demonstrates how to create and test LLM inference using various models and our processed dataset. We'll explore different approaches for testing inference capabilities including local models and cloud-based solutions.\n",
    "\n",
    "## Overview\n",
    "- Load and test pre-trained models\n",
    "- Configure inference parameters  \n",
    "- Test with our processed data\n",
    "- Benchmark performance\n",
    "- Compare different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdce21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "import psutil\n",
    "import gc\n",
    "from datetime import datetime\n",
    "\n",
    "# For Azure AI Inference\n",
    "try:\n",
    "    from azure.ai.inference import ChatCompletionsClient\n",
    "    from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "    from azure.core.credentials import AzureKeyCredential\n",
    "    print(\"‚úì Azure AI Inference SDK available\")\n",
    "except ImportError:\n",
    "    print(\"Installing Azure AI Inference...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"azure-ai-inference\"])\n",
    "    from azure.ai.inference import ChatCompletionsClient\n",
    "    from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "    from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# For OpenAI SDK (alternative)\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "    print(\"‚úì OpenAI SDK available\")\n",
    "except ImportError:\n",
    "    print(\"Installing OpenAI SDK...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"openai\"])\n",
    "    from openai import OpenAI\n",
    "\n",
    "# For local model testing (optional)\n",
    "try:\n",
    "    import torch\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "    print(\"‚úì Transformers available for local models\")\n",
    "    TRANSFORMERS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Transformers not available - cloud models only\")\n",
    "    TRANSFORMERS_AVAILABLE = False\n",
    "\n",
    "# Set up paths\n",
    "BASE_DIR = Path(r\"c:\\Github\\Learn-GenAI\\genai_book\")\n",
    "DATA_FILE = BASE_DIR / \"llm_data_medium_chunks.jsonl\"\n",
    "\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "print(f\"Data file: {DATA_FILE}\")\n",
    "print(f\"Data file exists: {DATA_FILE.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56207bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare test data from our ingestion pipeline\n",
    "print(\"=== LOADING TEST DATA ===\")\n",
    "\n",
    "def load_processed_data(file_path: Path, max_samples: int = 50) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Load processed data from JSONL file\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    if not file_path.exists():\n",
    "        print(f\"‚ùå Data file not found: {file_path}\")\n",
    "        return data\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= max_samples:\n",
    "                    break\n",
    "                data.append(json.loads(line))\n",
    "        \n",
    "        print(f\"‚úì Loaded {len(data)} data samples\")\n",
    "        return data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading data: {e}\")\n",
    "        return []\n",
    "\n",
    "# Load test data\n",
    "test_data = load_processed_data(DATA_FILE, max_samples=20)\n",
    "\n",
    "if test_data:\n",
    "    # Display data statistics\n",
    "    categories = [item['category'] for item in test_data]\n",
    "    category_counts = pd.Series(categories).value_counts()\n",
    "    \n",
    "    print(f\"\\nTest Data Overview:\")\n",
    "    print(f\"Total samples: {len(test_data)}\")\n",
    "    print(f\"Categories: {list(category_counts.index)}\")\n",
    "    print(f\"Category distribution:\")\n",
    "    for cat, count in category_counts.items():\n",
    "        print(f\"  {cat}: {count} samples\")\n",
    "    \n",
    "    # Show a sample\n",
    "    sample = test_data[0]\n",
    "    print(f\"\\nSample data:\")\n",
    "    print(f\"Category: {sample['category']}\")\n",
    "    print(f\"Source: {sample['source_file']}\")\n",
    "    print(f\"Text preview: {sample['text'][:200]}...\")\n",
    "    print(f\"Chunk size: {sample['chunk_size']} words\")\n",
    "else:\n",
    "    print(\"‚ùå No test data available. Please run the data ingestion pipeline first.\")\n",
    "    # Create some sample data for testing\n",
    "    test_data = [\n",
    "        {\n",
    "            'text': 'This is a sample business text about market trends and economic indicators.',\n",
    "            'category': 'business',\n",
    "            'source_file': 'sample_business.txt',\n",
    "            'chunk_size': 12\n",
    "        },\n",
    "        {\n",
    "            'text': 'Technology advances in artificial intelligence and machine learning are transforming industries.',\n",
    "            'category': 'technology',\n",
    "            'source_file': 'sample_tech.txt',\n",
    "            'chunk_size': 11\n",
    "        }\n",
    "    ]\n",
    "    print(\"‚úì Created sample test data for demonstration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa28f26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Cloud-based Model for Inference (GitHub Models)\n",
    "print(\"=== CLOUD MODEL CONFIGURATION ===\")\n",
    "\n",
    "class CloudLLMInference:\n",
    "    \"\"\"\n",
    "    Cloud-based LLM inference using GitHub Models via Azure AI Inference SDK\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"openai/gpt-4.1-mini\"):\n",
    "        self.endpoint = \"https://models.github.ai/inference\"\n",
    "        self.model = model_name\n",
    "        self.token = os.environ.get(\"GITHUB_TOKEN\")\n",
    "        \n",
    "        if not self.token:\n",
    "            print(\"‚ùå GITHUB_TOKEN environment variable not set\")\n",
    "            print(\"üí° Set your GitHub Personal Access Token:\")\n",
    "            print(\"   export GITHUB_TOKEN='your_github_pat'\")\n",
    "            self.client = None\n",
    "        else:\n",
    "            try:\n",
    "                self.client = ChatCompletionsClient(\n",
    "                    endpoint=self.endpoint,\n",
    "                    credential=AzureKeyCredential(self.token),\n",
    "                )\n",
    "                print(f\"‚úì Connected to GitHub Models\")\n",
    "                print(f\"‚úì Model: {self.model}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error connecting to GitHub Models: {e}\")\n",
    "                self.client = None\n",
    "    \n",
    "    def generate_text(self, \n",
    "                     prompt: str, \n",
    "                     system_message: str = \"You are a helpful AI assistant.\",\n",
    "                     temperature: float = 0.7,\n",
    "                     max_tokens: int = 200) -> Dict:\n",
    "        \"\"\"\n",
    "        Generate text using the cloud model\n",
    "        \"\"\"\n",
    "        if not self.client:\n",
    "            return {\"error\": \"Client not initialized\"}\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            response = self.client.complete(\n",
    "                messages=[\n",
    "                    SystemMessage(system_message),\n",
    "                    UserMessage(prompt),\n",
    "                ],\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "                model=self.model\n",
    "            )\n",
    "            \n",
    "            end_time = time.time()\n",
    "            \n",
    "            result = {\n",
    "                \"generated_text\": response.choices[0].message.content,\n",
    "                \"model\": self.model,\n",
    "                \"prompt\": prompt,\n",
    "                \"system_message\": system_message,\n",
    "                \"generation_time\": end_time - start_time,\n",
    "                \"temperature\": temperature,\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"finish_reason\": response.choices[0].finish_reason,\n",
    "                \"usage\": {\n",
    "                    \"prompt_tokens\": response.usage.prompt_tokens if response.usage else None,\n",
    "                    \"completion_tokens\": response.usage.completion_tokens if response.usage else None,\n",
    "                    \"total_tokens\": response.usage.total_tokens if response.usage else None\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"error\": str(e),\n",
    "                \"model\": self.model,\n",
    "                \"prompt\": prompt\n",
    "            }\n",
    "\n",
    "# Initialize cloud model\n",
    "cloud_model = CloudLLMInference(\"openai/gpt-4.1-mini\")\n",
    "\n",
    "# Test connection\n",
    "if cloud_model.client:\n",
    "    test_result = cloud_model.generate_text(\"Hello! Can you generate a short response?\", max_tokens=50)\n",
    "    if \"error\" not in test_result:\n",
    "        print(f\"‚úì Test successful!\")\n",
    "        print(f\"  Response: {test_result['generated_text'][:100]}...\")\n",
    "        print(f\"  Time: {test_result['generation_time']:.2f}s\")\n",
    "        if test_result['usage']['total_tokens']:\n",
    "            print(f\"  Tokens: {test_result['usage']['total_tokens']}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Test failed: {test_result['error']}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Cloud model not available - set GITHUB_TOKEN environment variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e91e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Local Model for Inference (Optional)\n",
    "print(\"=== LOCAL MODEL CONFIGURATION ===\")\n",
    "\n",
    "class LocalLLMInference:\n",
    "    \"\"\"\n",
    "    Local LLM inference using Hugging Face transformers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"microsoft/DialoGPT-small\"):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.pipeline = None\n",
    "        \n",
    "        if not TRANSFORMERS_AVAILABLE:\n",
    "            print(\"‚ùå Transformers not available for local models\")\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            print(f\"Loading model: {model_name}\")\n",
    "            print(\"‚ö†Ô∏è This may take a while for first-time download...\")\n",
    "            \n",
    "            # Use a small, efficient model for testing\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "            \n",
    "            # Add pad token if not present\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "                device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "            )\n",
    "            \n",
    "            # Create text generation pipeline\n",
    "            self.pipeline = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=self.model,\n",
    "                tokenizer=self.tokenizer,\n",
    "                device=0 if torch.cuda.is_available() else -1\n",
    "            )\n",
    "            \n",
    "            print(f\"‚úì Local model loaded successfully\")\n",
    "            print(f\"‚úì Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading local model: {e}\")\n",
    "            print(\"üí° Trying smaller model...\")\n",
    "            try:\n",
    "                # Fallback to a tiny model\n",
    "                self.model_name = \"gpt2\"\n",
    "                self.pipeline = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "                print(f\"‚úì Fallback model (GPT-2) loaded\")\n",
    "            except:\n",
    "                print(\"‚ùå Could not load any local model\")\n",
    "    \n",
    "    def generate_text(self, \n",
    "                     prompt: str,\n",
    "                     max_length: int = 100,\n",
    "                     temperature: float = 0.7,\n",
    "                     num_return_sequences: int = 1) -> Dict:\n",
    "        \"\"\"\n",
    "        Generate text using local model\n",
    "        \"\"\"\n",
    "        if not self.pipeline:\n",
    "            return {\"error\": \"Model not loaded\"}\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Monitor memory before generation\n",
    "            memory_before = psutil.virtual_memory().used / (1024**3)  # GB\n",
    "            \n",
    "            outputs = self.pipeline(\n",
    "                prompt,\n",
    "                max_length=max_length,\n",
    "                temperature=temperature,\n",
    "                num_return_sequences=num_return_sequences,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                do_sample=True,\n",
    "                truncation=True\n",
    "            )\n",
    "            \n",
    "            end_time = time.time()\n",
    "            memory_after = psutil.virtual_memory().used / (1024**3)  # GB\n",
    "            \n",
    "            result = {\n",
    "                \"generated_text\": outputs[0][\"generated_text\"],\n",
    "                \"model\": self.model_name,\n",
    "                \"prompt\": prompt,\n",
    "                \"generation_time\": end_time - start_time,\n",
    "                \"temperature\": temperature,\n",
    "                \"max_length\": max_length,\n",
    "                \"memory_used_gb\": memory_after - memory_before,\n",
    "                \"device\": \"GPU\" if torch.cuda.is_available() else \"CPU\"\n",
    "            }\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"error\": str(e),\n",
    "                \"model\": self.model_name,\n",
    "                \"prompt\": prompt\n",
    "            }\n",
    "\n",
    "# Initialize local model (if available)\n",
    "if TRANSFORMERS_AVAILABLE:\n",
    "    print(\"Initializing local model...\")\n",
    "    local_model = LocalLLMInference(\"gpt2\")  # Using GPT-2 for quick testing\n",
    "    \n",
    "    if local_model.pipeline:\n",
    "        # Test local model\n",
    "        test_result = local_model.generate_text(\"The future of AI is\", max_length=50)\n",
    "        if \"error\" not in test_result:\n",
    "            print(f\"‚úì Local model test successful!\")\n",
    "            print(f\"  Response: {test_result['generated_text']}\")\n",
    "            print(f\"  Time: {test_result['generation_time']:.2f}s\")\n",
    "            print(f\"  Device: {test_result['device']}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Local model test failed: {test_result['error']}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Local models not available - using cloud models only\")\n",
    "    local_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb1844d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Inference with Our Processed Data\n",
    "print(\"=== INFERENCE TESTING ===\")\n",
    "\n",
    "def create_test_prompts(data_samples: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Create various test prompts from our processed data\n",
    "    \"\"\"\n",
    "    prompts = []\n",
    "    \n",
    "    for sample in data_samples[:5]:  # Test with first 5 samples\n",
    "        text = sample['text']\n",
    "        category = sample['category']\n",
    "        \n",
    "        # Different types of prompts to test various capabilities\n",
    "        test_cases = [\n",
    "            {\n",
    "                \"prompt\": f\"Summarize this {category} text in one sentence:\\n\\n{text}\",\n",
    "                \"task\": \"summarization\",\n",
    "                \"category\": category,\n",
    "                \"source\": sample['source_file']\n",
    "            },\n",
    "            {\n",
    "                \"prompt\": f\"What is the main topic of this {category} text?\\n\\n{text}\",\n",
    "                \"task\": \"topic_identification\", \n",
    "                \"category\": category,\n",
    "                \"source\": sample['source_file']\n",
    "            },\n",
    "            {\n",
    "                \"prompt\": f\"Based on this {category} text, answer: What insights can be drawn?\\n\\n{text}\",\n",
    "                \"task\": \"insight_generation\",\n",
    "                \"category\": category,\n",
    "                \"source\": sample['source_file']\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        prompts.extend(test_cases)\n",
    "    \n",
    "    return prompts\n",
    "\n",
    "def run_inference_tests(model, prompts: List[Dict], model_name: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Run inference tests on the model\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, prompt_data in enumerate(prompts):\n",
    "        print(f\"Running test {i+1}/{len(prompts)} - {prompt_data['task']}\")\n",
    "        \n",
    "        # Generate response\n",
    "        if model_name == \"cloud\":\n",
    "            result = model.generate_text(\n",
    "                prompt_data[\"prompt\"],\n",
    "                system_message=f\"You are an expert in {prompt_data['category']} analysis.\",\n",
    "                temperature=0.7,\n",
    "                max_tokens=150\n",
    "            )\n",
    "        else:  # local model\n",
    "            result = model.generate_text(\n",
    "                prompt_data[\"prompt\"],\n",
    "                max_length=len(prompt_data[\"prompt\"].split()) + 50,\n",
    "                temperature=0.7\n",
    "            )\n",
    "        \n",
    "        # Store results\n",
    "        test_result = {\n",
    "            **prompt_data,\n",
    "            **result,\n",
    "            \"test_id\": i + 1,\n",
    "            \"model_type\": model_name,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        results.append(test_result)\n",
    "        \n",
    "        # Brief pause between requests\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Create test prompts\n",
    "test_prompts = create_test_prompts(test_data)\n",
    "print(f\"Created {len(test_prompts)} test prompts\")\n",
    "\n",
    "# Show sample prompts\n",
    "print(f\"\\nSample prompts:\")\n",
    "for i, prompt in enumerate(test_prompts[:2]):\n",
    "    print(f\"\\n{i+1}. Task: {prompt['task']}\")\n",
    "    print(f\"   Category: {prompt['category']}\")\n",
    "    print(f\"   Prompt: {prompt['prompt'][:100]}...\")\n",
    "\n",
    "all_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db89f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Cloud Model Inference Tests\n",
    "if cloud_model.client:\n",
    "    print(\"\\n=== TESTING CLOUD MODEL ===\")\n",
    "    cloud_results = run_inference_tests(cloud_model, test_prompts[:6], \"cloud\")  # Test subset for demo\n",
    "    all_results.extend(cloud_results)\n",
    "    \n",
    "    print(f\"‚úì Completed {len(cloud_results)} cloud model tests\")\n",
    "    \n",
    "    # Show sample results\n",
    "    for result in cloud_results[:2]:\n",
    "        print(f\"\\nüìù Test: {result['task']} | Category: {result['category']}\")\n",
    "        print(f\"ü§ñ Model: {result['model']}\")\n",
    "        print(f\"‚ùì Prompt: {result['prompt'][:80]}...\")\n",
    "        print(f\"üí¨ Response: {result.get('generated_text', 'Error')[:120]}...\")\n",
    "        print(f\"‚è±Ô∏è Time: {result.get('generation_time', 0):.2f}s\")\n",
    "        if result.get('usage', {}).get('total_tokens'):\n",
    "            print(f\"üéØ Tokens: {result['usage']['total_tokens']}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping cloud model tests - not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d856ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Local Model Inference Tests\n",
    "if TRANSFORMERS_AVAILABLE and local_model and local_model.pipeline:\n",
    "    print(\"\\n=== TESTING LOCAL MODEL ===\")\n",
    "    local_results = run_inference_tests(local_model, test_prompts[:3], \"local\")  # Fewer tests for local\n",
    "    all_results.extend(local_results)\n",
    "    \n",
    "    print(f\"‚úì Completed {len(local_results)} local model tests\")\n",
    "    \n",
    "    # Show sample results\n",
    "    for result in local_results[:2]:\n",
    "        print(f\"\\nüìù Test: {result['task']} | Category: {result['category']}\")\n",
    "        print(f\"ü§ñ Model: {result['model']}\")\n",
    "        print(f\"‚ùì Prompt: {result['prompt'][:80]}...\")\n",
    "        print(f\"üí¨ Response: {result.get('generated_text', 'Error')[:120]}...\")\n",
    "        print(f\"‚è±Ô∏è Time: {result.get('generation_time', 0):.2f}s\")\n",
    "        print(f\"üíæ Device: {result.get('device', 'Unknown')}\")\n",
    "    \n",
    "    # Clean up memory\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping local model tests - not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5a3076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark Performance Analysis\n",
    "print(\"\\n=== PERFORMANCE ANALYSIS ===\")\n",
    "\n",
    "def analyze_performance(results: List[Dict]) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze performance metrics from inference results\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        return {\"error\": \"No results to analyze\"}\n",
    "    \n",
    "    # Separate by model type\n",
    "    cloud_results = [r for r in results if r.get('model_type') == 'cloud' and 'error' not in r]\n",
    "    local_results = [r for r in results if r.get('model_type') == 'local' and 'error' not in r]\n",
    "    \n",
    "    analysis = {\n",
    "        \"total_tests\": len(results),\n",
    "        \"successful_tests\": len([r for r in results if 'error' not in r]),\n",
    "        \"failed_tests\": len([r for r in results if 'error' in r])\n",
    "    }\n",
    "    \n",
    "    # Cloud model performance\n",
    "    if cloud_results:\n",
    "        generation_times = [r['generation_time'] for r in cloud_results]\n",
    "        token_counts = [r.get('usage', {}).get('total_tokens', 0) for r in cloud_results if r.get('usage')]\n",
    "        \n",
    "        analysis['cloud_model'] = {\n",
    "            \"test_count\": len(cloud_results),\n",
    "            \"avg_generation_time\": np.mean(generation_times),\n",
    "            \"min_generation_time\": np.min(generation_times),\n",
    "            \"max_generation_time\": np.max(generation_times),\n",
    "            \"avg_tokens\": np.mean(token_counts) if token_counts else None,\n",
    "            \"total_tokens\": sum(token_counts) if token_counts else None\n",
    "        }\n",
    "    \n",
    "    # Local model performance  \n",
    "    if local_results:\n",
    "        generation_times = [r['generation_time'] for r in local_results]\n",
    "        \n",
    "        analysis['local_model'] = {\n",
    "            \"test_count\": len(local_results),\n",
    "            \"avg_generation_time\": np.mean(generation_times),\n",
    "            \"min_generation_time\": np.min(generation_times),\n",
    "            \"max_generation_time\": np.max(generation_times),\n",
    "            \"device\": local_results[0].get('device', 'Unknown')\n",
    "        }\n",
    "    \n",
    "    # Task performance\n",
    "    task_performance = {}\n",
    "    for result in results:\n",
    "        if 'error' not in result:\n",
    "            task = result['task']\n",
    "            if task not in task_performance:\n",
    "                task_performance[task] = []\n",
    "            task_performance[task].append(result['generation_time'])\n",
    "    \n",
    "    analysis['task_performance'] = {}\n",
    "    for task, times in task_performance.items():\n",
    "        analysis['task_performance'][task] = {\n",
    "            \"avg_time\": np.mean(times),\n",
    "            \"test_count\": len(times)\n",
    "        }\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Run performance analysis\n",
    "if all_results:\n",
    "    performance = analyze_performance(all_results)\n",
    "    \n",
    "    print(f\"Performance Analysis Results:\")\n",
    "    print(f\"Total tests: {performance['total_tests']}\")\n",
    "    print(f\"Successful: {performance['successful_tests']}\")\n",
    "    print(f\"Failed: {performance['failed_tests']}\")\n",
    "    \n",
    "    # Cloud model performance\n",
    "    if 'cloud_model' in performance:\n",
    "        cloud_perf = performance['cloud_model']\n",
    "        print(f\"\\nüåê Cloud Model Performance:\")\n",
    "        print(f\"  Tests: {cloud_perf['test_count']}\")\n",
    "        print(f\"  Avg time: {cloud_perf['avg_generation_time']:.2f}s\")\n",
    "        print(f\"  Time range: {cloud_perf['min_generation_time']:.2f}s - {cloud_perf['max_generation_time']:.2f}s\")\n",
    "        if cloud_perf.get('avg_tokens'):\n",
    "            print(f\"  Avg tokens: {cloud_perf['avg_tokens']:.0f}\")\n",
    "            print(f\"  Total tokens: {cloud_perf['total_tokens']}\")\n",
    "    \n",
    "    # Local model performance\n",
    "    if 'local_model' in performance:\n",
    "        local_perf = performance['local_model']\n",
    "        print(f\"\\nüíª Local Model Performance:\")\n",
    "        print(f\"  Tests: {local_perf['test_count']}\")\n",
    "        print(f\"  Avg time: {local_perf['avg_generation_time']:.2f}s\")\n",
    "        print(f\"  Time range: {local_perf['min_generation_time']:.2f}s - {local_perf['max_generation_time']:.2f}s\")\n",
    "        print(f\"  Device: {local_perf['device']}\")\n",
    "    \n",
    "    # Task performance\n",
    "    print(f\"\\nüìä Performance by Task:\")\n",
    "    for task, perf in performance['task_performance'].items():\n",
    "        print(f\"  {task}: {perf['avg_time']:.2f}s avg ({perf['test_count']} tests)\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No results available for analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1eda36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Results and Generate Report\n",
    "print(\"\\n=== SAVING RESULTS ===\")\n",
    "\n",
    "def save_inference_results(results: List[Dict], performance: Dict):\n",
    "    \"\"\"\n",
    "    Save inference results and performance analysis\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Save detailed results\n",
    "    results_file = BASE_DIR / f\"inference_results_{timestamp}.json\"\n",
    "    with open(results_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump({\n",
    "            \"timestamp\": timestamp,\n",
    "            \"results\": results,\n",
    "            \"performance\": performance,\n",
    "            \"metadata\": {\n",
    "                \"total_tests\": len(results),\n",
    "                \"data_source\": str(DATA_FILE),\n",
    "                \"models_tested\": list(set(r.get('model_type', 'unknown') for r in results))\n",
    "            }\n",
    "        }, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"‚úì Results saved to: {results_file}\")\n",
    "    \n",
    "    # Create summary CSV\n",
    "    summary_data = []\n",
    "    for result in results:\n",
    "        summary_data.append({\n",
    "            'test_id': result.get('test_id'),\n",
    "            'model_type': result.get('model_type'),\n",
    "            'model': result.get('model'),\n",
    "            'task': result.get('task'),\n",
    "            'category': result.get('category'),\n",
    "            'generation_time': result.get('generation_time'),\n",
    "            'success': 'error' not in result,\n",
    "            'tokens_used': result.get('usage', {}).get('total_tokens') if result.get('usage') else None\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_file = BASE_DIR / f\"inference_summary_{timestamp}.csv\"\n",
    "    summary_df.to_csv(summary_file, index=False)\n",
    "    \n",
    "    print(f\"‚úì Summary saved to: {summary_file}\")\n",
    "    \n",
    "    return results_file, summary_file\n",
    "\n",
    "# Save results if we have any\n",
    "if all_results:\n",
    "    results_file, summary_file = save_inference_results(all_results, performance)\n",
    "    \n",
    "    # Display summary table\n",
    "    print(f\"\\nüìã INFERENCE TESTING SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    summary_df = pd.read_csv(summary_file)\n",
    "    \n",
    "    # Group by model type and task\n",
    "    if not summary_df.empty:\n",
    "        print(\"\\nResults by Model and Task:\")\n",
    "        pivot = summary_df.groupby(['model_type', 'task']).agg({\n",
    "            'generation_time': ['mean', 'count'],\n",
    "            'success': 'sum'\n",
    "        }).round(2)\n",
    "        print(pivot)\n",
    "        \n",
    "        print(f\"\\nSuccess Rate by Model:\")\n",
    "        success_rate = summary_df.groupby('model_type')['success'].agg(['sum', 'count'])\n",
    "        success_rate['rate'] = (success_rate['sum'] / success_rate['count'] * 100).round(1)\n",
    "        print(success_rate)\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No results to save\")\n",
    "\n",
    "print(f\"\\nüéâ INFERENCE TESTING COMPLETED!\")\n",
    "print(f\"Files saved to: {BASE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b579126",
   "metadata": {},
   "source": [
    "## Next Steps & Recommendations\n",
    "\n",
    "Based on your inference testing results, here are recommendations for different use cases:\n",
    "\n",
    "### üåê **Cloud Models (GitHub Models)**\n",
    "**Best for:**\n",
    "- Production applications\n",
    "- High-quality responses\n",
    "- Cost-effective scaling\n",
    "- No local compute requirements\n",
    "\n",
    "**Recommended models:**\n",
    "- `openai/gpt-4.1-mini` - Best balance of quality and cost\n",
    "- `openai/gpt-4.1` - Maximum quality for critical tasks  \n",
    "- `microsoft/phi-4-mini-instruct` - Efficient for specific tasks\n",
    "\n",
    "### üíª **Local Models**\n",
    "**Best for:**\n",
    "- Privacy-sensitive applications\n",
    "- Offline deployment\n",
    "- Cost control at scale\n",
    "- Custom fine-tuning\n",
    "\n",
    "**Recommended approaches:**\n",
    "- Use quantized models for better performance\n",
    "- Consider GPU acceleration for faster inference\n",
    "- Fine-tune smaller models on your specific domain\n",
    "\n",
    "### üéØ **For Your Use Case**\n",
    "Based on your processed data categories:\n",
    "\n",
    "1. **Business Analysis**: Use cloud models for complex reasoning\n",
    "2. **Content Summarization**: Local models can handle well\n",
    "3. **Domain-specific Tasks**: Consider fine-tuning on your data\n",
    "\n",
    "### üìà **Performance Optimization**\n",
    "- **Batch processing** for multiple requests\n",
    "- **Caching** for repeated similar queries\n",
    "- **Prompt engineering** to improve output quality\n",
    "- **Response streaming** for better user experience\n",
    "\n",
    "### üîß **Production Deployment**\n",
    "```python\n",
    "# Example production setup\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "import os\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=\"https://models.github.ai/inference\",\n",
    "    credential=AzureKeyCredential(os.environ[\"GITHUB_TOKEN\"])\n",
    ")\n",
    "\n",
    "def production_inference(text, category):\n",
    "    response = client.complete(\n",
    "        messages=[\n",
    "            SystemMessage(f\"You are an expert {category} analyst.\"),\n",
    "            UserMessage(f\"Analyze: {text}\")\n",
    "        ],\n",
    "        model=\"openai/gpt-4.1-mini\",\n",
    "        temperature=0.3,  # Lower for consistency\n",
    "        max_tokens=200\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
