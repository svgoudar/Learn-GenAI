{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a7ef39d",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## Supervised Fine-Tuning (SFT)\n",
    "\n",
    "**Supervised Fine-Tuning (SFT)** is the process of training a *pretrained* Large Language Model (LLM) on **labeled instruction–response pairs** so that it can learn to **follow instructions**, behave like an **assistant**, and perform **specific tasks** reliably.\n",
    "\n",
    "SFT transforms a raw pretrained model (which only predicts the next token) into an **instruction-following model**.\n",
    "\n",
    "---\n",
    "\n",
    "### Why SFT Is Needed\n",
    "\n",
    "A pretrained model:\n",
    "\n",
    "* learns language patterns, grammar, facts\n",
    "* predicts the next token in text\n",
    "* does **not** understand instructions\n",
    "* does **not** know how to respond like an assistant\n",
    "\n",
    "Example of a base model:\n",
    "\n",
    "```\n",
    "User: Summarize this paragraph.\n",
    "Model: Summarize this paragraph by saying that...\n",
    "```\n",
    "\n",
    "→ It *continues* the user text instead of answering.\n",
    "\n",
    "SFT fixes this.\n",
    "\n",
    "---\n",
    "\n",
    "### What SFT Actually Does\n",
    "\n",
    "During SFT, the model is trained on examples like:\n",
    "\n",
    "```\n",
    "Instruction: Translate to French\n",
    "Input: How are you?\n",
    "Output: Comment allez-vous ?\n",
    "```\n",
    "\n",
    "The model learns:\n",
    "\n",
    "* how to interpret a user request\n",
    "* how to generate the correct style of answer\n",
    "* how to provide structured outputs\n",
    "* how to follow the expected conversational format\n",
    "\n",
    "---\n",
    "\n",
    "### How SFT Works (Process)\n",
    "\n",
    "#### **1. Prepare an instruction dataset**\n",
    "\n",
    "Examples of:\n",
    "\n",
    "* summarization\n",
    "* classification\n",
    "* translation\n",
    "* question answering\n",
    "* reasoning\n",
    "* coding\n",
    "* safe refusal examples\n",
    "\n",
    "Each sample has:\n",
    "\n",
    "```\n",
    "instruction\n",
    "input (optional)\n",
    "response (target label)\n",
    "```\n",
    "\n",
    "#### **2. Convert to a chat template**\n",
    "\n",
    "Such as:\n",
    "\n",
    "```\n",
    "### Instruction:\n",
    "Summarize this text.\n",
    "### Input:\n",
    "Cats are mammals...\n",
    "### Response:\n",
    "Cats are mammals that...\n",
    "```\n",
    "\n",
    "#### **3. Fine-tune the model**\n",
    "\n",
    "Use **supervised learning** with cross-entropy loss:\n",
    "\n",
    "$$\n",
    "\\text{Train the model to predict the correct response tokens.}\n",
    "$$\n",
    "\n",
    "This adjusts the model’s behavior to match the dataset examples.\n",
    "\n",
    "#### **4. (Optional) Apply RLHF / DPO**\n",
    "\n",
    "After SFT, preference optimization further improves:\n",
    "\n",
    "* helpfulness\n",
    "* safety\n",
    "* correctness\n",
    "\n",
    "---\n",
    "\n",
    "### Techniques Used for SFT\n",
    "\n",
    "| Method               | Description                      | Usage                            |\n",
    "| -------------------- | -------------------------------- | -------------------------------- |\n",
    "| **Full Fine-Tuning** | Update *all* model weights       | Highest quality, expensive       |\n",
    "| **LoRA**             | Train only small adapter modules | Efficient, widely used           |\n",
    "| **QLoRA**            | LoRA + 4-bit quantization        | Train large models on small GPUs |\n",
    "\n",
    "---\n",
    "\n",
    "###  What SFT Achieves\n",
    "\n",
    "SFT creates a model that:\n",
    "\n",
    "* follows human instructions\n",
    "* responds in a helpful, conversational manner\n",
    "* performs domain-specific tasks\n",
    "* produces structured outputs (JSON, code)\n",
    "* generalizes across tasks\n",
    "\n",
    "Without SFT, LLMs would *not* behave like chat assistants.\n",
    "\n",
    "---\n",
    "\n",
    "**One-Sentence Summary**\n",
    "\n",
    "**Supervised Fine-Tuning (SFT) trains a pretrained LLM on labeled instruction–response examples so it learns to follow instructions, solve tasks, and act like an assistant rather than a text-completion model.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee0fafe",
   "metadata": {},
   "source": [
    "### **Intuition Behind SFT (Supervised Fine-Tuning)**\n",
    "\n",
    "#### **Intuition 1 — A pretrained LLM is a “completion engine,” not a “helper.”**\n",
    "\n",
    "A pretrained model learns **only one skill**:\n",
    "\n",
    "> Predict the next token.\n",
    "\n",
    "So if the user says:\n",
    "\n",
    "```\n",
    "Summarize this text:\n",
    "```\n",
    "\n",
    "the model thinks:\n",
    "\n",
    "> “What token usually comes after ‘Summarize this text:’ in the training data?”\n",
    "\n",
    "Often the continuation is **another instruction** or **irrelevant text**.\n",
    "\n",
    "It has knowledge, but **no idea how to behave**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Intuition 2 — SFT acts like showing the model “examples of good behavior.”**\n",
    "\n",
    "During SFT, the model sees pairs like:\n",
    "\n",
    "```\n",
    "User: Explain how airplanes fly.\n",
    "Assistant: Airplanes fly because...\n",
    "```\n",
    "\n",
    "After thousands of these examples, the model learns:\n",
    "\n",
    "* how to respond\n",
    "* how long to respond\n",
    "* what tone to use\n",
    "* what role it should play\n",
    "* how instructions map to answers\n",
    "\n",
    "It's similar to teaching a student by **showing solved examples**.\n",
    "\n",
    "No guessing — just follow patterns.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Intuition 3 — SFT unlocks abilities the model already has.**\n",
    "\n",
    "Pretraining gives the model:\n",
    "\n",
    "* facts\n",
    "* reasoning ability\n",
    "* language fluency\n",
    "\n",
    "But it doesn’t know **when** to use each skill.\n",
    "\n",
    "SFT tells the model:\n",
    "\n",
    "> “When the user asks for a summary, use your knowledge of summarizing.\n",
    "> When the user asks for translation, use your multilingual knowledge.”\n",
    "\n",
    "SFT is like adding a **user manual** to a machine that already has powerful internal capabilities.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Intuition 4 — The model learns the *shape* of a helpful response.**\n",
    "\n",
    "SFT examples teach:\n",
    "\n",
    "* Start directly answering the question\n",
    "* Be concise when asked\n",
    "* Use step-by-step reasoning when appropriate\n",
    "* Wrap answers in JSON or code when required\n",
    "* Follow conversational structure\n",
    "* Stop at the right point\n",
    "\n",
    "Before SFT, the model produces free-form continuation.\n",
    "After SFT, it produces **task-specific, structured behavior**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Intuition 5 — SFT teaches the model to generalize across tasks.**\n",
    "\n",
    "If the SFT dataset contains many different tasks:\n",
    "\n",
    "* summarization\n",
    "* reasoning\n",
    "* translation\n",
    "* coding\n",
    "* chatting\n",
    "* multi-turn conversations\n",
    "\n",
    "The model learns a **single universal rule**:\n",
    "\n",
    "> “Read the instruction → understand user intent → produce the kind of answer shown in examples.”\n",
    "\n",
    "Even if a user asks something **never seen before**, the model generalizes the pattern.\n",
    "\n",
    "---\n",
    "\n",
    "####  **Intuition 6 — SFT changes the model’s *behavior*, not its *knowledge*.**\n",
    "\n",
    "Pretraining = “What do I know?”\n",
    "SFT = “How should I act when users talk to me?”\n",
    "\n",
    "This is why SFT is small (often 1–4% of total compute) but hugely impactful.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Intuition 7 — SFT aligns the model with *human expectations*.**\n",
    "\n",
    "Without SFT, the model:\n",
    "\n",
    "* rambles\n",
    "* ignores instructions\n",
    "* continues text endlessly\n",
    "* does not format answers\n",
    "* does not stop\n",
    "\n",
    "With SFT, the model:\n",
    "\n",
    "* listens\n",
    "* responds\n",
    "* formats\n",
    "* follows rules\n",
    "* engages in a conversation\n",
    "\n",
    "This makes the model feel like an **assistant**, not a text generator.\n",
    "\n",
    "---\n",
    "\n",
    "#### Simple Analogy\n",
    "\n",
    "##### **Pretraining:**\n",
    "\n",
    "You read thousands of books and learn everything about the world.\n",
    "\n",
    "##### **SFT:**\n",
    "\n",
    "Someone finally shows you **how to answer questions** based on what you already know.\n",
    "\n",
    "---\n",
    "\n",
    "**One-Sentence Intuition**\n",
    "\n",
    "**SFT teaches an LLM how to behave like a helpful assistant by showing it thousands of examples of how instructions should be answered, turning raw knowledge into usable behavior.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe5fccd",
   "metadata": {},
   "source": [
    "### Supervised Fine-Tuning (SFT) \n",
    "\n",
    "We will:\n",
    "\n",
    "1. Create a small instruction dataset\n",
    "2. Convert it into prompt–response format\n",
    "3. Load a pretrained LLM in **4-bit**\n",
    "4. Apply **QLoRA** adapters\n",
    "5. Train with `SFTTrainer`\n",
    "6. Generate text with the tuned model\n",
    "\n",
    "This is a **real SFT pipeline** used in industry.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Install dependencies\n",
    "\n",
    "```bash\n",
    "pip install transformers datasets peft accelerate bitsandbytes trl\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Create a tiny instruction dataset\n",
    "\n",
    "```python\n",
    "from datasets import Dataset\n",
    "\n",
    "data = [\n",
    "    {\n",
    "        \"instruction\": \"Summarize the text.\",\n",
    "        \"input\": \"Large language models are trained using unsupervised learning.\",\n",
    "        \"output\": \"Large language models learn patterns from unlabeled data.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Translate to French.\",\n",
    "        \"input\": \"How are you?\",\n",
    "        \"output\": \"Comment allez-vous ?\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Explain like I'm five.\",\n",
    "        \"input\": \"What is gravity?\",\n",
    "        \"output\": \"Gravity is a force that pulls things down, like when you drop a ball.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "dataset = Dataset.from_list(data)\n",
    "dataset\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Convert dataset to training format (prompt + response)\n",
    "\n",
    "```python\n",
    "def format_example(example):\n",
    "    prompt = f\"\"\"### Instruction:\n",
    "{example['instruction']}\n",
    "\n",
    "### Input:\n",
    "{example['input']}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": example[\"output\"]\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(format_example)\n",
    "dataset\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Load tokenizer + base LLM in 4-bit (QLoRA)\n",
    "\n",
    "Here we use a small model `facebook/opt-350m` so you can run it anywhere.\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_NAME = \"facebook/opt-350m\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    load_in_4bit=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Apply LoRA adapters (Parameter-efficient SFT)\n",
    "\n",
    "```python\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. Tokenize prompts for supervised training\n",
    "\n",
    "```python\n",
    "def tokenize(example):\n",
    "    full_text = example[\"prompt\"] + example[\"response\"]\n",
    "    return tokenizer(full_text, truncation=True, max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 7. Train using TRL’s SFTTrainer (best practice)\n",
    "\n",
    "```python\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./sft_model\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=3,\n",
    "    fp16=True,\n",
    "    logging_steps=1,\n",
    "    max_steps=30\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    dataset_text_field=None,\n",
    "    max_seq_length=512,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "This updates **only LoRA parameters**, making tuning extremely fast and memory-efficient.\n",
    "\n",
    "---\n",
    "\n",
    "#### 8. Save the tuned model\n",
    "\n",
    "```python\n",
    "trainer.model.save_pretrained(\"sft_lora_model\")\n",
    "tokenizer.save_pretrained(\"sft_lora_model\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 9. Test the Instruction-Tuned Model\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"sft_lora_model\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=100\n",
    ")\n",
    "\n",
    "prompt = \"\"\"### Instruction:\n",
    "Explain like I'm five.\n",
    "\n",
    "### Input:\n",
    "Why is the sky blue?\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "print(pipe(prompt)[0][\"generated_text\"])\n",
    "```\n",
    "\n",
    "You will now see the model respond **according to the SFT examples**, behaving like a simple assistant.\n",
    "\n",
    "---\n",
    "\n",
    "#### What This Demo Shows\n",
    "\n",
    "* How to **create an instruction dataset**\n",
    "* How to **prepare prompts**\n",
    "* How to **load a pretrained model in 4-bit**\n",
    "* How to apply **QLoRA adapters**\n",
    "* How to run **SFT using TRL’s SFTTrainer**\n",
    "* How to **test** the instruction-following ability\n",
    "\n",
    "This is the **same workflow** used to build:\n",
    "\n",
    "* LLaMA-2-Chat\n",
    "* Mistral-Instruct\n",
    "* Falcon-Instruct\n",
    "* GPT-Instruct models (scaled up)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
