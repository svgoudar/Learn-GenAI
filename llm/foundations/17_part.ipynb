{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "152a25cd",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## PPO (Proximal Policy Optimization)\n",
    "\n",
    "**PPO** is a reinforcement learning algorithm that improves a policy (an agent’s behavior) in a way that is:\n",
    "\n",
    "* **stable**\n",
    "* **efficient**\n",
    "* **safe**\n",
    "* **easy to implement**\n",
    "* **high-performance**\n",
    "\n",
    "PPO is the algorithm used by **OpenAI** to train ChatGPT from human feedback (RLHF).\n",
    "\n",
    "PPO solves a major problem in RL:\n",
    "\n",
    "> **How to update a policy without letting it change too much and break training stability?**\n",
    "\n",
    "It does this by restricting updates to stay within a *safe* region — the “proximal” part.\n",
    "\n",
    "---\n",
    "\n",
    "### Why PPO Is Needed (Intuition)\n",
    "\n",
    "In classic policy gradient RL:\n",
    "\n",
    "* You take steps to increase rewards\n",
    "* Big gradient steps can **destroy** the existing policy\n",
    "* Training becomes unstable or collapses\n",
    "\n",
    "PPO fixes this.\n",
    "\n",
    "### Key idea:\n",
    "\n",
    "> Take the *biggest possible improvement step* **without deviating too far** from the original policy.\n",
    "\n",
    "It is like saying:\n",
    "\n",
    "* “Improve your behavior… but **don’t change too fast**.”\n",
    "\n",
    "---\n",
    "\n",
    "### PPO in the RLHF Context (ChatGPT)\n",
    "\n",
    "In RLHF:\n",
    "\n",
    "1. The **LLM** = policy\n",
    "2. **Reward Model (RM)** gives reward to generated answers\n",
    "3. PPO updates the LLM to **maximize reward**\n",
    "\n",
    "The objective becomes:\n",
    "\n",
    "```\n",
    "Make the model more likely to produce high-reward answers,\n",
    "while staying close to the original SFT model.\n",
    "```\n",
    "\n",
    "This prevents:\n",
    "\n",
    "* hallucinations\n",
    "* toxic drift\n",
    "* over-optimization\n",
    "* forgetting instruction-following behavior\n",
    "\n",
    "---\n",
    "\n",
    "### PPO Core Formula (Simple Version)\n",
    "\n",
    "PPO modifies the policy update using a **clipped objective**:\n",
    "\n",
    "$$\n",
    "L_{PPO} = \\min \\Big(\n",
    "r(\\theta) A,\\\n",
    "\\text{clip}(r(\\theta),\\ 1-\\epsilon,\\ 1+\\epsilon) A\n",
    "\\Big)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $r(\\theta) = \\frac{\\pi_\\theta(a|s)}{\\pi_{old}(a|s)}$\n",
    "  Ratio of new policy to old policy\n",
    "\n",
    "* $A$ = advantage (how good an action was)\n",
    "\n",
    "* $\\epsilon$ = small number like 0.1\n",
    "\n",
    "### Meaning:\n",
    "\n",
    "* If the update is too big → clipping stops it\n",
    "* If it’s safe → policy improves normally\n",
    "\n",
    "This ensures **stable**, **bounded** updates.\n",
    "\n",
    "---\n",
    "\n",
    "### PPO’s Key Innovations\n",
    "\n",
    "#### **1. Clipped Ratio**\n",
    "\n",
    "Prevents big destructive updates.\n",
    "\n",
    "#### **2. KL Constraint**\n",
    "\n",
    "Ensures the new model stays close to the old one.\n",
    "\n",
    "#### **3. Surrogate Objective**\n",
    "\n",
    "Optimizes expected reward in a stable way.\n",
    "\n",
    "#### **4. Multiple Mini-Batch Updates**\n",
    "\n",
    "Efficient and GPU-friendly.\n",
    "\n",
    "#### **5. On-Policy but Sample-Efficient**\n",
    "\n",
    "Unlike vanilla policy gradient, PPO reuses data multiple times.\n",
    "\n",
    "---\n",
    "\n",
    "### ⭐ 5. PPO Training Loop (Conceptual)\n",
    "\n",
    "#### Step 1 — Generate Data\n",
    "\n",
    "The model generates several candidate answers to prompts.\n",
    "\n",
    "#### Step 2 — Reward Scoring\n",
    "\n",
    "A reward model scores answers:\n",
    "\n",
    "* Safe?\n",
    "* Helpful?\n",
    "* Correct?\n",
    "* Not toxic?\n",
    "\n",
    "#### Step 3 — Policy Update Using PPO\n",
    "\n",
    "PPO adjusts the LLM:\n",
    "\n",
    "* increase probability of high-scored answers\n",
    "* decrease probability of low-scored answers\n",
    "* keep the model close to the original policy\n",
    "\n",
    "This loop repeats thousands of times.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Simple Example (Intuition)\n",
    "\n",
    "Prompt:\n",
    "\n",
    "```\n",
    "Explain gravity.\n",
    "```\n",
    "\n",
    "The model generates 3 answers:\n",
    "\n",
    "| Answer              | Reward |\n",
    "| ------------------- | ------ |\n",
    "| A (good scientific) | +6     |\n",
    "| B (half wrong)      | +2     |\n",
    "| C (nonsense)        | -3     |\n",
    "\n",
    "PPO update:\n",
    "\n",
    "* Increase probability of **A**\n",
    "* Slightly increase **B** (but less)\n",
    "* Decrease probability of **C**\n",
    "* Prevent the model from changing too much\n",
    "* Ensure it still follows instructions\n",
    "\n",
    "Over time → The model becomes aligned.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. PPO in ChatGPT Training\n",
    "\n",
    "PPO is used in RLHF:\n",
    "\n",
    "```\n",
    "Base Pretrained Model\n",
    "→ SFT Model\n",
    "→ PPO using Reward Model\n",
    "→ Final ChatGPT\n",
    "```\n",
    "\n",
    "During PPO:\n",
    "\n",
    "* High-quality answers get high rewards\n",
    "* Harmful or hallucinated answers get low rewards\n",
    "* Model is nudged towards preferred behavior\n",
    "* Safety improves\n",
    "* Reasoning improves\n",
    "* Chat-like behavior emerges\n",
    "\n",
    "---\n",
    "\n",
    "### 8. PPO vs DPO\n",
    "\n",
    "| Feature            | PPO    | DPO       |\n",
    "| ------------------ | ------ | --------- |\n",
    "| Needs Reward Model | Yes    | No        |\n",
    "| Uses RL            | Yes    | No        |\n",
    "| Training Stability | Medium | High      |\n",
    "| Cost               | High   | Low       |\n",
    "| Simplicity         | Low    | Very high |\n",
    "| Alignment Quality  | High   | High      |\n",
    "\n",
    "Most modern open-source models now use **DPO** because it is much simpler.\n",
    "\n",
    "OpenAI/GPT models still use **PPO + RM**.\n",
    "\n",
    "---\n",
    "\n",
    "**One-Sentence Summary**\n",
    "\n",
    "**PPO is a reinforcement learning algorithm that updates a model’s behavior by increasing good actions and limiting destructive updates, forming the core of RLHF used to train aligned LLMs like ChatGPT.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daf401e",
   "metadata": {},
   "source": [
    "### Demo\n",
    "\n",
    "#### **1. Install Dependencies**\n",
    "\n",
    "```bash\n",
    "pip install gymnasium torch numpy\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. PPO Implementation (Minimal, Complete, Works Anywhere)**\n",
    "\n",
    "```python\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# --------------------------\n",
    "#  Policy + Value Networks\n",
    "# --------------------------\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(4, 64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.policy = nn.Linear(64, 2)   # 2 actions\n",
    "        self.value = nn.Linear(64, 1)    # state value\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.fc(x)\n",
    "        return self.policy(h), self.value(h)\n",
    "\n",
    "# --------------------------\n",
    "#  PPO Agent\n",
    "# --------------------------\n",
    "class PPO:\n",
    "    def __init__(self, lr=3e-4, gamma=0.99, eps_clip=0.2):\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.model = ActorCritic()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state)\n",
    "        logits, value = self.model(state)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        action = dist.sample()\n",
    "\n",
    "        return action.item(), dist.log_prob(action), value\n",
    "\n",
    "    def compute_returns(self, rewards, values, dones, gamma):\n",
    "        returns = []\n",
    "        R = 0\n",
    "        for r, d in zip(reversed(rewards), reversed(dones)):\n",
    "            if d: R = 0\n",
    "            R = r + gamma * R\n",
    "            returns.insert(0, R)\n",
    "        return torch.tensor(returns)\n",
    "\n",
    "    # --------------------------\n",
    "    #  PPO Update (Core Logic)\n",
    "    # --------------------------\n",
    "    def update(self, states, actions, old_log_probs, returns, advantages):\n",
    "        for _ in range(5):  # 5 epochs\n",
    "            logits, values = self.model(states)\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            dist = torch.distributions.Categorical(probs)\n",
    "\n",
    "            new_log_probs = dist.log_prob(actions)\n",
    "            ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "\n",
    "            surr1 = ratio * advantages\n",
    "            surr2 = torch.clamp(ratio, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "\n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "            critic_loss = (returns - values.squeeze())**2\n",
    "\n",
    "            loss = actor_loss + critic_loss.mean()\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "# --------------------------\n",
    "#  Training Loop\n",
    "# --------------------------\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "agent = PPO()\n",
    "\n",
    "episodes = 200\n",
    "\n",
    "for ep in range(episodes):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "\n",
    "    states, actions, log_probs, rewards, values, dones = [], [], [], [], [], []\n",
    "\n",
    "    while not done:\n",
    "        action, log_prob, value = agent.select_action(state)\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # store transitions\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        log_probs.append(log_prob)\n",
    "        rewards.append(reward)\n",
    "        values.append(value.squeeze())\n",
    "        dones.append(done)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    states = torch.FloatTensor(states)\n",
    "    actions = torch.tensor(actions)\n",
    "    old_log_probs = torch.stack(log_probs)\n",
    "    values = torch.stack(values)\n",
    "\n",
    "    returns = agent.compute_returns(rewards, values, dones, agent.gamma)\n",
    "    advantages = returns - values.detach()\n",
    "\n",
    "    # PPO update\n",
    "    agent.update(states, actions, old_log_probs, returns, advantages)\n",
    "\n",
    "    print(f\"Episode {ep+1}, Total Reward: {sum(rewards)}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. What This Code Demonstrates (Mapping PPO Concepts)**\n",
    "\n",
    "| PPO Concept                 | Shown in Code                                |\n",
    "| --------------------------- | -------------------------------------------- |\n",
    "| **Policy network**          | `self.policy`                                |\n",
    "| **Value network**           | `self.value`                                 |\n",
    "| **Action sampling**         | `dist.sample()`                              |\n",
    "| **Old vs new policy ratio** | `ratio = exp(new_log_probs - old_log_probs)` |\n",
    "| **Clipped loss**            | `torch.clamp(ratio, 1-eps, 1+eps)`           |\n",
    "| **Advantage calculation**   | `advantages = returns - values`              |\n",
    "| **Multiple update epochs**  | `for _ in range(5)`                          |\n",
    "| **On-policy learning**      | uses data from the latest episode only       |\n",
    "\n",
    "This is exactly the simplified version of PPO used in LLM RLHF:\n",
    "\n",
    "* **Policy** = LLM\n",
    "* **Value function** = \"critic\" for evaluating responses\n",
    "* **Rewards** = from reward model\n",
    "* **Advantages** = quality above baseline\n",
    "* **Clipping** = protects model from drifting too far\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. What You’ll See When Running**\n",
    "\n",
    "Training starts at:\n",
    "\n",
    "```\n",
    "Total Reward: ~10–20\n",
    "```\n",
    "\n",
    "Over episodes, reward increases:\n",
    "\n",
    "```\n",
    "Total Reward: 80\n",
    "Total Reward: 150\n",
    "Total Reward: 200 (max)\n",
    "```\n",
    "\n",
    "CartPole solved = stable balancing.\n",
    "\n",
    "This is PPO learning **through rewards**, like RLHF—but with a simple environment.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ecf519",
   "metadata": {},
   "source": [
    "| Concept                | What It Teaches                   | Data Needed                     | Algorithm Used | Goal                             | Difficulty |\n",
    "| ---------------------- | --------------------------------- | ------------------------------- | -------------- | -------------------------------- | ---------- |\n",
    "| **Instruction Tuning** | Follow instructions               | Instruction → Response          | Cross-entropy  | Make model task-aware            | Easy       |\n",
    "| **SFT**                | Provide examples of good behavior | Labeled responses               | Cross-entropy  | Make model act like an assistant | Easy       |\n",
    "| **RLHF**               | Learn human preferences           | Preference pairs + reward model | PPO            | Align model to human values      | Hard       |\n",
    "| **PPO**                | Stable policy updates             | Reward signals                  | PPO RL         | Improve model using reward model | Hard       |\n",
    "| **DPO**                | Prefer good answers directly      | Preference pairs                | DPO loss       | Alignment without RL             | Medium     |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
