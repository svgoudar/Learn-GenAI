{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbcaf240",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Quantizations\n",
    "\n",
    "Below is a **clear, complete, LLM-focused explanation** of Quantization and the four major methods used today: **8-bit, 4-bit, AWQ, GPTQ**.\n",
    "This is the exact level used in LLM engineering interviews and production documentation.\n",
    "\n",
    "---\n",
    "\n",
    "### **What Is Quantization in LLMs?**\n",
    "\n",
    "Quantization is the process of **reducing the precision** of model weights (and sometimes activations) from high-bit formats (FP32, FP16, BF16) to **lower-bit formats like INT8 or INT4**.\n",
    "\n",
    "### **Goal:**\n",
    "\n",
    "* Reduce **memory usage**\n",
    "* Reduce **VRAM requirements**\n",
    "* Increase **inference speed**\n",
    "* Allow LLMs to run on smaller hardware (like single GPUs/CPUs)\n",
    "\n",
    "### **What Changes?**\n",
    "\n",
    "Weights normally stored as FP16 (16-bit floating point):\n",
    "\n",
    "```\n",
    "FP16 (16 bits) â†’ INT8 (8 bits) â†’ INT4 (4 bits)\n",
    "```\n",
    "\n",
    "Lower bits â†’ smaller, faster model.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Quantization is Critical for LLMs\n",
    "\n",
    "Example: A 70B model in FP16:\n",
    "\n",
    "```\n",
    "70 billion params Ã— 2 bytes â‰ˆ 140 GB VRAM\n",
    "```\n",
    "\n",
    "Impossible on normal hardware.\n",
    "\n",
    "Quantization reduces memory:\n",
    "\n",
    "| Format                         | Size reduction              |\n",
    "| ------------------------------ | --------------------------- |\n",
    "| **8-bit**                      | ~2Ã— smaller                 |\n",
    "| **4-bit**                      | ~4Ã— smaller                 |\n",
    "| **GPTQ / AWQ optimized 4-bit** | Very close to FP16 accuracy |\n",
    "\n",
    "This is why models like LLaMA-13B can run on a **single 8GB GPU** using 4-bit quantization.\n",
    "\n",
    "---\n",
    "\n",
    "### **8-bit Quantization**\n",
    "\n",
    "\n",
    "#### **What It Is**\n",
    "\n",
    "Weights are stored using **8 bits (INT8)** rather than FP16.\n",
    "\n",
    "#### **Pros**\n",
    "\n",
    "* 2x smaller model\n",
    "* Fast inference\n",
    "* Minimal accuracy drop\n",
    "* Supported natively in Transformers, bitsandbytes\n",
    "\n",
    "#### **Cons**\n",
    "\n",
    "* Still too big for large models (e.g., 70B)\n",
    "* Only weights quantized (not activations)\n",
    "\n",
    "#### **Where Used**\n",
    "\n",
    "* bitsandbytes `load_in_8bit=True`\n",
    "* Intel/NVIDIA INT8 inference\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### **4-bit Quantization**\n",
    "\n",
    "\n",
    "#### **What It Is**\n",
    "\n",
    "Weights stored in **4 bits (INT4 / NF4)**.\n",
    "\n",
    "4-bit techniques include:\n",
    "\n",
    "* **NF4 (Normalized Float 4)**\n",
    "* **FP4**\n",
    "* **INT4**\n",
    "\n",
    "#### **Pros**\n",
    "\n",
    "* 4Ã— smaller model\n",
    "* Enables 13B, 33B, 70B LLMs on consumer GPUs\n",
    "* Good accuracy when using NF4 (special distribution)\n",
    "\n",
    "#### **Cons**\n",
    "\n",
    "* More accuracy drop than INT8\n",
    "* Some hardware struggles with pure INT4 math\n",
    "\n",
    "#### **Where Used**\n",
    "\n",
    "* QLoRA uses **NF4 quantization**\n",
    "* bitsandbytes `load_in_4bit=True`\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### **GPTQ (GPT Quantization)**\n",
    "\n",
    "\n",
    "A **post-training** quantization method designed specifically for LLMs.\n",
    "\n",
    "#### **What GPTQ Does**\n",
    "\n",
    "* Quantizes **each layer optimally**\n",
    "* Minimizes the **error between quantized and original output**\n",
    "* Uses a calibration dataset to match full-precision outputs\n",
    "* Highly optimized for **4-bit inference**\n",
    "\n",
    "#### **Pros**\n",
    "\n",
    "* Best 4-bit inference quality\n",
    "* Fast and accurate\n",
    "* Supports very large models (70B+)\n",
    "* Used in HuggingFace `.gptq` models\n",
    "\n",
    "#### **Cons**\n",
    "\n",
    "* Offline quantization step required\n",
    "* Slower to quantize, but fast to run\n",
    "\n",
    "#### **Where Used**\n",
    "\n",
    "* LM Studio\n",
    "* text-generation-webui\n",
    "* ExLlama, ExLlamaV2 engines\n",
    "* HuggingFace GPTQ models\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### **AWQ (Activation-aware Weight Quantization)**\n",
    "\n",
    "\n",
    "AWQ quantizes **only the weights**, but keeps activations high precision.\n",
    "It focuses on **preserving important weights**.\n",
    "\n",
    "#### **Key Idea**\n",
    "\n",
    "* Identify important channels/weights\n",
    "* Leave those unquantized (or less quantized)\n",
    "* Quantize unimportant weights to INT4\n",
    "\n",
    "#### **Pros**\n",
    "\n",
    "* High accuracy at 4-bit\n",
    "* Better than GPTQ for some models\n",
    "* Very fast inference (supported by vLLM + AWQ)\n",
    "* Stable for extremely large models\n",
    "\n",
    "#### **Cons**\n",
    "\n",
    "* Slightly larger memory than pure 4-bit GPTQ\n",
    "* Not as widely supported (except vLLM, AWQ libraries)\n",
    "\n",
    "#### **Where Used**\n",
    "\n",
    "* vLLM AWQ\n",
    "* llama.cpp (partial)\n",
    "* Commercial inference (AWS, Together, Mosaic)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### **Accuracy Comparison**\n",
    "\n",
    "\n",
    "| Method      | Bits | Speed   | Accuracy     | Notes                |\n",
    "| ----------- | ---- | ------- | ------------ | -------------------- |\n",
    "| FP16        | 16   | Slow    | ðŸ”¥ Best      | Baseline             |\n",
    "| INT8        | 8    | Faster  | ðŸ”¥ Very high | Good trade-off       |\n",
    "| NF4 (QLoRA) | 4    | Fast    | â­ High       | For fine-tuning      |\n",
    "| GPTQ 4-bit  | 4    | Fastest | â­â­ High+     | For inference        |\n",
    "| AWQ 4-bit   | 4    | Fastest | â­â­â­ Highest  | Best for LLM serving |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### **Which One to Use? (Short Guide)**\n",
    "\n",
    "\n",
    "#### **For inference (running a model)**\n",
    "\n",
    "| Hardware                 | Best choice           |\n",
    "| ------------------------ | --------------------- |\n",
    "| GPU (NVIDIA)             | **GPTQ or AWQ 4-bit** |\n",
    "| CPU                      | **GGUF / llama.cpp**  |\n",
    "| Apple Silicon (M1/M2/M3) | **GGUF (Q4, Q5)**     |\n",
    "| Multi-GPU cluster        | **AWQ with vLLM**     |\n",
    "\n",
    "---\n",
    "\n",
    "#### **For fine-tuning**\n",
    "\n",
    "| Goal                         | Best choice     |\n",
    "| ---------------------------- | --------------- |\n",
    "| Full-scale fine-tuning       | FP16/BF16       |\n",
    "| Cheap supervised fine-tuning | **QLoRA (NF4)** |\n",
    "| Parameter-efficient tuning   | LoRA + 4-bit    |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### **Mini Example: Load a 4-bit LLM**\n",
    "\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"TheBloke/Llama-2-7B-GPTQ\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "```\n",
    "\n",
    "This loads a **GPTQ-quantized 7B LLM** in ~4-bit precision.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### **Mini Example: QLoRA with 4-bit NF4**\n",
    "\n",
    "\n",
    "```python\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\",\n",
    "    load_in_4bit=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config={\"bnb_4bit_compute_dtype\": \"bfloat16\"}\n",
    ")\n",
    "```\n",
    "\n",
    "This loads the LLM in **4-bit NF4** for fine-tuning.\n",
    "\n",
    "---\n",
    "\n",
    "**Final Summary**\n",
    "\n",
    "| Method                | Purpose                    | Best For                        |\n",
    "| --------------------- | -------------------------- | ------------------------------- |\n",
    "| **INT8**              | Smaller model, stable      | General LLM inference           |\n",
    "| **4-bit (NF4 / FP4)** | Very small model           | QLoRA fine-tuning               |\n",
    "| **GPTQ**              | Layer-wise optimized quant | Fast inference                  |\n",
    "| **AWQ**               | Activation-aware quant     | High-accuracy 4-bit LLM serving |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e3af34",
   "metadata": {},
   "source": [
    "### Demonstration\n",
    "\n",
    "* Loads the same causal LLM in different precisions (if supported on your machine),\n",
    "* Runs a small set of prompts,\n",
    "* Measures **inference latency** and **GPU memory usage** (when a CUDA device is present),\n",
    "* Prints generated outputs so you can compare quality.\n",
    "\n",
    "The script is defensive: it attempts each loading method only if the required libraries are installed and the hardware supports it, and it falls back gracefully otherwise.\n",
    "\n",
    "> **Requirements**\n",
    ">\n",
    "> * Python 3.8+\n",
    "> * `transformers`, `torch` (with CUDA if you have GPU), optionally:\n",
    ">\n",
    ">   * `bitsandbytes` (for 8-bit): `pip install bitsandbytes`\n",
    ">   * `auto-gptq` (for GPTQ 4-bit): `pip install auto-gptq`\n",
    ">   * `accelerate` (helpful): `pip install accelerate`\n",
    "> * A GPU is strongly recommended for meaningful speed/memory differences. The script will still run on CPU but INT8/4-bit tests may not be available.\n",
    "\n",
    "Save as `compare_precisions.py` and run: `python compare_precisions.py`.\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "compare_precisions.py\n",
    "\n",
    "Demonstrate FP16 vs INT8 vs GPTQ(4-bit) inference: latency, memory, and outputs.\n",
    "\n",
    "Usage:\n",
    "    python compare_precisions.py\n",
    "\n",
    "Notes:\n",
    " - The script tries three loading methods and skips unsupported cases.\n",
    " - For INT8 it uses bitsandbytes via transformers (load_in_8bit=True).\n",
    " - For GPTQ it uses auto-gptq (AutoGPTQForCausalLM), if available.\n",
    " - Replace MODEL_NAME with a model you have access to (HuggingFace).\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Optional imports (may not be installed)\n",
    "_try_bitsandbytes = True\n",
    "_try_auto_gptq = True\n",
    "try:\n",
    "    import bitsandbytes as bnb  # noqa: F401\n",
    "except Exception:\n",
    "    _try_bitsandbytes = False\n",
    "\n",
    "try:\n",
    "    from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig  # noqa: F401\n",
    "except Exception:\n",
    "    _try_auto_gptq = False\n",
    "\n",
    "# ------ Config ------\n",
    "MODEL_NAME = os.environ.get(\"MODEL_NAME\", \"facebook/opt-1.3b\")  # change as needed\n",
    "PROMPTS = [\n",
    "    \"Explain the greenhouse effect in simple terms.\",\n",
    "    \"Write a 2-line poem about coffee and coding.\",\n",
    "]\n",
    "MAX_NEW_TOKENS = 64\n",
    "TEMPERATURE = 0.2\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "WARMUP = 1  # warmup runs to stabilize timing\n",
    "# --------------------\n",
    "\n",
    "def print_header(title):\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(title)\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "def measure_inference(model, tokenizer, prompts, device, max_new_tokens=64, temperature=0.2):\n",
    "    \"\"\"\n",
    "    Runs generation for each prompt, returns timing (avg per prompt), outputs, and memory stats.\n",
    "    Measures GPU memory if device == 'cuda'.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    latencies = []\n",
    "    # clear cache stats\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.empty_cache()\n",
    "    # warmup\n",
    "    for _ in range(WARMUP):\n",
    "        for p in prompts[:1]:\n",
    "            tok = tokenizer(p, return_tensors=\"pt\").to(device)\n",
    "            with torch.no_grad():\n",
    "                _ = model.generate(**tok, max_new_tokens=8, do_sample=False)\n",
    "\n",
    "    for p in prompts:\n",
    "        tok = tokenizer(p, return_tensors=\"pt\").to(device)\n",
    "        torch.cuda.synchronize() if device == \"cuda\" else None\n",
    "        t0 = time.time()\n",
    "        with torch.no_grad():\n",
    "            out = model.generate(**tok, max_new_tokens=max_new_tokens, temperature=temperature)\n",
    "        torch.cuda.synchronize() if device == \"cuda\" else None\n",
    "        t1 = time.time()\n",
    "        latencies.append(t1 - t0)\n",
    "        decoded = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "        outputs.append(decoded)\n",
    "    avg_latency = sum(latencies) / len(latencies)\n",
    "    mem_stats = {}\n",
    "    if device == \"cuda\":\n",
    "        mem_stats[\"max_alloc_mb\"] = torch.cuda.max_memory_allocated() / 1024**2\n",
    "        mem_stats[\"reserved_mb\"] = torch.cuda.max_memory_reserved() / 1024**2 if hasattr(torch.cuda, \"max_memory_reserved\") else None\n",
    "    else:\n",
    "        # on CPU, report process memory usage if psutil available\n",
    "        try:\n",
    "            import psutil\n",
    "            p = psutil.Process(os.getpid())\n",
    "            mem_stats[\"rss_mb\"] = p.memory_info().rss / 1024**2\n",
    "        except Exception:\n",
    "            mem_stats[\"rss_mb\"] = None\n",
    "    return {\"avg_latency_s\": avg_latency, \"outputs\": outputs, \"mem\": mem_stats}\n",
    "\n",
    "def load_fp16(model_name, device):\n",
    "    \"\"\"Load FP16 (or BF16) model - best accuracy. Requires sufficient GPU memory.\"\"\"\n",
    "    print_header(f\"Loading FP16 model '{model_name}' to {device}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    # Try loading in half precision if device is CUDA\n",
    "    if device == \"cuda\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "    else:\n",
    "        # CPU fallback (will be heavy)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "def load_int8(model_name, device):\n",
    "    \"\"\"\n",
    "    Load an 8-bit model using bitsandbytes (transformers integration).\n",
    "    Requires bitsandbytes installed and a CUDA device.\n",
    "    \"\"\"\n",
    "    if not _try_bitsandbytes:\n",
    "        print(\"bitsandbytes not available; skipping INT8.\")\n",
    "        return None, None\n",
    "    if device != \"cuda\":\n",
    "        print(\"INT8 via bitsandbytes requires CUDA device; skipping INT8.\")\n",
    "        return None, None\n",
    "    print_header(f\"Loading INT8 (bitsandbytes) model '{model_name}' to CUDA (8-bit)\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    # transformers supports load_in_8bit\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, load_in_8bit=True, device_map=\"auto\")\n",
    "    return model, tokenizer\n",
    "\n",
    "def load_gptq(model_name, device):\n",
    "    \"\"\"\n",
    "    Load a GPTQ-quantized model if auto-gptq is available. The path should point to a quantized directory,\n",
    "    or you can quantize an FP16 model first (not done here).\n",
    "    Note: AutoGPTQ.from_quantized expects a directory with quantized files.\n",
    "    \"\"\"\n",
    "    if not _try_auto_gptq:\n",
    "        print(\"auto-gptq not installed; skipping GPTQ (INT4).\")\n",
    "        return None, None\n",
    "    # If the user provided a local GPTQ dir via env, use it; otherwise skip.\n",
    "    gptq_dir = os.environ.get(\"GPTQ_MODEL_DIR\", None)\n",
    "    if not gptq_dir:\n",
    "        print(\"No GPTQ model dir set in GPTQ_MODEL_DIR env var; skipping GPTQ.\")\n",
    "        return None, None\n",
    "    print_header(f\"Loading GPTQ quantized model from '{gptq_dir}'\")\n",
    "    # AutoGPTQ provides from_quantized; use device_map auto\n",
    "    tokenizer = AutoTokenizer.from_pretrained(gptq_dir, use_fast=True)\n",
    "    model = AutoGPTQForCausalLM.from_quantized(gptq_dir, device_map=\"auto\", use_safetensors=True)\n",
    "    return model, tokenizer\n",
    "\n",
    "def main():\n",
    "    print_header(\"Precision comparison demo\")\n",
    "    print(f\"Model: {MODEL_NAME}\")\n",
    "    print(f\"Device: {DEVICE}\")\n",
    "    print(f\"Prompts: {len(PROMPTS)} prompts, max_new_tokens={MAX_NEW_TOKENS}\")\n",
    "    results = {}\n",
    "    # FP16\n",
    "    try:\n",
    "        model_fp16, tok_fp16 = load_fp16(MODEL_NAME, DEVICE)\n",
    "        res = measure_inference(model_fp16, tok_fp16, PROMPTS, DEVICE, max_new_tokens=MAX_NEW_TOKENS, temperature=TEMPERATURE)\n",
    "        results[\"fp16\"] = res\n",
    "        # free memory\n",
    "        del model_fp16\n",
    "        if DEVICE == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "    except Exception as e:\n",
    "        print(\"FP16 load or inference failed:\", e)\n",
    "        results[\"fp16\"] = {\"error\": str(e)}\n",
    "\n",
    "    # INT8\n",
    "    try:\n",
    "        model_int8, tok_int8 = load_int8(MODEL_NAME, DEVICE)\n",
    "        if model_int8 is not None:\n",
    "            res = measure_inference(model_int8, tok_int8, PROMPTS, DEVICE, max_new_tokens=MAX_NEW_TOKENS, temperature=TEMPERATURE)\n",
    "            results[\"int8\"] = res\n",
    "            del model_int8\n",
    "            if DEVICE == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "        else:\n",
    "            results[\"int8\"] = {\"skipped\": True}\n",
    "    except Exception as e:\n",
    "        print(\"INT8 load or inference failed:\", e)\n",
    "        results[\"int8\"] = {\"error\": str(e)}\n",
    "\n",
    "    # GPTQ (4-bit) - requires GPTQ model directory in GPTQ_MODEL_DIR env var\n",
    "    try:\n",
    "        model_gptq, tok_gptq = load_gptq(MODEL_NAME, DEVICE)\n",
    "        if model_gptq is not None:\n",
    "            res = measure_inference(model_gptq, tok_gptq, PROMPTS, DEVICE, max_new_tokens=MAX_NEW_TOKENS, temperature=TEMPERATURE)\n",
    "            results[\"gptq_4bit\"] = res\n",
    "            del model_gptq\n",
    "            if DEVICE == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "        else:\n",
    "            results[\"gptq_4bit\"] = {\"skipped\": True}\n",
    "    except Exception as e:\n",
    "        print(\"GPTQ load or inference failed:\", e)\n",
    "        results[\"gptq_4bit\"] = {\"error\": str(e)}\n",
    "\n",
    "    # Print summary\n",
    "    print_header(\"Summary\")\n",
    "    print(json.dumps(results, indent=2, default=lambda o: str(o)))\n",
    "\n",
    "    # Print outputs side by side for visual comparison (if available)\n",
    "    print_header(\"Outputs comparison\")\n",
    "    for i, p in enumerate(PROMPTS):\n",
    "        print(f\"\\nPrompt {i+1}: {p}\\n\")\n",
    "        for key in [\"fp16\", \"int8\", \"gptq_4bit\"]:\n",
    "            entry = results.get(key, {})\n",
    "            outputs = entry.get(\"outputs\")\n",
    "            if outputs:\n",
    "                print(f\"--- {key}:\")\n",
    "                print(outputs[i][:1000])  # trim\n",
    "            else:\n",
    "                note = entry.get(\"error\") or (\"skipped\" if entry.get(\"skipped\") else \"n/a\")\n",
    "                print(f\"--- {key}: {note}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### How to use / adapt the script\n",
    "\n",
    "* Replace `MODEL_NAME` with a model you can access (HF model id). For a small, fast test use `\"gpt2\"` or `\"facebook/opt-125m\"` â€” but note many small models don't meaningfully show memory differences.\n",
    "* To test **GPTQ (INT4)**, produce a GPTQ quantized model directory and set `GPTQ_MODEL_DIR=/path/to/quantized_model` in your environment before running the script. (AutoGPTQ quantization step is separate.)\n",
    "* For **INT8**, ensure `bitsandbytes` is installed and CUDA is available; transformers `load_in_8bit=True` requires bitsandbytes.\n",
    "* The script measures **average latency** across prompts and **GPU peak allocation** via `torch.cuda.max_memory_allocated()` (MB). On CPU, it attempts to measure RSS via `psutil` if available.\n",
    "* You can extend the script to compute **perplexity**, **BLEU**, or **embedding similarity** between outputs to quantify quality differences.\n",
    "\n",
    "---\n",
    "\n",
    "### Interpretation guidelines\n",
    "\n",
    "* **FP16**: expected highest quality and highest memory use; baseline latency.\n",
    "* **INT8**: typically halves memory with minimal accuracy loss; latency usually improves.\n",
    "* **INT4 / GPTQ**: biggest memory reduction and fastest latency; slight accuracy differences may appear depending on model and prompt."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
