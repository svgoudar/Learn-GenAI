{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5c20d7b",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## Self Attention & Cross Attention\n",
    "\n",
    "### 1. Why Attention Exists (Core Intuition)\n",
    "\n",
    "Before explaining self-attention and cross-attention, understand the **problem** they solve.\n",
    "\n",
    "Neural Machine Translation (NMT) requires:\n",
    "\n",
    "* Reading a source sentence (English)\n",
    "* Understanding it as a whole\n",
    "* Generating a target sentence (French)\n",
    "\n",
    "RNNs and LSTMs struggled because:\n",
    "\n",
    "* They compress the entire meaning of a sentence into a *single* hidden vector.\n",
    "* Long sentences are hard to encode correctly.\n",
    "* They process words sequentially, slowing down training.\n",
    "\n",
    "**Attention** solved this by allowing the model to:\n",
    "\n",
    "* Look back at specific words it needs,\n",
    "* Weigh them differently depending on context,\n",
    "* And process many words in parallel.\n",
    "\n",
    "This idea became the foundation of Transformers.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. What Q, K, V Represent (Intuitive View)\n",
    "\n",
    "In all attention mechanisms, we project token embeddings into:\n",
    "\n",
    "* **Query (Q)** → What I am looking for\n",
    "* **Key (K)** → What information I offer\n",
    "* **Value (V)** → The actual information content\n",
    "\n",
    "Analogy:\n",
    "Imagine researching in a library.\n",
    "\n",
    "* Query = the question you're trying to answer\n",
    "* Key = the index of each book\n",
    "* Value = the content inside the book\n",
    "\n",
    "Attention computes similarity between Query and Key, and uses that to decide how much of Value to read.\n",
    "\n",
    "The formula:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "This is:\n",
    "\n",
    "1. Compare Q with every K\n",
    "2. Convert similarities into probabilities (softmax)\n",
    "3. Blend the Value vectors using these probabilities\n",
    "\n",
    "This blending produces **contextual embeddings**.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Self-Attention (Detailed, Practical Intuition)\n",
    "\n",
    "Self-attention means:\n",
    "**A token looks at every other token in the same sentence to understand its contextual meaning.**\n",
    "\n",
    "All Q, K, V come from the *same* sentence.\n",
    "\n",
    "#### Why do we need this?\n",
    "\n",
    "Words change meaning based on context:\n",
    "\n",
    "* *“bank”* (river bank vs monetary bank)\n",
    "* *“trains”* (verb or noun)\n",
    "\n",
    "Self-attention adjusts the embedding of each word based on the other words around it.\n",
    "\n",
    "#### How it works in translation\n",
    "\n",
    "Example:\n",
    "“The boy **trains** the puppy.”\n",
    "\n",
    "The raw word embedding of “trains” is ambiguous.\n",
    "Self-attention allows “trains” to check:\n",
    "\n",
    "* \"boy\" → subject\n",
    "* \"puppy\" → object\n",
    "* \"the\" → determiner\n",
    "\n",
    "Because it sees these words, the layer learns that “trains” is a *verb*, not a noun.\n",
    "\n",
    "**Effect:**\n",
    "A new enriched, context-aware embedding of “trains” is produced.\n",
    "This enriched embedding is what the encoder passes to the decoder.\n",
    "\n",
    "#### Bidirectional vs Masked\n",
    "\n",
    "* **Encoder self-attention**: Can look left and right (bidirectional).\n",
    "* **Decoder self-attention**: Only looks left (causal mask), to prevent cheating by seeing future words when predicting.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Cross-Attention (Detailed, Practical Intuition)\n",
    "\n",
    "Cross-attention connects the decoder with the encoder.\n",
    "\n",
    "**The decoder uses its own Query, and attends to the encoder's Key and Value.**\n",
    "\n",
    "#### Why?\n",
    "\n",
    "When generating a translation, the decoder needs to look back at the source sentence.\n",
    "\n",
    "Example:\n",
    "Translating to French:\n",
    "\n",
    "“The boy trains the puppy.” → “Le garçon entraîne le chiot.”\n",
    "\n",
    "When the decoder is about to output the French equivalent of “trains”:\n",
    "\n",
    "* Query = the decoder’s current hidden state\n",
    "* Keys = encoder’s representation of each English word\n",
    "* Values = same encoder representations\n",
    "\n",
    "Cross-attention determines which source word is most relevant.\n",
    "\n",
    "#### What happens internally?\n",
    "\n",
    "Decoder asks:\n",
    "\n",
    "> “Which English word should I focus on now?”\n",
    "\n",
    "The attention score becomes highest for the source word “trains”.\n",
    "\n",
    "So the decoder retrieves that part of the encoder's output and uses it to output the correct French verb form “entraîne”.\n",
    "\n",
    "#### Why cross-attention is critical\n",
    "\n",
    "Without cross-attention:\n",
    "\n",
    "* Decoder would generate output blindly\n",
    "* Translation quality would collapse\n",
    "* Long-range dependencies would be lost\n",
    "\n",
    "Cross-attention is a learnable lookup into encoder memory.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Putting Both Together (Full Translation Process)\n",
    "\n",
    "#### Step 1: Encoder (Self-Attention)\n",
    "\n",
    "The encoder reads the English sentence.\n",
    "\n",
    "Self-attention refines each word:\n",
    "\n",
    "* “trains” becomes a verb representation\n",
    "* “boy” becomes a subject representation\n",
    "* “puppy” becomes an object representation\n",
    "\n",
    "It outputs a sequence of embeddings that represent the whole sentence meaningfully.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 2: Decoder (Masked Self-Attention)\n",
    "\n",
    "When generating output token by token:\n",
    "\n",
    "* The decoder uses masked self-attention to understand what it has generated so far.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 3: Cross-Attention (Connecting encoder and decoder)\n",
    "\n",
    "At each decoding step:\n",
    "\n",
    "* Decoder Q looks at encoder K, V\n",
    "* Retrieves most relevant part of the source sentence\n",
    "* Uses that to produce the next word\n",
    "\n",
    "This is how alignment between languages emerges.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary Table**\n",
    "\n",
    "| Concept                      | Source of Q    | Source of K & V | Purpose                             |\n",
    "| ---------------------------- | -------------- | --------------- | ----------------------------------- |\n",
    "| **Self-attention (encoder)** | Encoder tokens | Encoder tokens  | Understand source sentence context  |\n",
    "| **Self-attention (decoder)** | Decoder tokens | Decoder tokens  | Understand partial output so far    |\n",
    "| **Cross-attention**          | Decoder tokens | Encoder tokens  | Link source meaning to output words |\n",
    "\n",
    "---\n",
    "\n",
    "**Most Important Intuition**\n",
    "\n",
    "* **Self-attention helps each word understand its meaning by looking at surrounding words.**\n",
    "* **Cross-attention helps the decoder retrieve the right source information at the right time.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88595a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input embeddings:\n",
      " tensor([[ 0.4867, -0.7886, -0.3865, -0.2724],\n",
      "        [-0.5073, -1.5010,  0.9716,  0.0105],\n",
      "        [-1.3607, -0.7603,  0.0541,  0.3580],\n",
      "        [ 0.1713, -0.1785,  0.4401,  1.4594],\n",
      "        [ 1.6816, -0.4191,  0.7418,  0.0577]])\n",
      "\n",
      "Self-Attention weights:\n",
      " tensor([[0.2117, 0.2329, 0.2196, 0.1565, 0.1792],\n",
      "        [0.2151, 0.2241, 0.2617, 0.1501, 0.1490],\n",
      "        [0.1838, 0.2075, 0.2144, 0.1938, 0.2005],\n",
      "        [0.2282, 0.1933, 0.2238, 0.1869, 0.1679],\n",
      "        [0.2539, 0.2129, 0.2398, 0.1509, 0.1424]], grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "Contextualized representations:\n",
      " tensor([[ 0.1459, -0.2501,  0.0917, -0.2328],\n",
      "        [ 0.1493, -0.3187,  0.1532, -0.2441],\n",
      "        [ 0.1338, -0.2090,  0.0313, -0.2046],\n",
      "        [ 0.1243, -0.2514,  0.0796, -0.1968],\n",
      "        [ 0.1358, -0.3046,  0.1489, -0.2246]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# ----- Toy input: 5 words, embedding dim = 4 -----\n",
    "x = torch.randn(5, 4)          # shape: (seq_len, embed_dim)\n",
    "\n",
    "embed_dim = x.size(1)\n",
    "d_k = embed_dim\n",
    "\n",
    "# ----- Learnable projection matrices -----\n",
    "W_q = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "W_k = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "W_v = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "\n",
    "# ----- Compute Q, K, V -----\n",
    "Q = W_q(x)   # (5,4)\n",
    "K = W_k(x)   # (5,4)\n",
    "V = W_v(x)   # (5,4)\n",
    "\n",
    "# ----- Attention scores: QK^T -----\n",
    "scores = Q @ K.transpose(0, 1) / (d_k ** 0.5)   # (5,5)\n",
    "\n",
    "# ----- Softmax along keys dimension -----\n",
    "attn_weights = F.softmax(scores, dim=-1)        # (5,5)\n",
    "\n",
    "# ----- Weighted sum of Values -----\n",
    "output = attn_weights @ V                      # (5,4)\n",
    "\n",
    "print(\"Input embeddings:\\n\", x)\n",
    "print(\"\\nSelf-Attention weights:\\n\", attn_weights)\n",
    "print(\"\\nContextualized representations:\\n\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b1e47c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder Q:\n",
      " tensor([[ 0.0743, -0.0228, -0.1231, -0.1664],\n",
      "        [-0.4445,  0.1650,  0.5854,  0.3921],\n",
      "        [-0.4303,  0.2577,  1.3873,  0.8675]], grad_fn=<MmBackward0>)\n",
      "\n",
      "Encoder K:\n",
      " tensor([[ 0.1421, -0.3960,  0.2355, -0.1776],\n",
      "        [ 0.2310,  0.1648,  0.5230,  0.1143],\n",
      "        [-1.0206,  0.4274, -0.3964, -0.1257],\n",
      "        [ 0.6489,  0.1754,  0.3166,  0.2640],\n",
      "        [-0.1461, -0.2791, -0.0909, -0.1741]], grad_fn=<MmBackward0>)\n",
      "\n",
      "Cross-attention weights:\n",
      " tensor([[0.2034, 0.1944, 0.1998, 0.1975, 0.2050],\n",
      "        [0.1862, 0.2202, 0.2166, 0.1947, 0.1823],\n",
      "        [0.1817, 0.2655, 0.1712, 0.2247, 0.1568]], grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "Cross-attention context vectors:\n",
      " tensor([[ 0.0272,  0.0640, -0.1257,  0.0343],\n",
      "        [ 0.0811,  0.0557, -0.1466,  0.0161],\n",
      "        [ 0.1628,  0.0298, -0.2224,  0.0031]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ----- Encoder output: 5 tokens, emb_dim = 4 -----\n",
    "encoder_out = torch.randn(5, 4)   # (src_seq_len, embed_dim)\n",
    "\n",
    "# ----- Decoder input: 3 tokens -----\n",
    "decoder_in = torch.randn(3, 4)    # (tgt_seq_len, embed_dim)\n",
    "\n",
    "embed_dim = 4\n",
    "d_k = embed_dim\n",
    "\n",
    "# ----- Projection matrices -----\n",
    "W_q = nn.Linear(embed_dim, embed_dim, bias=False)   # from decoder\n",
    "W_k = nn.Linear(embed_dim, embed_dim, bias=False)   # from encoder\n",
    "W_v = nn.Linear(embed_dim, embed_dim, bias=False)   # from encoder\n",
    "\n",
    "# ----- Create Q (decoder) and K,V (encoder) -----\n",
    "Q = W_q(decoder_in)           # (tgt_seq_len, embed_dim)\n",
    "K = W_k(encoder_out)          # (src_seq_len, embed_dim)\n",
    "V = W_v(encoder_out)          # (src_seq_len, embed_dim)\n",
    "\n",
    "# ----- Compute attention scores -----\n",
    "scores = Q @ K.transpose(0, 1) / (d_k ** 0.5)   # (tgt_seq_len, src_seq_len)\n",
    "\n",
    "# ----- Convert to attention weights -----\n",
    "attn_weights = F.softmax(scores, dim=-1)        # (tgt_seq_len, src_seq_len)\n",
    "\n",
    "# ----- Compute contextualized decoder state -----\n",
    "context = attn_weights @ V                      # (tgt_seq_len, embed_dim)\n",
    "\n",
    "print(\"Decoder Q:\\n\", Q)\n",
    "print(\"\\nEncoder K:\\n\", K)\n",
    "print(\"\\nCross-attention weights:\\n\", attn_weights)\n",
    "print(\"\\nCross-attention context vectors:\\n\", context)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
