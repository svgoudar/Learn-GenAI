{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cf4a354",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## Direct Preference Optimization (DPO)\n",
    "\n",
    "DPO is a **post-training alignment method** that teaches an LLM to prefer *better* responses over *worse* ones **without training a reward model and without using RL**.\n",
    "\n",
    "It directly learns from **preference pairs**:\n",
    "\n",
    "```\n",
    "Given two answers:\n",
    "A = human-preferred (\"chosen\")\n",
    "B = human-rejected (\"rejected\")\n",
    "\n",
    "→ Make the model more likely to generate A than B\n",
    "```\n",
    "\n",
    "DPO is simpler, cheaper, and more stable than RLHF, yet achieves comparable or superior results.\n",
    "\n",
    "---\n",
    "\n",
    "### Why DPO Was Invented\n",
    "\n",
    "RLHF (Reinforcement Learning from Human Feedback) has **three costly steps**:\n",
    "\n",
    "1. **Train a Reward Model (RM)** from preference data\n",
    "2. **Run PPO reinforcement learning** using the RM\n",
    "3. **Stabilize** training with KL penalties\n",
    "\n",
    "Problems:\n",
    "\n",
    "* RM training requires a separate neural network\n",
    "* PPO (policy gradient RL) is unstable and expensive\n",
    "* Hard to tune\n",
    "* More compute, more code, more engineering\n",
    "\n",
    "### DPO solves ALL of this:\n",
    "\n",
    "✔ No reward model\n",
    "✔ No reinforcement learning\n",
    "✔ No PPO\n",
    "✔ No policy gradient\n",
    "✔ No KL regularization loops\n",
    "\n",
    "It is **pure supervised learning** with a clever loss function.\n",
    "\n",
    "---\n",
    "\n",
    "### What DPO Does in Simple Words\n",
    "\n",
    "DPO teaches the model:\n",
    "\n",
    "> “For this prompt, make the chosen answer more likely than the rejected answer.”\n",
    "\n",
    "Meaning:\n",
    "\n",
    "* Increase log-probability of the chosen output\n",
    "* Decrease log-probability of the rejected one\n",
    "* Keep the model close to the original model (to avoid drift)\n",
    "\n",
    "---\n",
    "\n",
    "### The Core DPO Loss (Intuition, Not Math)\n",
    "\n",
    "For each sample:\n",
    "\n",
    "```\n",
    "User Prompt: \"Explain gravity.\"\n",
    "\n",
    "Chosen Answer:   accurate answer\n",
    "Rejected Answer: poor, harmful, wrong answer\n",
    "```\n",
    "\n",
    "The DPO objective pushes the LLM to:\n",
    "\n",
    "* **Increase** likelihood(chosen | prompt)\n",
    "* **Decrease** likelihood(rejected | prompt)\n",
    "* Stay close to the base model\n",
    "\n",
    "The key intuition:\n",
    "\n",
    "> The model learns human preferences directly from the difference between chosen and rejected responses.\n",
    "\n",
    "No reward model needed.\n",
    "\n",
    "---\n",
    "\n",
    "### The Actual (Simple) DPO Formula\n",
    "\n",
    "The core term:\n",
    "\n",
    "$$\n",
    "\\log \\pi(chosen) - \\log \\pi(rejected)\n",
    "$$\n",
    "\n",
    "DPO maximizes this difference.\n",
    "\n",
    "To prevent drifting too far from the base model π₀, DPO includes a KL preference:\n",
    "\n",
    "$$\n",
    "\\text{Encourages π to stay close to π₀}\n",
    "$$\n",
    "\n",
    "This ensures stability.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Why DPO Works Better Than RLHF\n",
    "\n",
    "#### ✔ **1. No Reward Model Needed**\n",
    "\n",
    "The reward model often over-fits or misaligns behavior.\n",
    "\n",
    "#### ✔ **2. No PPO (No RL Loop)**\n",
    "\n",
    "Training becomes:\n",
    "\n",
    "* Simple\n",
    "* Stable\n",
    "* Fast\n",
    "\n",
    "#### ✔ **3. Direct Optimization**\n",
    "\n",
    "You directly optimize preference data instead of relying on an auxiliary reward signal.\n",
    "\n",
    "#### ✔ **4. Better at Alignment**\n",
    "\n",
    "Experiments show DPO produces:\n",
    "\n",
    "* safer responses\n",
    "* higher quality answers\n",
    "* more stable training\n",
    "* better generalization\n",
    "\n",
    "#### ✔ **5. Lower Cost**\n",
    "\n",
    "You skip:\n",
    "\n",
    "* RM training\n",
    "* RL optimization\n",
    "* long PPO rollout steps\n",
    "\n",
    "DPO = **SFT-style training on preference pairs**.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. What Data DPO Uses\n",
    "\n",
    "DPO uses *preference datasets*:\n",
    "\n",
    "```\n",
    "prompt\n",
    "chosen_answer\n",
    "rejected_answer\n",
    "```\n",
    "\n",
    "Examples from Anthropic HH-RLHF, OpenAI InstructGPT, StackLLM, etc.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Simple Analogy\n",
    "\n",
    "**SFT**: “Here is a correct answer. Learn to copy it.”\n",
    "**DPO**: “Here are two answers. Learn why A is better than B.”\n",
    "\n",
    "This makes DPO a **behavior optimizer**, not a task teacher.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Where DPO Fits in the LLM Pipeline\n",
    "\n",
    "```\n",
    "1. Pretraining  \n",
    "   (next-token prediction)\n",
    "\n",
    "2. SFT  \n",
    "   (instruction-following)\n",
    "\n",
    "3. Preference Data  \n",
    "   (chosen vs rejected answers)\n",
    "\n",
    "4. DPO  \n",
    "   (optimize model to prefer chosen answers)\n",
    "```\n",
    "\n",
    "DPO replaces the RLHF step.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. When To Use DPO\n",
    "\n",
    "Use DPO when:\n",
    "\n",
    "* You have human preference data\n",
    "* You want ChatGPT-like alignment\n",
    "* You want to avoid reward models\n",
    "* You want stability and simplicity\n",
    "* You want lower cost alignment\n",
    "\n",
    "Most modern open LLMs (LLaMA-Instruct, Mistral-Instruct, Zephyr, Falcon-Instruct) now use **DPO instead of RLHF**.\n",
    "\n",
    "---\n",
    "\n",
    "### 10. One Example (VERY intuitive)\n",
    "\n",
    "Prompt:\n",
    "\n",
    "```\n",
    "What is 2 + 2?\n",
    "```\n",
    "\n",
    "Chosen:\n",
    "\n",
    "```\n",
    "The answer is 4.\n",
    "```\n",
    "\n",
    "Rejected:\n",
    "\n",
    "```\n",
    "It depends on your perspective.\n",
    "```\n",
    "\n",
    "DPO tells the model:\n",
    "\n",
    "* Increase probability of “The answer is 4.”\n",
    "* Decrease probability of “It depends…”\n",
    "* Don’t change the rest of the model too much\n",
    "\n",
    "Over many examples, the model becomes:\n",
    "\n",
    "* helpful\n",
    "* accurate\n",
    "* safe\n",
    "* aligned with human preference\n",
    "\n",
    "---\n",
    "\n",
    "**One-Sentence Summary**\n",
    "\n",
    "**Direct Preference Optimization (DPO) trains an LLM to prefer human-preferred responses over rejected ones using a direct supervised objective, eliminating the need for reward models and reinforcement learning while producing high-quality aligned models.**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. Install Dependencies\n",
    "\n",
    "```bash\n",
    "pip install transformers datasets trl peft accelerate bitsandbytes\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Create a Toy Preference Dataset\n",
    "\n",
    "```python\n",
    "from datasets import Dataset\n",
    "\n",
    "data = [\n",
    "    {\n",
    "        \"prompt\": \"Explain gravity.\",\n",
    "        \"chosen\": \"Gravity is a force that pulls objects together.\",\n",
    "        \"rejected\": \"Gravity is magic glue that makes things stick.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"What is photosynthesis?\",\n",
    "        \"chosen\": \"Photosynthesis is how plants convert sunlight into energy.\",\n",
    "        \"rejected\": \"Photosynthesis means plants like the sun.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "dataset = Dataset.from_list(data)\n",
    "dataset\n",
    "```\n",
    "\n",
    "This dataset has:\n",
    "\n",
    "* **prompt**\n",
    "* **chosen response**\n",
    "* **rejected response**\n",
    "\n",
    "Exactly what DPO needs.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Load a Small Base Model in 4-bit (QLoRA friendly)\n",
    "\n",
    "We use a tiny model to run anywhere.\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL = \"facebook/opt-350m\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL,\n",
    "    load_in_4bit=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Apply LoRA Adapters (Efficient Training)\n",
    "\n",
    "```python\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Setup DPOTrainer\n",
    "\n",
    "```python\n",
    "from trl import DPOTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    output_dir=\"./dpo_out\",\n",
    "    logging_steps=1,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    beta=0.1,                # KL strength (DPO hyperparameter)\n",
    "    max_prompt_length=128,\n",
    "    max_length=256,\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. Train the Model Using DPO\n",
    "\n",
    "```python\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "DPO does the following automatically:\n",
    "\n",
    "* Compute log-probability of chosen response\n",
    "* Compute log-probability of rejected response\n",
    "* Compute KL term w.r.t base model\n",
    "* Optimize the objective:\n",
    "  $$\n",
    "  \\log π(chosen) - \\log π(rejected)\n",
    "  $$\n",
    "\n",
    "This directly improves alignment.\n",
    "\n",
    "---\n",
    "\n",
    "#### 7. Save the Tuned Model\n",
    "\n",
    "```python\n",
    "trainer.model.save_pretrained(\"dpo_lora_model\")\n",
    "tokenizer.save_pretrained(\"dpo_lora_model\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 8. Test the DPO-Tuned Model\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"dpo_lora_model\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=80\n",
    ")\n",
    "\n",
    "prompt = \"Explain gravity.\"\n",
    "print(pipe(prompt)[0][\"generated_text\"])\n",
    "```\n",
    "\n",
    "You should see the model:\n",
    "\n",
    "* Prefer accurate, safe, helpful responses\n",
    "* Avoid the \"rejected\" style responses\n",
    "* Produce more aligned behavior\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
