{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fba0d68a",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## Low-Rank Adaptation (LoRA)\n",
    "\n",
    "LoRA is a **parameter-efficient fine-tuning technique** that lets you adapt a large pretrained model **without updating its original weights**.\n",
    "\n",
    "Instead of modifying a big weight matrix (W), LoRA learns a **small low-rank update** (\\Delta W):\n",
    "\n",
    "$$\n",
    "W_{\\text{new}} = W + \\Delta W\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "\\Delta W = B A\n",
    "$$\n",
    "\n",
    "* (A) and (B) are small trainable matrices\n",
    "* Rank (r) is tiny (like 4, 8, 16)\n",
    "* (W) stays **frozen**\n",
    "\n",
    "This reduces training cost, memory, and overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "### Why LoRA Exists (Problem Statement)\n",
    "\n",
    "Full fine-tuning is expensive because:\n",
    "\n",
    "* LLMs have **billions of parameters**\n",
    "* Updating all of them requires **huge GPU memory**\n",
    "* Risk of **catastrophic forgetting**\n",
    "* Each task requires training/storing another full model\n",
    "\n",
    "We want to fine-tune for new tasks (medical, legal, customer support) **without retraining the whole model**.\n",
    "\n",
    "LoRA solves this.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Insight Behind LoRA (Intuition)\n",
    "\n",
    "#### Most of the knowledge in a large model is already correct.\n",
    "\n",
    "To adapt it to a new task, you *only need a small adjustment*.\n",
    "\n",
    "This adjustment usually lies in a **low-dimensional subspace**.\n",
    "\n",
    "So instead of updating a big matrix:\n",
    "\n",
    "```\n",
    "4096 × 4096 = 16,777,216 parameters\n",
    "```\n",
    "\n",
    "LoRA updates two small matrices:\n",
    "\n",
    "```\n",
    "4096 × r and r × 4096\n",
    "```\n",
    "\n",
    "If r = 8:\n",
    "\n",
    "```\n",
    "4096×8 + 8×4096 = 65,536 parameters\n",
    "```\n",
    "\n",
    "→ **250× fewer trainable parameters**\n",
    "\n",
    "This small update is the “nudge” the model needs.\n",
    "\n",
    "---\n",
    "\n",
    "### How LoRA Works (Mechanism)\n",
    "\n",
    "For a weight matrix (W):\n",
    "\n",
    "1. **Freeze** the original weight (no gradients)\n",
    "2. Add a low-rank decomposition:\n",
    "   $$\n",
    "   \\Delta W = B A\n",
    "   $$\n",
    "3. Train **only A and B**\n",
    "4. During inference:\n",
    "   $$\n",
    "   y = Wx + B(Ax)\n",
    "   $$\n",
    "\n",
    "You don’t modify (W); you add a learned correction on top.\n",
    "\n",
    "---\n",
    "\n",
    "### Where LoRA Applies in Transformers\n",
    "\n",
    "LoRA is usually applied to the **attention projection matrices**:\n",
    "\n",
    "* Query (Q)\n",
    "* Key (K)\n",
    "* Value (V)\n",
    "* Output projection (O)\n",
    "\n",
    "These matrices are the most influential and high-dimensional in LLMs.\n",
    "\n",
    "Applying LoRA here gives maximum effect with minimal parameters.\n",
    "\n",
    "---\n",
    "\n",
    "### Benefits of LoRA\n",
    "\n",
    "#### **✔ Train fewer parameters**\n",
    "\n",
    "Often **<1%** of the model is trainable.\n",
    "\n",
    "#### **✔ Preserve original model**\n",
    "\n",
    "No risk of damaging pretrained knowledge.\n",
    "\n",
    "#### **✔ Efficient storage**\n",
    "\n",
    "Adapters are **tiny** (few MB).\n",
    "\n",
    "#### **✔ Fast training**\n",
    "\n",
    "Fits on a single GPU.\n",
    "\n",
    "#### **✔ Modular**\n",
    "\n",
    "Load different LoRA adapters for:\n",
    "\n",
    "* coding\n",
    "* math\n",
    "* medical\n",
    "* translation\n",
    "\n",
    "One base model → many skills.\n",
    "\n",
    "---\n",
    "\n",
    "### Mini Demonstration (PyTorch Style)\n",
    "\n",
    "#### Original weight:\n",
    "\n",
    "```python\n",
    "W: (4096 × 4096)\n",
    "```\n",
    "\n",
    "#### LoRA learns:\n",
    "\n",
    "```python\n",
    "A: (r × 4096)\n",
    "B: (4096 × r)\n",
    "```\n",
    "\n",
    "Forward pass:\n",
    "\n",
    "```python\n",
    "y = W @ x + B @ (A @ x)\n",
    "```\n",
    "\n",
    "W is frozen; only A and B learn.\n",
    "\n",
    "---\n",
    "\n",
    "**One-Sentence Summary**\n",
    "\n",
    "**LoRA fine-tunes large models by learning a small low-rank matrix update on top of frozen pretrained weights, making adaptation fast, cheap, and parameter-efficient.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7e17c3",
   "metadata": {},
   "source": [
    "### Demonstration\n",
    "\n",
    "#### 1) Intuition (one short paragraph)\n",
    "\n",
    "LoRA keeps the pretrained weight `W` frozen and learns a **low-rank update** (\\Delta W = B A) with small rank `r`. Forward becomes:\n",
    "[\n",
    "y = W x + \\frac{\\alpha}{r} B (A x)\n",
    "]\n",
    "Only `A` and `B` are trained — drastically fewer params and fast fine-tuning.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2) Minimal LoRA module (linear layer adapter)\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    \"\"\"\n",
    "    LoRA adapter for a single nn.Linear module.\n",
    "    It wraps a frozen base linear layer and adds a low-rank trainable update.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_linear: nn.Linear, r: int = 4, alpha: float = 1.0):\n",
    "        super().__init__()\n",
    "        # store base (frozen) linear\n",
    "        self.base = base_linear\n",
    "        self.base.weight.requires_grad = False\n",
    "        if self.base.bias is not None:\n",
    "            self.base.bias.requires_grad = False\n",
    "\n",
    "        in_dim = self.base.in_features\n",
    "        out_dim = self.base.out_features\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / max(1, r)\n",
    "\n",
    "        # A: projects input -> r (down)\n",
    "        # B: projects r -> output (up)\n",
    "        # initialize A so outputs are small; initialize B to zero to start with base behaviour\n",
    "        self.A = nn.Parameter(torch.randn(r, in_dim) * 0.01)   # shape (r, in)\n",
    "        self.B = nn.Parameter(torch.zeros(out_dim, r))         # shape (out, r)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, ..., in_dim) -> we treat last dim as feature dim\n",
    "        # compute base output\n",
    "        base_out = self.base(x)  # uses frozen weights\n",
    "\n",
    "        # compute low-rank update: B @ (A @ x^T)  -> but do via (x @ A^T) then @ B^T\n",
    "        # x shape (BATCH, in_dim); xA_T shape (BATCH, r)\n",
    "        xA_T = torch.matmul(x, self.A.t())             # (BATCH, r)\n",
    "        lora_out = torch.matmul(xA_T, self.B.t())      # (BATCH, out_dim)\n",
    "        return base_out + self.scaling * lora_out\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 3) Helper: attach LoRA to all `nn.Linear` layers (or targeted layers)\n",
    "\n",
    "```python\n",
    "def inject_lora(model: nn.Module, r: int = 4, alpha: float = 1.0,\n",
    "                target_type=nn.Linear, module_name_filter=None):\n",
    "    \"\"\"\n",
    "    Replace target_type modules with LoRA-wrapped modules.\n",
    "    module_name_filter: optional function(name)->bool to pick which layers to wrap.\n",
    "    Returns a list of (orig_name, lora_module).\n",
    "    \"\"\"\n",
    "    replaced = []\n",
    "    for name, mod in list(model.named_modules()):\n",
    "        # only consider direct children to allow safe replacement\n",
    "        parent = None\n",
    "        name_in_parent = None\n",
    "        parts = name.split('.')\n",
    "        if len(parts) > 0 and parts[0] == '':\n",
    "            continue\n",
    "    # simpler: iterate over immediate children of top-level module only\n",
    "    for child_name, child in list(model._modules.items()):\n",
    "        if isinstance(child, target_type) and (module_name_filter is None or module_name_filter(child_name)):\n",
    "            model._modules[child_name] = LoRALayer(child, r=r, alpha=alpha)\n",
    "            replaced.append(child_name)\n",
    "        else:\n",
    "            # recurse into child\n",
    "            if len(list(child.children())) > 0:\n",
    "                inject_lora(child, r=r, alpha=alpha, target_type=target_type, module_name_filter=module_name_filter)\n",
    "    return replaced\n",
    "```\n",
    "\n",
    "> Use `module_name_filter` to target Q/K/V/O proj layers by name in transformers (e.g. names containing `'q_proj'`, `'k_proj'`, `'v_proj'`, `'o_proj'`).\n",
    "\n",
    "---\n",
    "\n",
    "#### 4) Tiny end-to-end example: an MLP, inject LoRA, check params, train only LoRA\n",
    "\n",
    "```python\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# simple base model\n",
    "class SmallMLP(nn.Module):\n",
    "    def __init__(self, input_dim=8, hidden=64, out=2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, hidden)\n",
    "        self.out = nn.Linear(hidden, out)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.out(x)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SmallMLP().to(device)\n",
    "\n",
    "# Count original params\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total params (orig):\", total_params)\n",
    "\n",
    "# Inject LoRA into fc1 and fc2 only (example)\n",
    "inject_lora(model, r=4, alpha=8.0, target_type=nn.Linear,\n",
    "            module_name_filter=lambda n: n in (\"fc1\",\"fc2\"))\n",
    "\n",
    "# Count params after injection and trainable params\n",
    "total_params_after = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total params (after):\", total_params_after)\n",
    "print(\"Trainable params (should be small):\", trainable_params)\n",
    "\n",
    "# Verify only LoRA params are trainable\n",
    "for name, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        print(\"Trainable:\", name, p.shape)\n",
    "\n",
    "# Dummy dataset\n",
    "X = torch.randn(128, 8).to(device)\n",
    "y = torch.randint(0,2,(128,)).to(device)\n",
    "\n",
    "# Optimizer must include only trainable params (LoRA A,B)\n",
    "opt = optim.Adam([p for p in model.parameters() if p.requires_grad], lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(5):\n",
    "    logits = model(X)\n",
    "    loss = loss_fn(logits, y)\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    print(f\"Epoch {epoch} loss {loss.item():.4f}\")\n",
    "```\n",
    "\n",
    "**What this does:**\n",
    "\n",
    "* Replaces the `fc1` and `fc2` `nn.Linear` with `LoRALayer` wrappers.\n",
    "* Base `Linear` weights are frozen.\n",
    "* Only LoRA `A` and `B` parameters have `requires_grad=True`.\n",
    "* Optimizer updates only LoRA params.\n",
    "* You can observe training loss decreasing while only small adapter weights are learned.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5) Parameter-count demonstration (why LoRA is small)\n",
    "\n",
    "Example numbers (conceptual):\n",
    "\n",
    "* `fc1` weight: 64 × 8 = 512 params\n",
    "* `fc2` weight: 64 × 64 = 4,096 params\n",
    "\n",
    "LoRA with `r=4`:\n",
    "\n",
    "* For `fc1`: A: 4×8 = 32; B: 64×4 = 256 → total 288 params\n",
    "* For `fc2`: A: 4×64 = 256; B: 64×4 = 256 → total 512 params\n",
    "\n",
    "Total trainable ≈ 800 params vs full fine-tune thousands — big savings.\n",
    "\n",
    "---\n",
    "\n",
    "#### 6) Save / load LoRA adapter only (recommended)\n",
    "\n",
    "```python\n",
    "# save only LoRA parameters\n",
    "lora_state = {k:v.cpu() for k,v in model.state_dict().items() if 'A' in k or 'B' in k}\n",
    "torch.save(lora_state, \"lora_adapter.pth\")\n",
    "\n",
    "# load back (on same architecture)\n",
    "adapter = torch.load(\"lora_adapter.pth\", map_location='cpu')\n",
    "model.load_state_dict(adapter, strict=False)  # strict=False because only subset loaded\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 7) Notes & best practices\n",
    "\n",
    "* **Where to apply LoRA:** Q/K/V/O projection matrices in attention yield strong effect for transformers. Use `module_name_filter` to target these layers by name.\n",
    "* **Rank `r`:** small (4–16) usually works. Higher `r` = more capacity but more params.\n",
    "* **Alpha scaling:** common to use `alpha = r` or some value to scale update magnitude.\n",
    "* **Initialization:** A small random init and B zero is common so model behavior starts unchanged.\n",
    "* **Saving:** keep adapter only — multiple small adapters can be stored for different tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7b2a27",
   "metadata": {},
   "source": [
    "### LLM\n",
    "Below is a **clean, practical demonstration of LoRA applied to a real LLM** using **HuggingFace + PEFT** — the *correct modern method* for LoRA fine-tuning large language models like LLaMA, Mistral, GPT-J, Falcon, etc.\n",
    "\n",
    "This example is:\n",
    "\n",
    "* **Minimal** (copy–paste runnable)\n",
    "* **Uses PEFT (official library for LoRA)**\n",
    "* **Shows where LoRA attaches inside an LLM**\n",
    "* **Shows training only LoRA params**\n",
    "* **Works on CPU or GPU**\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Install Required Packages**\n",
    "\n",
    "```bash\n",
    "pip install transformers peft accelerate datasets sentencepiece\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Load an LLM + Apply LoRA (Using PEFT)**\n",
    "\n",
    "Example: LLaMA-like or Mistral model (small).\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "model_name = \"tiiuae/falcon-7b-instruct\"  # choose any HF causal LM\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_8bit=True,            # optional: use 8-bit to reduce VRAM\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Define LoRA Configuration**\n",
    "\n",
    "LoRA usually targets **Q, K, V, O** attention projection layers of the LLM.\n",
    "\n",
    "```python\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                          # rank of LoRA matrices\n",
    "    lora_alpha=16,                # scaling factor\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], \n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "```\n",
    "\n",
    "### Why target these layers?\n",
    "\n",
    "Because Q/K/V/O projection matrices are where most transformer adaptation happens.\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Inject LoRA Into the Model**\n",
    "\n",
    "```python\n",
    "model = get_peft_model(model, lora_config)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **5. Show Trainable vs Frozen Parameters**\n",
    "\n",
    "LoRA trains **<1%** of the parameters.\n",
    "\n",
    "```python\n",
    "model.print_trainable_parameters()\n",
    "```\n",
    "\n",
    "Typical output:\n",
    "\n",
    "```\n",
    "trainable params: 3,276,800 || all params: 7,000,000,000 \n",
    "trainable = 0.046%\n",
    "```\n",
    "\n",
    "This is the *true power* of LoRA.\n",
    "\n",
    "---\n",
    "\n",
    "#### **6. Prepare Dataset (Tiny Example)**\n",
    "\n",
    "Use any text dataset. Example using `datasets` library:\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "data = load_dataset(\"yelp_review_full\", split=\"train[:2000]\")  # small sample\n",
    "```\n",
    "\n",
    "Tokenize:\n",
    "\n",
    "```python\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized = data.map(tokenize, batched=True)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **7. Training Loop Using Transformers Trainer**\n",
    "\n",
    "```python\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"lora-llm\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=50,\n",
    "    max_steps=200,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-4\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "LoRA updates only the low-rank parameters (`A` and `B`) and keeps the main model frozen.\n",
    "\n",
    "---\n",
    "\n",
    "#### **8. Use the Fine-Tuned LLM**\n",
    "\n",
    "```python\n",
    "model.eval()\n",
    "prompt = \"Explain LoRA in simple words.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "output = model.generate(**inputs, max_new_tokens=80)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **9. Save Only the LoRA Adapter (Not the Full LLM)**\n",
    "\n",
    "```python\n",
    "model.save_pretrained(\"lora_adapter\")\n",
    "```\n",
    "\n",
    "This directory contains only ~5–20MB of LoRA weights (tiny).\n",
    "\n",
    "You can later apply them to the full LLM again:\n",
    "\n",
    "```python\n",
    "from peft import PeftModel\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "model = PeftModel.from_pretrained(model, \"lora_adapter\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### ✔ **What Happens Inside the LLM? (Mechanism)**\n",
    "\n",
    "LoRA modifies transformer attention layers:\n",
    "\n",
    "### Original Q projection:\n",
    "\n",
    "$$\n",
    "Q = x W_q\n",
    "$$\n",
    "\n",
    "### With LoRA:\n",
    "\n",
    "$$\n",
    "Q = x W_q + x (B A)\n",
    "$$\n",
    "\n",
    "here:\n",
    "\n",
    "* $A: d \\rightarrow r$ small\n",
    "* $B: r \\rightarrow d$ small\n",
    "* $W_q$ frozen\n",
    "* only $A$ and $B$ are trained\n",
    "\n",
    "This gives the model a **learned correction (\"nudge\")** without touching the original LLM weights.\n",
    "\n",
    "---\n",
    "\n",
    "**FINAL SUMMARY — LoRA on LLMs**\n",
    "\n",
    "| Feature                      | Benefit                             |\n",
    "| ---------------------------- | ----------------------------------- |\n",
    "| Only 0.1% parameters trained | Fast & cheap fine-tuning            |\n",
    "| LLM weights frozen           | Prevents catastrophic forgetting    |\n",
    "| LoRA adapters tiny           | Can store many skills               |\n",
    "| Works for all LLMs           | GPT-J, Falcon, Mistral, LLaMA, etc. |\n",
    "| Easy with PEFT               | One line of code                    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967045a3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
