{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35beab7d",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## Tokenization\n",
    "\n",
    "### Why Do We Need Tokenization?\n",
    "\n",
    "Neural networks operate on **numbers**, not raw text.\n",
    "Tokenization converts text into an index sequence.\n",
    "\n",
    "The challenge:\n",
    "\n",
    "* Natural language has **millions** of possible words.\n",
    "* New words appear constantly (“microplastics”, “uninstallable”, “ChatGPT”).\n",
    "* Typing errors (“accomodate”), slang (“lol”), and morphology (“running”, “runs”, “ran”) increase variety.\n",
    "* A fixed vocabulary will cause **Out-of-Vocabulary (OOV)** problems.\n",
    "\n",
    "#### Example:\n",
    "\n",
    "Traditional word tokenizers break on unknown words:\n",
    "\n",
    "```\n",
    "\"unhappiness\" → [unknown]\n",
    "```\n",
    "\n",
    "This is unacceptable for real-world language modeling.\n",
    "\n",
    "**Solution:**\n",
    "Use **subword tokenization** to break words into meaningful pieces.\n",
    "\n",
    "---\n",
    "\n",
    "### Core Idea Behind Subword Tokenization\n",
    "\n",
    "Instead of full words, models use **subwords**:\n",
    "\n",
    "* “unhappiness” → “un”, “happy”, “ness”\n",
    "* “microplastics” → “micro”, “plastic”, “s”\n",
    "* “playing” → “play”, “ing”\n",
    "\n",
    "Benefits:\n",
    "\n",
    "* Covers all words (no OOV)\n",
    "* Compact vocabulary (20k–50k tokens)\n",
    "* Keeps common words whole\n",
    "* Splits rare words into pieces\n",
    "* Efficient for multilingual models\n",
    "\n",
    "---\n",
    "\n",
    "### Byte Pair Encoding (BPE)\n",
    "\n",
    "#### **Algorithm Summary**\n",
    "\n",
    "BPE builds tokens by repeatedly merging **the most frequent pair of symbols**.\n",
    "\n",
    "Initial symbols are characters.\n",
    "\n",
    "Example:\n",
    "\n",
    "Training text:\n",
    "\n",
    "```\n",
    "low lowly low\n",
    "```\n",
    "\n",
    "Start with characters:\n",
    "\n",
    "```\n",
    "l o w\n",
    "l o w l y\n",
    "l o w\n",
    "```\n",
    "\n",
    "Count pairs:\n",
    "\n",
    "* (\"l\",\"o\")\n",
    "* (\"o\",\"w\")\n",
    "* (\"w\",\"l\")\n",
    "* (\"l\",\"y\")\n",
    "\n",
    "Merge the most frequent pair, e.g.:\n",
    "\n",
    "1. merge \"l\"+\"o\" → \"lo\"\n",
    "2. merge \"lo\"+\"w\" → \"low\"\n",
    "3. merge \"l\"+\"y\" → \"ly\"\n",
    "\n",
    "Final tokens:\n",
    "\n",
    "* low\n",
    "* lowly\n",
    "\n",
    "#### **Properties**\n",
    "\n",
    "* Deterministic\n",
    "* Based strictly on frequency\n",
    "* Used in GPT-2, GPT-3, LLaMA-1, CLIP\n",
    "\n",
    "---\n",
    "\n",
    "### 4. WordPiece (Used in BERT)\n",
    "\n",
    "WordPiece is similar to BPE but uses **likelihood maximization**, not frequency.\n",
    "\n",
    "#### Key difference:\n",
    "\n",
    "**Select merges that maximize the likelihood of the training corpus under a language model**.\n",
    "\n",
    "Example:\n",
    "BPE merges the most frequent pair (\"ing\", \"ly\")\n",
    "WordPiece merges the pair that most improves the model’s probability of generating the text.\n",
    "\n",
    "This leads to:\n",
    "\n",
    "* A more semantically consistent vocabulary\n",
    "* Better handling of rare words\n",
    "\n",
    "WordPiece outputs tokens prefixed with “##” to mark continuation:\n",
    "\n",
    "```\n",
    "playing → play, ##ing\n",
    "unwanted → un, ##want, ##ed\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### SentencePiece (Used in T5, LLaMA-2, GPT-NeoX)\n",
    "\n",
    "SentencePiece does **not require whitespace**.\n",
    "It treats the input as a raw byte stream.\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "Hello world\n",
    "```\n",
    "\n",
    "Becomes:\n",
    "\n",
    "```\n",
    "H e l l o _ w o r l d\n",
    "```\n",
    "\n",
    "(\"_\" means space)\n",
    "\n",
    "Advantages:\n",
    "\n",
    "* Language-agnostic (works for Japanese, Chinese)\n",
    "* Robust to noise and misspellings\n",
    "* Can use BPE or Unigram algorithm internally\n",
    "\n",
    "---\n",
    "\n",
    "### PyTorch Implementation (Toy but Faithful)\n",
    "\n",
    "Below is a **minimal working demonstration** of:\n",
    "\n",
    "* BPE training\n",
    "* BPE tokenization\n",
    "* PyTorch encoding\n",
    "\n",
    "This is simplified but accurate enough to understand the mechanism.\n",
    "\n",
    "---\n",
    "\n",
    "### Toy BPE Training\n",
    "\n",
    "```python\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "def build_bpe_vocab(corpus, num_merges=10):\n",
    "    # Step 1: Split words into characters + EOS marker\n",
    "    vocab = Counter([' '.join(w) + ' </w>' for w in corpus])\n",
    "\n",
    "    merges = []\n",
    "\n",
    "    for _ in range(num_merges):\n",
    "        # Count all symbol pairs\n",
    "        pair_counts = Counter()\n",
    "\n",
    "        for word, freq in vocab.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols)-1):\n",
    "                pair = (symbols[i], symbols[i+1])\n",
    "                pair_counts[pair] += freq\n",
    "\n",
    "        if not pair_counts:\n",
    "            break\n",
    "\n",
    "        # Most frequent pair\n",
    "        best_pair = max(pair_counts, key=pair_counts.get)\n",
    "        merges.append(best_pair)\n",
    "\n",
    "        # Merge the pair in all words\n",
    "        new_vocab = Counter()\n",
    "        bigram = ' '.join(best_pair)\n",
    "\n",
    "        for word, freq in vocab.items():\n",
    "            updated = word.replace(bigram, ''.join(best_pair))\n",
    "            new_vocab[updated] += freq\n",
    "\n",
    "        vocab = new_vocab\n",
    "\n",
    "    return merges\n",
    "```\n",
    "\n",
    "#### Train BPE\n",
    "\n",
    "```python\n",
    "corpus = [\"low\", \"lower\", \"lowest\", \"lowly\"]\n",
    "merges = build_bpe_vocab(corpus, num_merges=10)\n",
    "print(\"Learned merges:\", merges)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 6.2 Apply BPE tokenization\n",
    "\n",
    "```python\n",
    "def bpe_tokenize(word, merges):\n",
    "    word = list(word) + [\"</w>\"]\n",
    "\n",
    "    merges_set = {\"\".join(a): a for a in merges}\n",
    "\n",
    "    while True:\n",
    "        pairs = [(word[i], word[i+1]) for i in range(len(word)-1)]\n",
    "        merge_candidates = [(a+b, (a,b)) for (a,b) in pairs if (a,b) in merges_set]\n",
    "\n",
    "        if not merge_candidates:\n",
    "            break\n",
    "\n",
    "        # Merge the first eligible pair\n",
    "        merge_str, (a, b) = merge_candidates[0]\n",
    "        new_word = []\n",
    "        i = 0\n",
    "        while i < len(word):\n",
    "            if i < len(word)-1 and word[i] == a and word[i+1] == b:\n",
    "                new_word.append(a+b)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_word.append(word[i])\n",
    "                i += 1\n",
    "\n",
    "        word = new_word\n",
    "\n",
    "    return word\n",
    "\n",
    "print(bpe_tokenize(\"lowly\", merges))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 6.3 Convert Tokens to PyTorch Tensor\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# Build a vocab mapping\n",
    "vocab = {}\n",
    "idx = 0\n",
    "for w in corpus:\n",
    "    tokens = bpe_tokenize(w, merges)\n",
    "    for t in tokens:\n",
    "        if t not in vocab:\n",
    "            vocab[t] = idx\n",
    "            idx += 1\n",
    "\n",
    "# Encode sentence\n",
    "sentence = \"lowly\"\n",
    "tokens = bpe_tokenize(sentence, merges)\n",
    "token_ids = torch.tensor([vocab[t] for t in tokens])\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token IDs tensor:\", token_ids)\n",
    "```\n",
    "\n",
    "This produces a PyTorch tensor ready for embedding layers.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary Table**\n",
    "\n",
    "| Method            | Main idea                            | Used in               |\n",
    "| ----------------- | ------------------------------------ | --------------------- |\n",
    "| **BPE**           | Merge most frequent character pairs  | GPT-2, LLaMA-1        |\n",
    "| **WordPiece**     | Maximize LM likelihood when merging  | BERT, RoBERTa         |\n",
    "| **SentencePiece** | Works without whitespace; byte-level | T5, LLaMA-2, GPT-NeoX |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
