{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea9cbf91",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## Multi-Head Attention\n",
    "\n",
    "**Multi-Head Attention = Multiple attention mechanisms running in parallel.**\n",
    "\n",
    "Each head learns to focus on **different relationships** between tokens.\n",
    "\n",
    "Example:\n",
    "\n",
    "* Head 1: learns subject–verb alignment\n",
    "* Head 2: learns long-range dependencies\n",
    "* Head 3: learns coreference (“it” → “cat”)\n",
    "* Head 4: learns punctuation/syntax patterns\n",
    "\n",
    "The outputs of all heads are concatenated → projected → passed to next layer.\n",
    "\n",
    "---\n",
    "\n",
    "###  Why multiple heads?\n",
    "\n",
    "One attention head can only learn **one type** of relation.\n",
    "Multiple heads allow the model to process **different patterns simultaneously**.\n",
    "\n",
    "For example, in the sentence:\n",
    "\n",
    "**“The cat that I adopted sleeps.”**\n",
    "\n",
    "A good LLM needs to learn:\n",
    "\n",
    "* subject relation: cat → sleeps\n",
    "* relative clause: I adopted → cat\n",
    "* article relations: The → cat\n",
    "* semantic meaning: sleeps → cat\n",
    "\n",
    "One head alone cannot learn all this.\n",
    "\n",
    "---\n",
    "\n",
    "### How Multi-Head Attention works\n",
    "\n",
    "Suppose we have **h heads**.\n",
    "For each head:\n",
    "\n",
    "#### 1. Create separate projection matrices:\n",
    "\n",
    "* $W_Q^1, W_K^1, W_V^1$\n",
    "* $W_Q^2, W_K^2, W_V^2$\n",
    "* ...\n",
    "* $W_Q^h, W_K^h, W_V^h$\n",
    "\n",
    "#### 2. Compute attention independently for each head:\n",
    "\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(XW_Q^i, XW_K^i, XW_V^i)\n",
    "$$\n",
    "\n",
    "### 3. Concatenate all heads:\n",
    "\n",
    "$$\n",
    "\\text{concat} = [\\text{head}_1, \\text{head}_2, \\dots, \\text{head}_h]\n",
    "$$\n",
    "\n",
    "#### 4. Apply a final output projection:\n",
    "\n",
    "$$\n",
    "\\text{MHAoutput} = \\text{concat} \\cdot W_O\n",
    "$$\n",
    "\n",
    "Where $W_O$ is another learned matrix.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Visual Overview (simple)**\n",
    "\n",
    "```\n",
    "                ┌─────────────┐\n",
    "Input Embedding →  Head 1      ─┐\n",
    "                ├─────────────┤ │\n",
    "                │  Head 2      │ │\n",
    "                ├─────────────┤ │\n",
    "                │  Head 3      │ │\n",
    "                └─────────────┘ │\n",
    "                                 ↓\n",
    "                    Concatenate Outputs\n",
    "                                 ↓\n",
    "                       Linear Projection\n",
    "                                 ↓\n",
    "                         MHA Output\n",
    "```\n",
    "\n",
    "Each head sees the same input but learns different patterns.\n",
    "\n",
    "---\n",
    "\n",
    "# **5. Mini Numerical Example (2 Heads)**\n",
    "\n",
    "To keep it simple:\n",
    "\n",
    "* Only **one token**\n",
    "* Model dimension = 4\n",
    "* Each head dimension = 2\n",
    "* We show how heads create different outputs\n",
    "\n",
    "### Input token embedding:\n",
    "\n",
    "```\n",
    "X = [1, 2, 3, 4]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Head 1 projection matrices**\n",
    "\n",
    "Pick simple values:\n",
    "\n",
    "```\n",
    "WQ1 = [[1,0],[0,1],[0,0],[1,0]]\n",
    "WK1 = [[1,1],[0,1],[1,0],[0,1]]\n",
    "WV1 = [[1,0],[0,2],[1,1],[0,1]]\n",
    "```\n",
    "\n",
    "Compute:\n",
    "\n",
    "```\n",
    "Q1 = X @ WQ1\n",
    "K1 = X @ WK1\n",
    "V1 = X @ WV1\n",
    "```\n",
    "\n",
    "After attention calculation → head1_output\n",
    "(Details skipped to keep it short)\n",
    "\n",
    "Assume:\n",
    "\n",
    "```\n",
    "head1_output = [0.5, 1.2]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Head 2 projection matrices**\n",
    "\n",
    "Different values:\n",
    "\n",
    "```\n",
    "WQ2 = [[0,1],[1,0],[1,0],[0,1]]\n",
    "WK2 = [[0,1],[1,1],[0,0],[1,0]]\n",
    "WV2 = [[0,1],[1,1],[0,2],[1,0]]\n",
    "```\n",
    "\n",
    "Compute:\n",
    "\n",
    "```\n",
    "Q2 = X @ WQ2\n",
    "K2 = X @ WK2\n",
    "V2 = X @ WV2\n",
    "```\n",
    "\n",
    "Assume:\n",
    "\n",
    "```\n",
    "head2_output = [−0.4, 2.3]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Concatenate heads**\n",
    "\n",
    "```\n",
    "concat = [0.5, 1.2, −0.4, 2.3]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Final output projection**\n",
    "\n",
    "With some matrix (W_O):\n",
    "\n",
    "```\n",
    "MHA_output = concat @ W_O\n",
    "```\n",
    "\n",
    "This produces the final vector passed to the next layer.\n",
    "\n",
    "---\n",
    "\n",
    "# **6. Key points to remember**\n",
    "\n",
    "### **A. Each head has different WQ, WK, WV**\n",
    "\n",
    "So each head attends to different features in the sequence.\n",
    "\n",
    "### **B. All heads see the full input**\n",
    "\n",
    "But learn different attention patterns.\n",
    "\n",
    "### **C. Multi-head attention == multi-perspective understanding**\n",
    "\n",
    "This is why LLMs can:\n",
    "\n",
    "* resolve pronouns\n",
    "* understand relationships\n",
    "* perform reasoning\n",
    "* encode structure\n",
    "* remember long context\n",
    "\n",
    "### **D. Outputs are merged**\n",
    "\n",
    "Concatenation → linear projection → next layer.\n",
    "\n",
    "---\n",
    "\n",
    "# **7. Summary (easy version)**\n",
    "\n",
    "Multi-head attention =\n",
    "**“Run attention several times with different learned projections, so the model can focus on multiple aspects of the text at once.”**\n",
    "\n",
    "Each head learns something different.\n",
    "Combine all → richer understanding.\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can also demonstrate:\n",
    "\n",
    "* A full multi-head numerical example (Q, K, V per head)\n",
    "* How multi-head differs from single-head attention\n",
    "* How multi-head works in GPT specifically\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e3cb06",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
