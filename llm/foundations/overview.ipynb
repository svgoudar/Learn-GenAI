{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c51bd888",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Large Language Model\n",
    "\n",
    "**LLM (Large Language Model)** refers to a very large neural network—usually built using the **Transformer** architecture—that is trained on massive text datasets to understand, generate, and reason with natural language.\n",
    "\n",
    "Below is a **clean, complete explanation**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. What an LLM is\n",
    "\n",
    "An LLM is a **deep learning model** trained to predict the next token (word/piece of text).\n",
    "By learning this prediction task at scale, it acquires skills like:\n",
    "\n",
    "* language understanding\n",
    "* reasoning\n",
    "* summarization\n",
    "* translation\n",
    "* generation\n",
    "* question answering\n",
    "* coding\n",
    "\n",
    "Examples: GPT-4, GPT-5, Claude, Llama 3, Gemini.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. How LLMs work (core concepts)\n",
    "\n",
    "#### **A. Tokenization**\n",
    "\n",
    "Text is broken into small pieces called **tokens**.\n",
    "Models operate on sequences of tokens.\n",
    "\n",
    "#### **B. Transformer architecture**\n",
    "\n",
    "LLMs use the **Transformer decoder**:\n",
    "\n",
    "* Self-attention layers\n",
    "* Multi-head attention\n",
    "* Feed-forward layers\n",
    "* Residual connections\n",
    "* Layer normalization\n",
    "\n",
    "#### **C. Self-attention**\n",
    "\n",
    "The model looks at all previous tokens and learns:\n",
    "\n",
    "* which parts of text are important\n",
    "* how words relate to each other\n",
    "* long-range dependencies\n",
    "\n",
    "#### **D. Next-token prediction**\n",
    "\n",
    "Core training objective:\n",
    "\n",
    "$$\n",
    "P(x_t \\mid x_1, x_2, ..., x_{t-1})\n",
    "$$\n",
    "\n",
    "Predicting the next token teaches:\n",
    "\n",
    "* grammar\n",
    "* facts\n",
    "* reasoning patterns\n",
    "* logic\n",
    "* structure of human language\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Why LLMs are “large”\n",
    "\n",
    "They have:\n",
    "\n",
    "* **billions of parameters**\n",
    "* trained on **trillions of tokens**\n",
    "* with **thousands of GPUs/TPUs**\n",
    "\n",
    "Scale leads to:\n",
    "\n",
    "* better reasoning\n",
    "* better generalization\n",
    "* more fluent text\n",
    "* broader world knowledge\n",
    "\n",
    "---\n",
    "\n",
    "### 4. How LLMs are trained\n",
    "\n",
    "#### Step 1 — Pretraining\n",
    "\n",
    "Train on massive corpora:\n",
    "\n",
    "* books\n",
    "* websites\n",
    "* code\n",
    "* articles\n",
    "* public datasets\n",
    "\n",
    "Objective: **predict next token**.\n",
    "\n",
    "#### Step 2 — Instruction Tuning (SFT)\n",
    "\n",
    "Make models follow instructions using human-written examples.\n",
    "\n",
    "#### Step 3 — Alignment (RLHF / DPO)\n",
    "\n",
    "Use human feedback to prefer:\n",
    "\n",
    "* helpful\n",
    "* safe\n",
    "* non-toxic\n",
    "* correct\n",
    "  responses.\n",
    "\n",
    "#### Step 4 — Capability Extensions\n",
    "\n",
    "* Tool use / function calling\n",
    "* Embeddings\n",
    "* Vision + audio + multimodal\n",
    "* Agents, planning, memory\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Capabilities of LLMs\n",
    "\n",
    "* Conversation\n",
    "* Summarization\n",
    "* Translation\n",
    "* Content generation\n",
    "* Coding assistance\n",
    "* Reasoning & chain-of-thought\n",
    "* Extracting structured data\n",
    "* Search + RAG\n",
    "* Task automation\n",
    "* Classification\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Limitations of LLMs\n",
    "\n",
    "* Hallucinations (confident wrong answers)\n",
    "* No guaranteed factuality\n",
    "* Sensitive to prompt wording\n",
    "* Limited by context window\n",
    "* Needs safety alignment\n",
    "\n",
    "---\n",
    "\n",
    "### 7. LLMs in Generative AI Systems\n",
    "\n",
    "LLMs act as the **intelligence engine** for:\n",
    "\n",
    "* chatbots\n",
    "* RAG applications\n",
    "* document analysis\n",
    "* workflow automation\n",
    "* multimodal interaction (vision/audio)\n",
    "* code generation\n",
    "\n",
    "They can:\n",
    "\n",
    "* generate text\n",
    "* interpret queries\n",
    "* write code\n",
    "* execute tools\n",
    "* orchestrate agents\n",
    "* combine search + reasoning\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**\n",
    "\n",
    "**LLM = Large neural model (Transformer-based) trained to predict the next token → learns language, reasoning, and generation capabilities.**\n",
    "\n",
    "It becomes powerful by combining:\n",
    "\n",
    "* massive training data\n",
    "* massive model size\n",
    "* attention mechanisms\n",
    "* alignment techniques\n",
    "\n",
    "If you want, I can also explain:\n",
    "\n",
    "* Transformers in simple terms\n",
    "* How LLM reasoning works internally\n",
    "* How LLMs handle context and memory\n",
    "* LLM vs RAG vs Fine-tuning\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```{dropdown} Click here for Sections\n",
    "```{tableofcontents}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7d13f4",
   "metadata": {},
   "source": [
    "Below is a **complete, structured list of all major concepts involved in Generative AI**, covering **theory, systems, models, data pipelines, MLOps, RAG, optimization, safety, evaluation, deployment, and governance**.\n",
    "\n",
    "This is a **master checklist** of everything you need to understand for Generative AI end-to-end.\n",
    "\n",
    "---\n",
    "\n",
    "# 1. Core AI & ML Foundations\n",
    "\n",
    "* Machine learning basics\n",
    "* Deep learning basics\n",
    "* Neural networks\n",
    "* Backpropagation\n",
    "* Loss functions (cross-entropy, KL divergence)\n",
    "* Activation functions (ReLU, GELU, SiLU)\n",
    "* Optimization (Adam, AdamW, SGD)\n",
    "* Regularization (dropout, weight decay)\n",
    "* Gradient clipping\n",
    "* Learning rate schedulers\n",
    "\n",
    "---\n",
    "\n",
    "# 2. Generative Model Families\n",
    "\n",
    "* Transformers\n",
    "* Encoder–decoder models\n",
    "* Decoder-only LLMs\n",
    "* Diffusion models\n",
    "* GANs\n",
    "* VAE (Variational Autoencoders)\n",
    "* Autoregressive models\n",
    "* Masked language modeling\n",
    "\n",
    "---\n",
    "\n",
    "# 3. Large Language Models (LLMs)\n",
    "\n",
    "* Attention mechanism\n",
    "* Multi-head attention\n",
    "* Self-attention vs cross-attention\n",
    "* Positional encodings\n",
    "* Tokenization (BPE, SentencePiece, WordPiece)\n",
    "* Context window & sliding windows\n",
    "* KV cache\n",
    "* Sampling strategies (top-k, top-p, temperature)\n",
    "* Beam search\n",
    "* Low-rank adaptation (LoRA)\n",
    "* Quantization (8-bit, 4-bit, AWQ, GPTQ)\n",
    "* Instruction tuning\n",
    "* Supervised fine-tuning (SFT)\n",
    "* Preference modeling\n",
    "* Reinforcement learning (RLHF)\n",
    "* DPO (Direct Preference Optimization)\n",
    "* PPO vs DPO differences\n",
    "\n",
    "---\n",
    "\n",
    "# 4. Multimodal Models\n",
    "\n",
    "* Vision Transformers (ViT)\n",
    "* CLIP\n",
    "* Vision–language models (VLM)\n",
    "* Speech-to-text\n",
    "* Text-to-speech\n",
    "* Audio representations (spectrograms)\n",
    "* Image embeddings\n",
    "* Video embeddings\n",
    "* OCR systems\n",
    "\n",
    "---\n",
    "\n",
    "# 5. Embeddings\n",
    "\n",
    "* Text embeddings\n",
    "* Sentence embeddings\n",
    "* Image/audio embeddings\n",
    "* Cosine similarity\n",
    "* Vector normalization\n",
    "* Dimensionality\n",
    "* Embedding model drift\n",
    "* Embedding versioning\n",
    "* Nearest neighbors search (ANN)\n",
    "* FAISS / HNSW / ScaNN\n",
    "\n",
    "---\n",
    "\n",
    "# 6. RAG (Retrieval Augmented Generation)\n",
    "\n",
    "## Retrieval\n",
    "\n",
    "* Chunking strategies (semantic, fixed-size, hybrid)\n",
    "* Overlap strategies\n",
    "* Semantic search\n",
    "* Hybrid search (BM25 + embeddings)\n",
    "* Ranking / re-ranking\n",
    "* Context assembly\n",
    "* Query rewriting\n",
    "* Multi-step retrieval\n",
    "* Routing / query classification\n",
    "\n",
    "## Storage\n",
    "\n",
    "* Vector databases (Pinecone, Qdrant, Weaviate, Chroma)\n",
    "* Metadata stores\n",
    "* Indexes (HNSW, IVF, Flat)\n",
    "* Sharding & replicas\n",
    "* TTL and versioning\n",
    "\n",
    "## Response synthesis\n",
    "\n",
    "* Context windows\n",
    "* Chain-of-thought (CoT)\n",
    "* Tools & function calling\n",
    "* Retrieval fusion\n",
    "* Guardrails\n",
    "\n",
    "---\n",
    "\n",
    "# 7. Data Engineering for GenAI\n",
    "\n",
    "## Data ingestion\n",
    "\n",
    "* Connectors (S3/GCS/SharePoint/API/DB)\n",
    "* Scheduling (Airflow, Dagster)\n",
    "* KubernetesPodOperator\n",
    "* ETL vs ELT\n",
    "* Micro-batching\n",
    "* Streaming (Kafka, Kinesis)\n",
    "\n",
    "## Data preprocessing\n",
    "\n",
    "* Parsing (PDF, HTML, DOCX)\n",
    "* OCR\n",
    "* Cleaning (boilerplate removal, noise removal)\n",
    "* Normalization\n",
    "* Deduplication (exact + near-dup)\n",
    "* Tokenization & chunking\n",
    "* Schema design & evolution\n",
    "* Metadata extraction\n",
    "* Lineage tracking\n",
    "* PII detection\n",
    "* Safety filters\n",
    "\n",
    "## Data storage\n",
    "\n",
    "* Data lakes (S3/GCS/ADLS)\n",
    "* Data warehouses (Snowflake/BigQuery/Redshift)\n",
    "* OLTP databases (Postgres/MySQL/DynamoDB)\n",
    "* OLAP systems (ClickHouse, Druid)\n",
    "\n",
    "## Batch & streaming versioning\n",
    "\n",
    "* Bronze/Silver/Gold medallion architecture\n",
    "* Lakehouse concepts\n",
    "* Delta/Iceberg/Hudi\n",
    "\n",
    "---\n",
    "\n",
    "# 8. Training & Fine-Tuning\n",
    "\n",
    "* Pretraining datasets\n",
    "* Tokenization pipelines\n",
    "* SFT (Supervised Fine-Tuning)\n",
    "* RLHF (Reinforcement Learning from Human Feedback)\n",
    "* Preference modeling\n",
    "* DPO, ORPO\n",
    "* Continual training\n",
    "* Model drift\n",
    "* Curriculum learning\n",
    "* Training infrastructure (GPU/TPU clusters, distributed training)\n",
    "* Checkpointing\n",
    "* Mixed-precision training (FP16/BF16/FP8)\n",
    "* Gradient accumulation\n",
    "* Data packing\n",
    "\n",
    "---\n",
    "\n",
    "# 9. Evaluation\n",
    "\n",
    "* Perplexity\n",
    "* BLEU/ROUGE (older metrics)\n",
    "* Win-rate evaluation\n",
    "* Human evaluation\n",
    "* Model hallucination tests\n",
    "* Groundedness checks\n",
    "* RAG evaluation (Faithfulness, Recall, Relevance)\n",
    "* Bias / toxicity eval\n",
    "* Adversarial prompts testing\n",
    "\n",
    "---\n",
    "\n",
    "# 10. Scaling & Performance\n",
    "\n",
    "* Latency vs consistency trade-offs\n",
    "* Throughput optimization\n",
    "* Batching & micro-batching\n",
    "* Caching (KV cache, response cache)\n",
    "* Parallel inference\n",
    "* Speculative decoding\n",
    "* GPU utilization\n",
    "* Autoscaling\n",
    "* Distributed serving\n",
    "* Cost optimization\n",
    "\n",
    "---\n",
    "\n",
    "# 11. Safety, Security & Governance\n",
    "\n",
    "* Access control & RBAC\n",
    "* ABAC\n",
    "* Secrets management\n",
    "* Private networking / VPC\n",
    "* Encryption (in-flight & at-rest)\n",
    "* PII detection & redaction\n",
    "* Policy filtering\n",
    "* Guardrails\n",
    "* Data quality rules\n",
    "* Audit logs\n",
    "* Compliance (SOC2, HIPAA, GDPR)\n",
    "\n",
    "---\n",
    "\n",
    "# 12. Monitoring & Observability\n",
    "\n",
    "* Prompt logs\n",
    "* Response latency\n",
    "* Embedding drift detection\n",
    "* Pipeline run metrics\n",
    "* Retrieval quality metrics\n",
    "* Model usage analytics\n",
    "* Reliability SLOs\n",
    "\n",
    "---\n",
    "\n",
    "# 13. Deployment\n",
    "\n",
    "* Model serving frameworks (Triton, vLLM, TensorRT)\n",
    "* API gateways\n",
    "* Load balancers\n",
    "* Autoscaling GPU pods\n",
    "* Canary + shadow deployments\n",
    "* CI/CD for LLM pipelines\n",
    "* Prompt router / model router\n",
    "\n",
    "---\n",
    "\n",
    "# 14. Applications & Workflows\n",
    "\n",
    "* RAG chatbots\n",
    "* QA search systems\n",
    "* Document automation\n",
    "* Code generation\n",
    "* Multi-agent workflows\n",
    "* Autonomous task execution\n",
    "* Summarization, translation\n",
    "* Fine-tuned domain assistants\n",
    "\n",
    "---\n",
    "\n",
    "# 15. Supporting Concepts\n",
    "\n",
    "* Knowledge graphs\n",
    "* Tool calling\n",
    "* Agent architectures\n",
    "* APIs, SDKs, LangChain, LlamaIndex\n",
    "* System prompts & prompt engineering\n",
    "* Self-embedding & long-context strategies\n",
    "* Context compression\n",
    "* Memory mechanisms\n",
    "\n",
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "These 15 categories include **all major concepts** you will encounter when working with Generative AI—covering **ML theory, data engineering, RAG, embeddings, training, MLOps, scaling, safety, evaluation, deployment, and governance**.\n",
    "\n",
    "If you want, I can also generate:\n",
    "\n",
    "* A **one-page cheat sheet**\n",
    "* A **roadmap for mastering Generative AI**\n",
    "* A **mind map diagram** of all concepts\n",
    "* A **learning path** based on your background\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
