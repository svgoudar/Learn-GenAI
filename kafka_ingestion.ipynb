{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de04f200",
   "metadata": {},
   "source": [
    "Build a small, end-to-end demo: logs → Kafka → PySpark clean → embeddings → Chroma → RAG with Ollama. Minimal, local, reproducible.\n",
    "\n",
    "# 0) Folder layout\n",
    "\n",
    "```\n",
    "genai-incidents/\n",
    "├─ docker-compose.yml\n",
    "├─ producer/producer.py\n",
    "├─ spark/stream_clean_from_kafka.py\n",
    "├─ ingest/embed_to_chroma.py\n",
    "├─ rag/query_rag.py\n",
    "└─ airflow/dags/reindex_daily.py\n",
    "```\n",
    "\n",
    "# 1) Infra: Kafka (Zookeeper mode, simplest)\n",
    "\n",
    "**docker-compose.yml**\n",
    "\n",
    "```yaml\n",
    "version: \"3.8\"\n",
    "services:\n",
    "  zookeeper:\n",
    "    image: confluentinc/cp-zookeeper:7.5.0\n",
    "    environment:\n",
    "      ZOOKEEPER_CLIENT_PORT: 2181\n",
    "    ports: [\"2181:2181\"]\n",
    "\n",
    "  kafka:\n",
    "    image: confluentinc/cp-kafka:7.5.0\n",
    "    depends_on: [zookeeper]\n",
    "    ports: [\"9092:9092\"]\n",
    "    environment:\n",
    "      KAFKA_BROKER_ID: 1\n",
    "      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n",
    "      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT\n",
    "      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092\n",
    "      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n",
    "```\n",
    "\n",
    "Run:\n",
    "\n",
    "```\n",
    "docker compose up -d\n",
    "```\n",
    "\n",
    "# 2) Stream in sample syslog lines → Kafka\n",
    "\n",
    "**producer/producer.py**\n",
    "\n",
    "```python\n",
    "from kafka import KafkaProducer\n",
    "import json, random, time, datetime\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=\"localhost:9092\",\n",
    "    value_serializer=lambda v: json.dumps(v).encode(\"utf-8\")\n",
    ")\n",
    "services = [\"nginx\",\"auth\",\"payment\",\"orders\",\"redis\",\"postgres\"]\n",
    "levels = [\"INFO\",\"WARN\",\"ERROR\"]\n",
    "\n",
    "def line():\n",
    "    svc = random.choice(services)\n",
    "    lvl = random.choices(levels, weights=[0.7,0.2,0.1])[0]\n",
    "    msg = {\n",
    "        \"INFO\":  \"service healthy\",\n",
    "        \"WARN\":  \"slow response detected\",\n",
    "        \"ERROR\": \"connection timeout to dependency\"\n",
    "    }[lvl]\n",
    "    return {\n",
    "        \"ts\": datetime.datetime.utcnow().isoformat(),\n",
    "        \"service\": svc,\n",
    "        \"level\": lvl,\n",
    "        \"message\": msg\n",
    "    }\n",
    "\n",
    "while True:\n",
    "    rec = line()\n",
    "    producer.send(\"logs\", value=rec)\n",
    "    print(\"→\", rec)\n",
    "    time.sleep(0.5)\n",
    "```\n",
    "\n",
    "Install and run:\n",
    "\n",
    "```\n",
    "pip install kafka-python\n",
    "python producer/producer.py\n",
    "```\n",
    "\n",
    "Create topic if needed:\n",
    "\n",
    "```\n",
    "docker exec -it $(docker ps -qf name=kafka) \\\n",
    "  kafka-topics --create --topic logs --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1\n",
    "```\n",
    "\n",
    "# 3) PySpark Structured Streaming: clean + land to parquet\n",
    "\n",
    "**spark/stream_clean_from_kafka.py**\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, lower, regexp_replace, trim\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"clean-logs\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"ts\", StringType()),\n",
    "    StructField(\"service\", StringType()),\n",
    "    StructField(\"level\", StringType()),\n",
    "    StructField(\"message\", StringType()),\n",
    "])\n",
    "\n",
    "raw = (spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\",\"localhost:9092\")\n",
    "    .option(\"subscribe\",\"logs\")\n",
    "    .option(\"startingOffsets\",\"latest\")\n",
    "    .load())\n",
    "\n",
    "json = raw.select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"j\")).select(\"j.*\")\n",
    "\n",
    "clean = (json\n",
    "    .withColumn(\"service\", lower(trim(col(\"service\"))))\n",
    "    .withColumn(\"level\", lower(trim(col(\"level\"))))\n",
    "    .withColumn(\"message\", lower(regexp_replace(col(\"message\"), r\"[^a-z0-9 _\\-:]\", \" \")))\n",
    "    .filter(\"message is not null and length(message) > 3\"))\n",
    "\n",
    "# Write cleaned stream to small parquet files for downstream embedding\n",
    "query = (clean.writeStream\n",
    "    .format(\"parquet\")\n",
    "    .option(\"path\",\"landing/clean_parquet\")\n",
    "    .option(\"checkpointLocation\",\"landing/_chk_clean\")\n",
    "    .outputMode(\"append\")\n",
    "    .start())\n",
    "\n",
    "query.awaitTermination()\n",
    "```\n",
    "\n",
    "Run:\n",
    "\n",
    "```\n",
    "pip install pyspark\n",
    "python spark/stream_clean_from_kafka.py\n",
    "```\n",
    "\n",
    "This continuously writes cleaned records into `landing/clean_parquet/`.\n",
    "\n",
    "# 4) Embed new records and upsert into Chroma\n",
    "\n",
    "**ingest/embed_to_chroma.py**\n",
    "\n",
    "```python\n",
    "import time, uuid, os\n",
    "from glob import glob\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Reuse Spark just to read Parquet easily (local mode)\n",
    "spark = SparkSession.builder.appName(\"read-clean\").getOrCreate()\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "client = chromadb.Client()\n",
    "col = client.get_or_create_collection(\"incidents\")\n",
    "\n",
    "# Simple file cursor to avoid reprocessing\n",
    "STATE = \".embed_state\"\n",
    "last_count = int(open(STATE).read().strip()) if os.path.exists(STATE) else 0\n",
    "\n",
    "def count_rows(path):\n",
    "    df = spark.read.parquet(path)\n",
    "    return df.count(), df\n",
    "\n",
    "while True:\n",
    "    if not os.path.exists(\"landing/clean_parquet\"):\n",
    "        time.sleep(2); continue\n",
    "\n",
    "    total, df = count_rows(\"landing/clean_parquet\")\n",
    "    if total <= last_count:\n",
    "        time.sleep(2); continue\n",
    "\n",
    "    new_df = df.limit(total - last_count)  # simplistic; good enough for demo\n",
    "    rows = [r.asDict() for r in new_df.collect()]\n",
    "\n",
    "    texts = [f\"[{r['ts']}] {r['service']} {r['level']} :: {r['message']}\" for r in rows]\n",
    "    embs  = model.encode(texts, show_progress_bar=False).tolist()\n",
    "    ids   = [str(uuid.uuid4()) for _ in texts]\n",
    "\n",
    "    col.add(ids=ids, embeddings=embs, metadatas=rows, documents=texts)\n",
    "    last_count = total\n",
    "    with open(STATE,\"w\") as f: f.write(str(last_count))\n",
    "    print(f\"Indexed {len(texts)} new records. Total seen: {last_count}\")\n",
    "    time.sleep(2)\n",
    "```\n",
    "\n",
    "Install and run:\n",
    "\n",
    "```\n",
    "pip install sentence-transformers chromadb\n",
    "python ingest/embed_to_chroma.py\n",
    "```\n",
    "\n",
    "# 5) Query via RAG with Ollama or HF\n",
    "\n",
    "First run an LLM locally. Example with **Ollama**:\n",
    "\n",
    "```\n",
    "ollama pull llama3\n",
    "ollama serve\n",
    "```\n",
    "\n",
    "**rag/query_rag.py**\n",
    "\n",
    "```python\n",
    "from chromadb import Client\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import requests, json\n",
    "\n",
    "db = Client().get_collection(\"incidents\")\n",
    "embed = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def ask_ollama(prompt):\n",
    "    r = requests.post(\"http://localhost:11434/api/generate\",\n",
    "                      json={\"model\":\"llama3\",\"prompt\":prompt,\"stream\":False})\n",
    "    print(json.loads(r.text)[\"response\"])\n",
    "\n",
    "def ask(question, k=5):\n",
    "    qv = embed.encode(question).tolist()\n",
    "    res = db.query(query_embeddings=[qv], n_results=k)\n",
    "    ctx = \"\\n\".join(res[\"documents\"][0])\n",
    "    prompt = f\"\"\"You are an incident analyst. Use only the context.\n",
    "\n",
    "Context:\n",
    "{ctx}\n",
    "\n",
    "Question: {question}\n",
    "Answer with a concise root cause or hypothesis and a fix.\"\"\"\n",
    "    ask_ollama(prompt)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ask(\"why are we seeing payment timeouts and how to fix?\")\n",
    "```\n",
    "\n",
    "Run:\n",
    "\n",
    "```\n",
    "pip install requests sentence-transformers chromadb\n",
    "python rag/query_rag.py\n",
    "```\n",
    "\n",
    "# 6) Airflow: nightly maintenance (batch reindex or vacuum)\n",
    "\n",
    "Minimal DAG that rebuilds the Chroma collection from parquet once per night.\n",
    "\n",
    "**airflow/dags/reindex_daily.py**\n",
    "\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "default_args = {\"owner\":\"you\",\"retries\":0}\n",
    "with DAG(\n",
    "    \"reindex_chroma\",\n",
    "    default_args=default_args,\n",
    "    schedule_interval=\"0 2 * * *\",  # 02:00 daily\n",
    "    start_date=datetime(2025,1,1),\n",
    "    catchup=False,\n",
    ") as dag:\n",
    "    rebuild = BashOperator(\n",
    "        task_id=\"rebuild\",\n",
    "        bash_command=\"python /opt/flows/rebuild_index.py\"\n",
    "    )\n",
    "```\n",
    "\n",
    "Your `/opt/flows/rebuild_index.py` can read all parquet, dedupe, and recreate the Chroma collection. For demo, you can point it to `ingest/embed_to_chroma.py` with a “full rebuild” flag.\n",
    "\n",
    "# How this proves each technology\n",
    "\n",
    "* **Kafka**: streaming ingestion of events.\n",
    "* **PySpark**: structured streaming clean + land to parquet.\n",
    "* **Embeddings + Vector DB (Chroma)**: semantic index.\n",
    "* **RAG**: retrieve top-k, prompt LLM with context.\n",
    "* **Ollama / HF**: local LLM inference.\n",
    "* **Airflow**: scheduled maintenance or rebuild.\n",
    "\n",
    "# Quick run order\n",
    "\n",
    "1. `docker compose up -d`\n",
    "2. Terminal A: `python spark/stream_clean_from_kafka.py`\n",
    "3. Terminal B: `python producer/producer.py`\n",
    "4. Terminal C: `python ingest/embed_to_chroma.py`\n",
    "5. Terminal D: start LLM → `ollama pull llama3 && ollama serve`\n",
    "6. Terminal D: `python rag/query_rag.py`\n",
    "\n",
    "You now have a working, local, real-time Incident Intelligence Assistant demonstrating Kafka + PySpark + Airflow + embeddings + Chroma + RAG + Ollama.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
