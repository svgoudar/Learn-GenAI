{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4f85a98",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## Chunking\n",
    "\n",
    "Chunking means **splitting large documents into smaller pieces (chunks)** so that LLMs and vector databases can process them efficiently.\n",
    "\n",
    "Example:\n",
    "A 20-page PDF → broken into small sections (chunks) of around 200–1000 tokens.\n",
    "\n",
    "Chunking determines **how documents are split** into pieces before sending them to an embedding model or LLM.\n",
    "Good chunking improves:\n",
    "\n",
    "* retrieval accuracy\n",
    "* context relevancy\n",
    "* summary quality\n",
    "* hallucination reduction\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Why Chunking Is Needed\n",
    "\n",
    "* Embedding models have token limits (e.g., 512 or 1024 tokens)\n",
    "* LLMs cannot process huge documents all at once\n",
    "* Retrieval works best on smaller, complete meaningful units\n",
    "\n",
    "Bad chunking → irrelevant retrieval → hallucinations\n",
    "Good chunking → precise answers → high RAG quality\n",
    "\n",
    "---\n",
    "\n",
    "### Common Chunking Strategies\n",
    "\n",
    "Below are the **8 major chunking strategies** used in RAG systems.\n",
    "\n",
    "---\n",
    "\n",
    "#### Fixed-Size Chunking\n",
    "\n",
    "Split by token or character count:\n",
    "\n",
    "* 200 tokens\n",
    "* 500 tokens\n",
    "* 1000 characters\n",
    "\n",
    "**Pros**\n",
    "\n",
    "* Simple\n",
    "* Fast\n",
    "* Works for uniform text\n",
    "\n",
    "**Cons**\n",
    "\n",
    "* Splits sentences awkwardly\n",
    "* Loses context\n",
    "* Causes hallucination when meaning is broken\n",
    "\n",
    "**Use Case**\n",
    "\n",
    "General documents where structure doesn’t matter.\n",
    "\n",
    "---\n",
    "\n",
    "#### Sliding Window / Overlapping Chunking\n",
    "\n",
    "Chunks overlap each other:\n",
    "\n",
    "Example (chunk size = 200, overlap = 50)\n",
    "\n",
    "```\n",
    "Chunk 1: tokens 1–200\n",
    "Chunk 2: tokens 150–350\n",
    "Chunk 3: tokens 300–500\n",
    "```\n",
    "\n",
    "**Pros**\n",
    "\n",
    "* Prevents text cuts across important sentences\n",
    "* Gives more context continuity\n",
    "* Works very well for QA and search\n",
    "\n",
    "**Cons**\n",
    "\n",
    "* More storage\n",
    "* More embeddings\n",
    "* More compute\n",
    "\n",
    "**Use Case**\n",
    "\n",
    "RAG for:\n",
    "\n",
    "* customer support\n",
    "* medical/legal documents\n",
    "* long articles\n",
    "\n",
    "---\n",
    "\n",
    "#### Sentence-Based Chunking\n",
    "\n",
    "Split text by sentence boundaries using NLP tools (Spacy, NLTK).\n",
    "\n",
    "**Pros**\n",
    "\n",
    "* Keeps semantic meaning intact\n",
    "* No abrupt mid-sentence cuts\n",
    "* Higher retrieval accuracy\n",
    "\n",
    "**Cons**\n",
    "\n",
    "* Sentences may be too short\n",
    "* Hard for embedding models that prefer 200–300 tokens\n",
    "\n",
    "**Use Case**\n",
    "\n",
    "Summarization, QA, reasoning tasks\n",
    "\n",
    "---\n",
    "\n",
    "#### Paragraph-Based Chunking\n",
    "\n",
    "Split by paragraphs (`\\n\\n` or heading markers).\n",
    "\n",
    "**Pros**\n",
    "\n",
    "* Natural semantic boundaries\n",
    "* Easy and fast\n",
    "\n",
    "**Cons**\n",
    "\n",
    "* Paragraphs vary in length\n",
    "* Some paragraphs may be very long (bad for embeddings)\n",
    "\n",
    "**Use Case**\n",
    "\n",
    "Documents, reports, books, blogs\n",
    "\n",
    "---\n",
    "\n",
    "#### Semantic Chunking (LLM-aware chunking)\n",
    "\n",
    "Uses LLM or embeddings to decide where to split.\n",
    "\n",
    "**How:**\n",
    "\n",
    "* Detect topic shift\n",
    "* Identify semantic similarity boundaries\n",
    "* Create chunks based on meaning, not size\n",
    "\n",
    "**Pros**\n",
    "\n",
    "* Best chunk quality\n",
    "* Very accurate retrieval\n",
    "* Avoids topic mixing\n",
    "\n",
    "**Cons**\n",
    "\n",
    "* Slower\n",
    "* Needs embeddings or LLM calls\n",
    "* More costly\n",
    "\n",
    "**Use Case**\n",
    "\n",
    "High-quality enterprise RAG (GraphRAG, agentic RAG)\n",
    "\n",
    "---\n",
    "\n",
    "#### Recursive Character/Text Splitter (LangChain Style)\n",
    "\n",
    "Strategy:\n",
    "\n",
    "* Try splitting by headings\n",
    "* If too long, split by paragraphs\n",
    "* If still too long, split by sentences\n",
    "* If still too long, split by chunk size\n",
    "\n",
    "**Pros**\n",
    "\n",
    "* Balances structure + size\n",
    "* Stable and widely used\n",
    "\n",
    "**Cons**\n",
    "\n",
    "* Still generic, not semantic\n",
    "\n",
    "**Use Case**\n",
    "\n",
    "Standard RAG implementations\n",
    "\n",
    "---\n",
    "\n",
    "#### Hybrid Chunking\n",
    "\n",
    "Combines multiple methods:\n",
    "\n",
    "* semantic + sliding window\n",
    "* paragraph + token splitting\n",
    "* sentence + overlap\n",
    "\n",
    "**Pros**\n",
    "\n",
    "* Best trade-off\n",
    "* High retrieval accuracy\n",
    "* Maintains context\n",
    "\n",
    "**Use Case**\n",
    "\n",
    "Production RAG systems\n",
    "\n",
    "---\n",
    "\n",
    "#### Structure-Aware Chunking\n",
    "\n",
    "Splits based on structure in files such as:\n",
    "\n",
    "* Markdown (`#`, `##`, lists)\n",
    "* HTML (`<h1>`, `<p>`)\n",
    "* PDFs using layout\n",
    "* Tables separated into cell-level chunks\n",
    "\n",
    "**Pros**\n",
    "\n",
    "* Very clean, meaningful chunks\n",
    "* Preserves document flow\n",
    "\n",
    "**Cons**\n",
    "\n",
    "* Harder to implement\n",
    "\n",
    "**Use Case**\n",
    "\n",
    "Technical docs (API docs, legal docs, research papers)\n",
    "\n",
    "---\n",
    "\n",
    "**Choosing the Right Chunking Strategy**\n",
    "\n",
    "| Goal                          | Best Strategy                 |\n",
    "| ----------------------------- | ----------------------------- |\n",
    "| QA over long text             | Sliding window + paragraph    |\n",
    "| Precise factual retrieval     | Semantic chunking             |\n",
    "| Fast + simple                 | Fixed size                    |\n",
    "| Very clean document structure | Structure-aware               |\n",
    "| Cost-optimized                | Sentence-based + mild overlap |\n",
    "| Enterprise RAG                | Hybrid (semantic + window)    |\n",
    "\n",
    "---\n",
    "\n",
    "**Ideal Chunk Size (Rule of Thumb)**\n",
    "\n",
    "* **200–400 tokens** = ideal for embedding models\n",
    "* **Overlap 10–20%**\n",
    "* Avoid chunks > 512 tokens\n",
    "\n",
    "Reason:\n",
    "Smaller chunks → higher precision\n",
    "Bigger chunks → more recall but slower + expensive\n",
    "\n",
    "---\n",
    "\n",
    "**Example Chunking (Sliding Window)**\n",
    "\n",
    "Text:\n",
    "\n",
    "```\n",
    "Machine learning is a field of AI...\n",
    "It uses statistical methods...\n",
    "Neural networks are widely used...\n",
    "```\n",
    "\n",
    "Chunk size = 20 tokens, overlap = 5 tokens\n",
    "\n",
    "```\n",
    "Chunk 1: tokens 1–20\n",
    "Chunk 2: tokens 15–35\n",
    "Chunk 3: tokens 30–50\n",
    "```\n",
    "\n",
    "This ensures continuity.\n",
    "\n",
    "---\n",
    "\n",
    "**One-Sentence Summary**\n",
    "\n",
    "**Chunking strategies determine how documents are split into meaningful pieces for embedding and retrieval. Better chunking = better RAG accuracy + fewer hallucinations.**\n",
    "\n",
    "| Document Type        | Best Chunking Strategy        | Why                          |\n",
    "| -------------------- | ----------------------------- | ---------------------------- |\n",
    "| PDFs                 | Recursive or Hybrid           | maintain structure + meaning |\n",
    "| Legal contracts      | Semantic                      | meaning-heavy, multi-hop     |\n",
    "| Medical records      | Semantic                      | context-sensitive            |\n",
    "| Blogs/articles       | Recursive                     | paragraph structure          |\n",
    "| Code                 | Fixed or Token-based          | predictable formatting       |\n",
    "| Transcripts          | Fixed or Semantic             | repetitive, long sequences   |\n",
    "| Enterprise knowledge | Hybrid (recursive + semantic) | mixed formats                |\n",
    "| Scientific Papers    | Semantic or Hybrid            | multi-hop reasoning          |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98366c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed-size Chunks:\n",
      "\n",
      "Chunk 1:\n",
      "LangChain provides multiple chunking techniques for splitting large documents.\n",
      "Chunking is essential for RAG because embedding models have token limits.\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text = \"\"\"\n",
    "LangChain provides multiple chunking techniques for splitting large documents.\n",
    "Chunking is essential for RAG because embedding models have token limits.\n",
    "\"\"\"\n",
    "\n",
    "splitter = CharacterTextSplitter(\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=10,\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(text)\n",
    "\n",
    "print(\"Fixed-size Chunks:\")\n",
    "for i, c in enumerate(chunks):\n",
    "    print(f\"\\nChunk {i+1}:\\n{c}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "094cecf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recursive Chunks:\n",
      "\n",
      "Chunk 1:\n",
      "LangChain provides multiple chunking techniques for splitting large documents.\n",
      "\n",
      "Chunk 2:\n",
      "Chunking is essential for RAG because embedding models have token limits.\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=80,\n",
    "    chunk_overlap=20,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "chunks = splitter.split_text(text)\n",
    "\n",
    "print(\"\\nRecursive Chunks:\")\n",
    "for i, c in enumerate(chunks):\n",
    "    print(f\"\\nChunk {i+1}:\\n{c}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "647633be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Token-based Chunks:\n",
      "\n",
      "Chunk 1:\n",
      "\n",
      "LangChain provides multiple chunking techniques for splitting large documents.\n",
      "Chunking is essential for RAG because embedding models have token limits\n",
      "\n",
      "Chunk 2:\n",
      "ding models have token limits.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_classic.text_splitter import TokenTextSplitter\n",
    "\n",
    "token_splitter = TokenTextSplitter(\n",
    "    chunk_size=30,\n",
    "    chunk_overlap=5\n",
    ")\n",
    "\n",
    "chunks = token_splitter.split_text(text)\n",
    "\n",
    "print(\"\\nToken-based Chunks:\")\n",
    "for i, c in enumerate(chunks):\n",
    "    print(f\"\\nChunk {i+1}:\\n{c}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3aa6bccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Semantic Chunks:\n",
      "\n",
      "Chunk 1:\n",
      "\n",
      "LangChain provides multiple chunking techniques for splitting large documents. Chunking is essential for RAG because embedding models have token limits.\n",
      "\n",
      "Chunk 2:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "emb = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "semantic_splitter = SemanticChunker(embeddings=emb)\n",
    "\n",
    "chunks = semantic_splitter.split_text(text)\n",
    "\n",
    "print(\"\\nSemantic Chunks:\")\n",
    "for i, c in enumerate(chunks):\n",
    "    print(f\"\\nChunk {i+1}:\\n{c}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
