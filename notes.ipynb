{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de42910f",
   "metadata": {},
   "source": [
    "Data ingestion topics:\n",
    "\n",
    "1. Batch vs streaming ingestion\n",
    "2. Source systems (databases, APIs, files, message queues)\n",
    "3. Change Data Capture (CDC)\n",
    "4. Data formats (CSV, JSON, Parquet, Avro, ORC)\n",
    "5. Schema design and schema evolution\n",
    "6. Data validation and quality checks\n",
    "7. Deduplication and noise removal\n",
    "8. Data transformation (ETL vs ELT)\n",
    "9. Orchestration and workflow scheduling\n",
    "10. Error handling and retry policies\n",
    "11. Metadata and lineage tracking\n",
    "12. Security and access control\n",
    "13. Scalability and throughput optimization\n",
    "14. Latency vs consistency trade-offs\n",
    "15. Storage targets (data lake, warehouse, OLTP/OLAP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b178e85",
   "metadata": {},
   "source": [
    "Below is a **complete, structured list of all major concepts involved in Generative AI**, covering **theory, systems, models, data pipelines, MLOps, RAG, optimization, safety, evaluation, deployment, and governance**.\n",
    "\n",
    "This is a **master checklist** of everything you need to understand for Generative AI end-to-end.\n",
    "\n",
    "---\n",
    "\n",
    "# 1. Core AI & ML Foundations\n",
    "\n",
    "* Machine learning basics\n",
    "* Deep learning basics\n",
    "* Neural networks\n",
    "* Backpropagation\n",
    "* Loss functions (cross-entropy, KL divergence)\n",
    "* Activation functions (ReLU, GELU, SiLU)\n",
    "* Optimization (Adam, AdamW, SGD)\n",
    "* Regularization (dropout, weight decay)\n",
    "* Gradient clipping\n",
    "* Learning rate schedulers\n",
    "\n",
    "---\n",
    "\n",
    "# 2. Generative Model Families\n",
    "\n",
    "* Transformers\n",
    "* Encoder–decoder models\n",
    "* Decoder-only LLMs\n",
    "* Diffusion models\n",
    "* GANs\n",
    "* VAE (Variational Autoencoders)\n",
    "* Autoregressive models\n",
    "* Masked language modeling\n",
    "\n",
    "---\n",
    "\n",
    "# 3. Large Language Models (LLMs)\n",
    "\n",
    "* Attention mechanism\n",
    "* Multi-head attention\n",
    "* Self-attention vs cross-attention\n",
    "* Positional encodings\n",
    "* Tokenization (BPE, SentencePiece, WordPiece)\n",
    "* Context window & sliding windows\n",
    "* KV cache\n",
    "* Sampling strategies (top-k, top-p, temperature)\n",
    "* Beam search\n",
    "* Low-rank adaptation (LoRA)\n",
    "* Quantization (8-bit, 4-bit, AWQ, GPTQ)\n",
    "* Instruction tuning\n",
    "* Supervised fine-tuning (SFT)\n",
    "* Preference modeling\n",
    "* Reinforcement learning (RLHF)\n",
    "* DPO (Direct Preference Optimization)\n",
    "* PPO vs DPO differences\n",
    "\n",
    "---\n",
    "\n",
    "# 4. Multimodal Models\n",
    "\n",
    "* Vision Transformers (ViT)\n",
    "* CLIP\n",
    "* Vision–language models (VLM)\n",
    "* Speech-to-text\n",
    "* Text-to-speech\n",
    "* Audio representations (spectrograms)\n",
    "* Image embeddings\n",
    "* Video embeddings\n",
    "* OCR systems\n",
    "\n",
    "---\n",
    "\n",
    "# 5. Embeddings\n",
    "\n",
    "* Text embeddings\n",
    "* Sentence embeddings\n",
    "* Image/audio embeddings\n",
    "* Cosine similarity\n",
    "* Vector normalization\n",
    "* Dimensionality\n",
    "* Embedding model drift\n",
    "* Embedding versioning\n",
    "* Nearest neighbors search (ANN)\n",
    "* FAISS / HNSW / ScaNN\n",
    "\n",
    "---\n",
    "\n",
    "# 6. RAG (Retrieval Augmented Generation)\n",
    "\n",
    "## Retrieval\n",
    "\n",
    "* Chunking strategies (semantic, fixed-size, hybrid)\n",
    "* Overlap strategies\n",
    "* Semantic search\n",
    "* Hybrid search (BM25 + embeddings)\n",
    "* Ranking / re-ranking\n",
    "* Context assembly\n",
    "* Query rewriting\n",
    "* Multi-step retrieval\n",
    "* Routing / query classification\n",
    "\n",
    "## Storage\n",
    "\n",
    "* Vector databases (Pinecone, Qdrant, Weaviate, Chroma)\n",
    "* Metadata stores\n",
    "* Indexes (HNSW, IVF, Flat)\n",
    "* Sharding & replicas\n",
    "* TTL and versioning\n",
    "\n",
    "## Response synthesis\n",
    "\n",
    "* Context windows\n",
    "* Chain-of-thought (CoT)\n",
    "* Tools & function calling\n",
    "* Retrieval fusion\n",
    "* Guardrails\n",
    "\n",
    "---\n",
    "\n",
    "# 7. Data Engineering for GenAI\n",
    "\n",
    "## Data ingestion\n",
    "\n",
    "* Connectors (S3/GCS/SharePoint/API/DB)\n",
    "* Scheduling (Airflow, Dagster)\n",
    "* KubernetesPodOperator\n",
    "* ETL vs ELT\n",
    "* Micro-batching\n",
    "* Streaming (Kafka, Kinesis)\n",
    "\n",
    "## Data preprocessing\n",
    "\n",
    "* Parsing (PDF, HTML, DOCX)\n",
    "* OCR\n",
    "* Cleaning (boilerplate removal, noise removal)\n",
    "* Normalization\n",
    "* Deduplication (exact + near-dup)\n",
    "* Tokenization & chunking\n",
    "* Schema design & evolution\n",
    "* Metadata extraction\n",
    "* Lineage tracking\n",
    "* PII detection\n",
    "* Safety filters\n",
    "\n",
    "## Data storage\n",
    "\n",
    "* Data lakes (S3/GCS/ADLS)\n",
    "* Data warehouses (Snowflake/BigQuery/Redshift)\n",
    "* OLTP databases (Postgres/MySQL/DynamoDB)\n",
    "* OLAP systems (ClickHouse, Druid)\n",
    "\n",
    "## Batch & streaming versioning\n",
    "\n",
    "* Bronze/Silver/Gold medallion architecture\n",
    "* Lakehouse concepts\n",
    "* Delta/Iceberg/Hudi\n",
    "\n",
    "---\n",
    "\n",
    "# 8. Training & Fine-Tuning\n",
    "\n",
    "* Pretraining datasets\n",
    "* Tokenization pipelines\n",
    "* SFT (Supervised Fine-Tuning)\n",
    "* RLHF (Reinforcement Learning from Human Feedback)\n",
    "* Preference modeling\n",
    "* DPO, ORPO\n",
    "* Continual training\n",
    "* Model drift\n",
    "* Curriculum learning\n",
    "* Training infrastructure (GPU/TPU clusters, distributed training)\n",
    "* Checkpointing\n",
    "* Mixed-precision training (FP16/BF16/FP8)\n",
    "* Gradient accumulation\n",
    "* Data packing\n",
    "\n",
    "---\n",
    "\n",
    "# 9. Evaluation\n",
    "\n",
    "* Perplexity\n",
    "* BLEU/ROUGE (older metrics)\n",
    "* Win-rate evaluation\n",
    "* Human evaluation\n",
    "* Model hallucination tests\n",
    "* Groundedness checks\n",
    "* RAG evaluation (Faithfulness, Recall, Relevance)\n",
    "* Bias / toxicity eval\n",
    "* Adversarial prompts testing\n",
    "\n",
    "---\n",
    "\n",
    "# 10. Scaling & Performance\n",
    "\n",
    "* Latency vs consistency trade-offs\n",
    "* Throughput optimization\n",
    "* Batching & micro-batching\n",
    "* Caching (KV cache, response cache)\n",
    "* Parallel inference\n",
    "* Speculative decoding\n",
    "* GPU utilization\n",
    "* Autoscaling\n",
    "* Distributed serving\n",
    "* Cost optimization\n",
    "\n",
    "---\n",
    "\n",
    "# 11. Safety, Security & Governance\n",
    "\n",
    "* Access control & RBAC\n",
    "* ABAC\n",
    "* Secrets management\n",
    "* Private networking / VPC\n",
    "* Encryption (in-flight & at-rest)\n",
    "* PII detection & redaction\n",
    "* Policy filtering\n",
    "* Guardrails\n",
    "* Data quality rules\n",
    "* Audit logs\n",
    "* Compliance (SOC2, HIPAA, GDPR)\n",
    "\n",
    "---\n",
    "\n",
    "# 12. Monitoring & Observability\n",
    "\n",
    "* Prompt logs\n",
    "* Response latency\n",
    "* Embedding drift detection\n",
    "* Pipeline run metrics\n",
    "* Retrieval quality metrics\n",
    "* Model usage analytics\n",
    "* Reliability SLOs\n",
    "\n",
    "---\n",
    "\n",
    "# 13. Deployment\n",
    "\n",
    "* Model serving frameworks (Triton, vLLM, TensorRT)\n",
    "* API gateways\n",
    "* Load balancers\n",
    "* Autoscaling GPU pods\n",
    "* Canary + shadow deployments\n",
    "* CI/CD for LLM pipelines\n",
    "* Prompt router / model router\n",
    "\n",
    "---\n",
    "\n",
    "# 14. Applications & Workflows\n",
    "\n",
    "* RAG chatbots\n",
    "* QA search systems\n",
    "* Document automation\n",
    "* Code generation\n",
    "* Multi-agent workflows\n",
    "* Autonomous task execution\n",
    "* Summarization, translation\n",
    "* Fine-tuned domain assistants\n",
    "\n",
    "---\n",
    "\n",
    "# 15. Supporting Concepts\n",
    "\n",
    "* Knowledge graphs\n",
    "* Tool calling\n",
    "* Agent architectures\n",
    "* APIs, SDKs, LangChain, LlamaIndex\n",
    "* System prompts & prompt engineering\n",
    "* Self-embedding & long-context strategies\n",
    "* Context compression\n",
    "* Memory mechanisms\n",
    "\n",
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "These 15 categories include **all major concepts** you will encounter when working with Generative AI—covering **ML theory, data engineering, RAG, embeddings, training, MLOps, scaling, safety, evaluation, deployment, and governance**.\n",
    "\n",
    "If you want, I can also generate:\n",
    "\n",
    "* A **one-page cheat sheet**\n",
    "* A **roadmap for mastering Generative AI**\n",
    "* A **mind map diagram** of all concepts\n",
    "* A **learning path** based on your background\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4d59143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Checking IAM User ===\n",
      "Error: The security token included in the request is invalid.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "def check_iam_user(username):\n",
    "    iam = boto3.client(\"iam\")\n",
    "\n",
    "    print(\"\\n=== Checking IAM User ===\")\n",
    "\n",
    "    # Check if user exists\n",
    "    try:\n",
    "        user = iam.get_user(UserName=username)\n",
    "        print(\"User Found:\", user[\"User\"][\"UserName\"])\n",
    "        print(\"User ARN:\", user[\"User\"][\"Arn\"])\n",
    "        print(\"Created On:\", user[\"User\"][\"CreateDate\"])\n",
    "    except ClientError as e:\n",
    "        print(\"Error:\", e.response[\"Error\"][\"Message\"])\n",
    "        return\n",
    "\n",
    "    print(\"\\n=== Access Keys ===\")\n",
    "\n",
    "    # List access keys\n",
    "    keys = iam.list_access_keys(UserName=username)[\"AccessKeyMetadata\"]\n",
    "    \n",
    "    if not keys:\n",
    "        print(\"No access keys found.\")\n",
    "    else:\n",
    "        for key in keys:\n",
    "            print(\"\\nAccessKeyId:\", key[\"AccessKeyId\"])\n",
    "            print(\"Status:\", key[\"Status\"])\n",
    "            print(\"Created:\", key[\"CreateDate\"])\n",
    "\n",
    "            # Check last used\n",
    "            try:\n",
    "                last_used = iam.get_access_key_last_used(AccessKeyId=key[\"AccessKeyId\"])\n",
    "                print(\"Last Used:\", last_used[\"AccessKeyLastUsed\"].get(\"LastUsedDate\", \"Never Used\"))\n",
    "                print(\"Service:\", last_used[\"AccessKeyLastUsed\"].get(\"ServiceName\", \"None\"))\n",
    "            except ClientError as e:\n",
    "                print(\"Could not fetch last used:\", e.response[\"Error\"][\"Message\"])\n",
    "    \n",
    "    print(\"\\n=== Login Profile (Console Password) ===\")\n",
    "\n",
    "    # Check password login profile (optional)\n",
    "    try:\n",
    "        profile = iam.get_login_profile(UserName=username)\n",
    "        print(\"Password Last Set On:\", profile[\"LoginProfile\"][\"CreateDate\"])\n",
    "    except ClientError as e:\n",
    "        if e.response[\"Error\"][\"Code\"] == \"NoSuchEntity\":\n",
    "            print(\"User does NOT have an AWS Console password.\")\n",
    "        else:\n",
    "            print(\"Error fetching login profile:\", e.response[\"Error\"][\"Message\"])\n",
    "\n",
    "# Example\n",
    "check_iam_user(\"myUser\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05fd4011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Downloading boto3-1.40.76-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting botocore<1.41.0,>=1.40.76 (from boto3)\n",
      "  Downloading botocore-1.40.76-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.15.0,>=0.14.0 (from boto3)\n",
      "  Downloading s3transfer-0.14.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\sangouda\\python_apps\\lib\\site-packages (from botocore<1.41.0,>=1.40.76->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in c:\\users\\sangouda\\python_apps\\lib\\site-packages (from botocore<1.41.0,>=1.40.76->boto3) (2.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sangouda\\python_apps\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.41.0,>=1.40.76->boto3) (1.17.0)\n",
      "Downloading boto3-1.40.76-py3-none-any.whl (139 kB)\n",
      "Downloading botocore-1.40.76-py3-none-any.whl (14.2 MB)\n",
      "   ---------------------------------------- 0.0/14.2 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/14.2 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/14.2 MB 1.7 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 0.8/14.2 MB 1.7 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 1.3/14.2 MB 1.5 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 1.6/14.2 MB 1.5 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 1.8/14.2 MB 1.5 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 2.4/14.2 MB 1.6 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 2.9/14.2 MB 1.6 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 3.1/14.2 MB 1.6 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 3.4/14.2 MB 1.7 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 3.9/14.2 MB 1.7 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 4.5/14.2 MB 1.7 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 4.7/14.2 MB 1.7 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 5.2/14.2 MB 1.8 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 5.8/14.2 MB 1.8 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 6.3/14.2 MB 1.8 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 6.6/14.2 MB 1.8 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 7.1/14.2 MB 1.8 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 7.6/14.2 MB 1.9 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 7.9/14.2 MB 1.9 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 8.4/14.2 MB 1.9 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 8.9/14.2 MB 1.9 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 9.2/14.2 MB 1.9 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 9.7/14.2 MB 1.9 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 10.0/14.2 MB 1.9 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 10.5/14.2 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 10.7/14.2 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 11.3/14.2 MB 1.9 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 12.1/14.2 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 12.8/14.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 13.4/14.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.2/14.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.2/14.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.2/14.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.2/14.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.2/14.2 MB 1.9 MB/s eta 0:00:00\n",
      "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading s3transfer-0.14.0-py3-none-any.whl (85 kB)\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.40.76 botocore-1.40.76 jmespath-1.0.1 s3transfer-0.14.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
