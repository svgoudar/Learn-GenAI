{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dc79de7",
   "metadata": {},
   "source": [
    "\n",
    "## Nearest Neighbor Search\n",
    "\n",
    "Nearest Neighbor Search is the process of finding **the vectors most similar** to a query vector inside a large collection of embeddings.\n",
    "\n",
    "In Generative AI, embeddings represent the *meaning* of:\n",
    "\n",
    "* text\n",
    "* sentences\n",
    "* documents\n",
    "* images\n",
    "* audio\n",
    "\n",
    "NNS retrieves the items whose embeddings lie **closest in vector space** to the query.\n",
    "\n",
    "---\n",
    "\n",
    "### Why NNS Is Critical in Generative AI\n",
    "\n",
    "#### LLMs cannot â€œrememberâ€ or â€œsearchâ€ billions of documents.\n",
    "\n",
    "So we store document embeddings in a **vector database** and use NNS to retrieve the most relevant chunks.\n",
    "\n",
    "RAG pipeline:\n",
    "\n",
    "```\n",
    "User query â†’ embed â†’ NNS â†’ retrieve relevant docs â†’ feed to LLM\n",
    "```\n",
    "\n",
    "Quality of NNS directly affects:\n",
    "\n",
    "* factual correctness\n",
    "* hallucination reduction\n",
    "* search accuracy\n",
    "* chatbot reliability\n",
    "\n",
    "---\n",
    "\n",
    "### How Nearest Neighbor Search Works\n",
    "\n",
    "Given:\n",
    "\n",
    "* a **query embedding** q\n",
    "* a list of **indexed embeddings** D = {dâ‚, dâ‚‚, â€¦ dâ‚™}\n",
    "\n",
    "NNS finds the top-k closest embeddings based on similarity.\n",
    "\n",
    "Most common metric:\n",
    "\n",
    "* **Cosine similarity (1 - angular distance)**\n",
    "  Others:\n",
    "* Euclidean distance\n",
    "* Dot-product similarity\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "Query: \"What is GDPR?\"\n",
    "Top neighbors:\n",
    "1. Chunk about EU data protection laws\n",
    "2. Chunk about privacy regulations\n",
    "3. Chunk about compliance rules\n",
    "```\n",
    "\n",
    "This is **semantic search**, not keyword search.\n",
    "\n",
    "---\n",
    "\n",
    "### Exact Search vs Approximate Search\n",
    "\n",
    "#### **Exact Nearest Neighbor Search (k-NN)**\n",
    "\n",
    "* Brute force comparison: compare query with all vectors\n",
    "* O(N Ã— D) time\n",
    "* Too slow for millions of vectors\n",
    "\n",
    "Works only for small datasets.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Approximate Nearest Neighbor Search (ANN)**\n",
    "\n",
    "Used in all production RAG systems.\n",
    "\n",
    "Goal:\n",
    "\n",
    "> Return neighbors that are *almost* the best, but **much faster**.\n",
    "\n",
    "Benefits:\n",
    "\n",
    "* 10Ã— to 1000Ã— faster\n",
    "* Works for millions/billions of embeddings\n",
    "* Small accuracy tradeoff (<1% loss)\n",
    "\n",
    "ANN frameworks:\n",
    "\n",
    "* **FAISS (Meta)**\n",
    "* **HNSW** (Hierarchical Navigable Small Worlds)\n",
    "* **ScaNN (Google)**\n",
    "* **Pinecone / Qdrant / Weaviate** (vector DBs that wrap ANN algorithms)\n",
    "\n",
    "---\n",
    "\n",
    "### Why ANN Is Needed (The Curse of Dimensionality)\n",
    "\n",
    "Embeddings from LLMs are high-dimensional:\n",
    "\n",
    "* 384\n",
    "* 768\n",
    "* 1536\n",
    "* 4096\n",
    "\n",
    "High-dimensional spaces break traditional search algorithms.\n",
    "\n",
    "ANN algorithms are specifically designed to operate efficiently in **high-dimensional** embedding spaces.\n",
    "\n",
    "---\n",
    "\n",
    "### How ANN Algorithms Work (Intuition)\n",
    "\n",
    "#### ðŸ”¹ **1. FAISS (IVF / PQ / Flat)**\n",
    "\n",
    "* Clusters embeddings\n",
    "* Assigns vectors to closest centroids\n",
    "* Searches only relevant clusters\n",
    "* Optional compression reduces RAM\n",
    "\n",
    "#### ðŸ”¹ **2. HNSW (Graph Search)**\n",
    "\n",
    "* Builds a graph where each vector points to its neighbors\n",
    "* Query = navigate through the graph\n",
    "* Extremely fast and accurate\n",
    "* Used by Qdrant, Weaviate\n",
    "\n",
    "#### ðŸ”¹ **3. ScaNN**\n",
    "\n",
    "* Google's optimized ANN system\n",
    "* Focus on TPU/CPU performance\n",
    "* Used internally by YouTube, Search\n",
    "\n",
    "---\n",
    "\n",
    "### Nearest Neighbor Search in RAG\n",
    "\n",
    "Hereâ€™s how NNS fits into Retrieval-Augmented Generation:\n",
    "\n",
    "#### Step 1: Embed query\n",
    "\n",
    "```\n",
    "q = embed(\"How does self-attention work?\")\n",
    "```\n",
    "\n",
    "#### Step 2: Query vector DB using ANN\n",
    "\n",
    "```\n",
    "results = vector_db.search(q, top_k=5)\n",
    "```\n",
    "\n",
    "#### Step 3: Retrieve best matching chunks\n",
    "\n",
    "Results returned based on **cosine similarity**, not keyword match.\n",
    "\n",
    "#### Step 4: Feed retrieved chunks to the LLM\n",
    "\n",
    "```\n",
    "context = concatenate(results)\n",
    "answer = llm(context + user_query)\n",
    "```\n",
    "\n",
    "Without good nearest-neighbor search:\n",
    "\n",
    "* wrong documents get retrieved\n",
    "* hallucination increases\n",
    "* answers become irrelevant\n",
    "\n",
    "---\n",
    "\n",
    "### Example (Simple Python Demonstration)\n",
    "\n",
    "Using **FAISS**:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# Example document vectors\n",
    "docs = np.random.rand(1000, 128).astype(\"float32\")\n",
    "\n",
    "# Build FAISS index\n",
    "index = faiss.IndexFlatL2(128)\n",
    "index.add(docs)\n",
    "\n",
    "# Query vector\n",
    "query = np.random.rand(1, 128).astype(\"float32\")\n",
    "\n",
    "# Find nearest neighbors\n",
    "scores, ids = index.search(query, k=3)\n",
    "\n",
    "print(\"Nearest neighbors:\", ids)\n",
    "print(\"Distances:\", scores)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**\n",
    "\n",
    "| Concept                  | Explanation                                  |\n",
    "| ------------------------ | -------------------------------------------- |\n",
    "| **NNS**                  | Find the closest vectors to a query          |\n",
    "| **Why important**        | Powers RAG, semantic search, VLM retrieval   |\n",
    "| **Metric**               | Mostly cosine similarity                     |\n",
    "| **Exact NNS**            | Slow, only for tiny datasets                 |\n",
    "| **ANN**                  | Fast, scalable, near-perfect accuracy        |\n",
    "| **FAISS / HNSW / ScaNN** | Industry-standard ANN algorithms             |\n",
    "| **In RAG**               | Determines which chunks are shown to the LLM |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
