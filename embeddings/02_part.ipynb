{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32da26aa",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## Cosine Similarity\n",
    "\n",
    "\n",
    "Cosine similarity measures how **similar two embeddings (vectors)** are based on the **angle** between them.\n",
    "\n",
    "Formula:\n",
    "\n",
    "$$\n",
    "\\text{cosine\\_similarity}(a, b)=\\frac{a \\cdot b}{|a| |b|}\n",
    "$$\n",
    "\n",
    "It doesn’t compare magnitudes—only direction.\n",
    "\n",
    "Values:\n",
    "\n",
    "* **+1** → identical meaning\n",
    "* **0** → unrelated\n",
    "* **-1** → opposite\n",
    "\n",
    "---\n",
    "\n",
    "### Why Cosine Similarity Is Essential in Gen AI\n",
    "\n",
    "Generative AI uses embeddings to represent **meaning** of:\n",
    "\n",
    "* text\n",
    "* sentences\n",
    "* images\n",
    "* documents\n",
    "* audio\n",
    "\n",
    "Cosine similarity is the standard method to compare these embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. In Retrieval-Augmented Generation (RAG)**\n",
    "\n",
    "When a user asks:\n",
    "\n",
    "```\n",
    "\"What are transformer models?\"\n",
    "```\n",
    "\n",
    "We convert the query to an embedding vector, then compute cosine similarity with stored document embeddings.\n",
    "\n",
    "Higher similarity → more relevant passage.\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "query_embedding     = [0.2, 0.5, -0.1]\n",
    "doc1_embedding      = [0.21, 0.49, -0.12]   → sim = high\n",
    "doc2_embedding      = [-0.6, 0.2, 0.8]      → sim = low\n",
    "```\n",
    "\n",
    "LLM retrieves doc1.\n",
    "\n",
    "Thus cosine similarity **drives RAG quality**.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Semantic Search (Embedding Search)**\n",
    "\n",
    "Cosine similarity is used to perform:\n",
    "\n",
    "* semantic search\n",
    "* FAQ matching\n",
    "* intent detection\n",
    "* recommendation systems\n",
    "\n",
    "It measures the *meaning* match instead of literal keywords.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. CLIP (Image–Text Matching)**\n",
    "\n",
    "CLIP produces:\n",
    "\n",
    "* image embedding\n",
    "* text embedding\n",
    "\n",
    "Cosine similarity determines which caption matches the image:\n",
    "\n",
    "```\n",
    "closest text embedding = best caption\n",
    "```\n",
    "\n",
    "This powers:\n",
    "\n",
    "* image search\n",
    "* captioning\n",
    "* VLMs like LLaVA, BLIP, Q-Former models\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Conversation Memory and LLM Context**\n",
    "\n",
    "Memory systems store old messages as embeddings.\n",
    "\n",
    "To find relevant memory:\n",
    "\n",
    "```\n",
    "cosine similarity(query, memory_vector)\n",
    "```\n",
    "\n",
    "This allows:\n",
    "\n",
    "* long-term memory\n",
    "* contextual personalization\n",
    "* conversation grounding\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Document Chunk Selection**\n",
    "\n",
    "In retrieval systems, each chunk of text is embedded.\n",
    "\n",
    "Cosine similarity ranks them.\n",
    "\n",
    "Better similarity → better chunk → fewer hallucinations.\n",
    "\n",
    "---\n",
    "\n",
    "### Why not Euclidean distance?\n",
    "\n",
    "Because embeddings may vary in magnitude due to:\n",
    "\n",
    "* model initialization\n",
    "* token frequency\n",
    "* internal scaling\n",
    "\n",
    "Cosine similarity is **scale-invariant**, so it captures meaning better.\n",
    "\n",
    "---\n",
    "\n",
    "### Example (code-level intuition)\n",
    "\n",
    "Vectors:\n",
    "\n",
    "```\n",
    "v1 = [2, 2]\n",
    "v2 = [10, 10]\n",
    "```\n",
    "\n",
    "Cosine similarity = **1**\n",
    "→ Same meaning, even though magnitudes differ.\n",
    "\n",
    "Euclidean distance would incorrectly say they are far apart.\n",
    "\n",
    "---\n",
    "\n",
    "**Intuition Summary**\n",
    "\n",
    "Cosine similarity answers:\n",
    "\n",
    "> “Are these two embeddings pointing in the same semantic direction?”\n",
    "\n",
    "If yes → high similarity\n",
    "If no → low similarity\n",
    "\n",
    "This makes it indispensable for:\n",
    "\n",
    "* RAG\n",
    "* search\n",
    "* VLMs\n",
    "* speech models\n",
    "* chat memory\n",
    "* classifier systems\n",
    "\n",
    "### **1. Cosine Similarity Using NumPy**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def cosine_similarity_numpy(a, b):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "\n",
    "    dot = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "\n",
    "    return dot / (norm_a * norm_b)\n",
    "\n",
    "v1 = [0.2, 0.5, -0.1]\n",
    "v2 = [0.21, 0.49, -0.12]\n",
    "\n",
    "print(\"Cosine similarity (NumPy):\", cosine_similarity_numpy(v1, v2))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Cosine Similarity Using PyTorch**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "v1 = torch.tensor([0.2, 0.5, -0.1])\n",
    "v2 = torch.tensor([0.21, 0.49, -0.12])\n",
    "\n",
    "similarity = F.cosine_similarity(v1, v2, dim=0)\n",
    "print(\"Cosine similarity (PyTorch):\", similarity.item())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Cosine Similarity Using sklearn**\n",
    "\n",
    "```python\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "v1 = np.array([[0.2, 0.5, -0.1]])\n",
    "v2 = np.array([[0.21, 0.49, -0.12]])\n",
    "\n",
    "print(\"Cosine similarity (sklearn):\", cosine_similarity(v1, v2)[0][0])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "###  **4. Comparing Multiple Embeddings (RAG-like Example)**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "query = np.array([[0.1, 0.9, -0.2]])\n",
    "\n",
    "docs = np.array([\n",
    "    [0.11, 0.88, -0.19],   # relevant\n",
    "    [-0.5, 0.2, 0.7],      # irrelevant\n",
    "    [0.13, 0.91, -0.18]    # relevant\n",
    "])\n",
    "\n",
    "scores = cosine_similarity(query, docs)[0]\n",
    "\n",
    "print(\"Cosine scores:\", scores)\n",
    "print(\"Most relevant document index:\", np.argmax(scores))\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
