{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b63da53",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## Embeddings\n",
    "\n",
    "### Text Embeddings (Token-Level Embeddings)\n",
    "\n",
    "#### **What they represent**\n",
    "\n",
    "**Text embeddings represent individual tokens (words, subwords, characters) as vectors.**\n",
    "\n",
    "These embeddings come from an **embedding matrix** inside every LLM.\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "\"apple\" → [0.12, -0.21, 0.59, ...]    (4096-dimensional vector)\n",
    "\"apples\" → [0.11, -0.20, 0.58, ...]\n",
    "```\n",
    "\n",
    "Even punctuation has embeddings:\n",
    "\n",
    "```\n",
    "\",\" → [0.03, 0.8, ...]\n",
    "```\n",
    "\n",
    "Tokens typically come from:\n",
    "\n",
    "* BPE (Byte-Pair Encoding)\n",
    "* WordPiece\n",
    "* SentencePiece\n",
    "\n",
    "Used inside **GPT, LLaMA, Mistral, T5**, etc.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Purpose**\n",
    "\n",
    "Text embeddings allow models to:\n",
    "\n",
    "* encode **semantics**\n",
    "* encode **syntax**\n",
    "* encode **word relationships**\n",
    "* reason over token sequences\n",
    "\n",
    "---\n",
    "\n",
    "#### Example (GPT internal text embeddings)\n",
    "\n",
    "Before any transformer layer:\n",
    "\n",
    "```\n",
    "token_id → embedding vector\n",
    "embedding + position_embed → transformer → prediction\n",
    "```\n",
    "\n",
    "These embeddings are **context-free** (only represent token itself).\n",
    "Context is added only after multiple transformer layers.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary of Text Embeddings**\n",
    "\n",
    "* represent *tokens*\n",
    "* learned during LLM training\n",
    "* used internally by GPT/LLMs\n",
    "* foundation for contextual understanding\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Sentence Embeddings (Semantic Embeddings)**\n",
    "\n",
    "#### **What they represent**\n",
    "\n",
    "**Sentence embeddings represent the meaning of an entire sentence, paragraph, or document.**\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "“Apple is releasing a new phone.”\n",
    "```\n",
    "\n",
    "→ vector representing the whole meaning\n",
    "\n",
    "Sentence embeddings are NOT token embeddings.\n",
    "They come from specially trained models, such as:\n",
    "\n",
    "* Sentence-BERT\n",
    "* OpenAI `text-embedding-3-small`\n",
    "* E5\n",
    "* Instructor XL\n",
    "* Jina Embeddings\n",
    "\n",
    "---\n",
    "\n",
    "#### **Purpose**\n",
    "\n",
    "Sentence embeddings enable:\n",
    "\n",
    "* search\n",
    "* clustering\n",
    "* recommendations\n",
    "* Retrieval-Augmented Generation (RAG)\n",
    "* semantic similarity\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "\"What is AI?\"  ↔  \"Explain artificial intelligence\"\n",
    "```\n",
    "\n",
    "These sentences are different, but their **embeddings are close**.\n",
    "\n",
    "---\n",
    "\n",
    "#### How they are trained\n",
    "\n",
    "Most sentence embedding models use **contrastive learning**:\n",
    "\n",
    "* positive pair → pull together\n",
    "* negative pair → push apart\n",
    "\n",
    "Example pairs:\n",
    "\n",
    "```\n",
    "(\"What is AI?\", \"Definition of artificial intelligence\") → similar\n",
    "(\"Cat\", \"Airplane\") → dissimilar\n",
    "```\n",
    "\n",
    "This creates an embedding **optimized for meaning**, not next-token prediction.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary of Sentence Embeddings**\n",
    "\n",
    "* represent entire **sentences or documents**\n",
    "* optimized for **semantic similarity**\n",
    "* used in **RAG**, search, clustering, retrieval\n",
    "* trained via **contrastive learning**\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Image and Audio Embeddings (Multimodal Embeddings)**\n",
    "\n",
    "These convert non-text data (image/audio) into embedding vectors so models can understand them.\n",
    "\n",
    "---\n",
    "\n",
    "#### **A. Image Embeddings**\n",
    "\n",
    "##### **What they represent**\n",
    "\n",
    "Image embeddings represent **objects, scenes, textures, colors, and concepts** inside an image.\n",
    "\n",
    "Generated by vision models:\n",
    "\n",
    "* CLIP ViT\n",
    "* Vision Transformer (ViT)\n",
    "* ConvNeXt\n",
    "* EfficientNet\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "dog.jpg → [0.11, 1.02, -0.55, ...]   (512-dimensional vector)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "##### How image embeddings work\n",
    "\n",
    "Pipeline:\n",
    "\n",
    "1. Image is resized + normalized\n",
    "2. Split into patches (ViT)\n",
    "3. Pass through Transformer\n",
    "4. Output CLS embedding (image-level vector)\n",
    "\n",
    "This embedding captures:\n",
    "\n",
    "* object categories\n",
    "* scene context\n",
    "* visual semantics\n",
    "\n",
    "Used in:\n",
    "\n",
    "* visual search\n",
    "* CLIP similarity\n",
    "* VLMs like LLaVA, BLIP-2\n",
    "* generating captions\n",
    "* grounding vision in LLMs\n",
    "\n",
    "---\n",
    "\n",
    "### **B. Audio Embeddings**\n",
    "\n",
    "#### **What they represent**\n",
    "\n",
    "Audio embeddings represent:\n",
    "\n",
    "* speech content\n",
    "* speaker identity\n",
    "* tone\n",
    "* pitch\n",
    "* emotion\n",
    "* noise patterns\n",
    "\n",
    "Generated by models like:\n",
    "\n",
    "* Whisper\n",
    "* Wav2Vec2\n",
    "* HuBERT\n",
    "* AudioMAE\n",
    "\n",
    "---\n",
    "\n",
    "#### How audio embeddings work\n",
    "\n",
    "Pipeline:\n",
    "\n",
    "1. Audio waveform → Mel Spectrogram\n",
    "2. Spectrogram → encoder (CNN/Transformer)\n",
    "3. Output = audio embedding vector\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "voice.wav → [0.33, -0.12, 0.98, ...]\n",
    "```\n",
    "\n",
    "These embeddings can be used for:\n",
    "\n",
    "* speech recognition\n",
    "* speaker verification\n",
    "* audio classification\n",
    "* sound event detection\n",
    "\n",
    "---\n",
    "\n",
    "### **Why These Embeddings Matter in Generative AI**\n",
    "\n",
    "#### Text embeddings\n",
    "\n",
    "→ foundation of LLM understanding\n",
    "\n",
    "#### Sentence embeddings\n",
    "\n",
    "→ backbone of RAG and enterprise search\n",
    "\n",
    "#### Image/audio embeddings\n",
    "\n",
    "→ enable multimodal LLMs:\n",
    "\n",
    "* GPT-4o\n",
    "* Gemini\n",
    "* LLaVA\n",
    "* CLIP-based VLMs\n",
    "\n",
    "These embeddings allow models to handle:\n",
    "\n",
    "* image→text\n",
    "* text→image\n",
    "* speech→text\n",
    "* video understanding\n",
    "* multimodal reasoning\n",
    "\n",
    "---\n",
    "\n",
    "**FINAL CHEAT SHEET**\n",
    "\n",
    "| Embedding Type          | Represents            | Used For                | Examples                     |\n",
    "| ----------------------- | --------------------- | ----------------------- | ---------------------------- |\n",
    "| **Text Embeddings**     | Tokens                | LLM internal processing | GPT, LLaMA                   |\n",
    "| **Sentence Embeddings** | Meaning of whole text | RAG, search, clustering | SBERT, E5, OpenAI embeddings |\n",
    "| **Image Embeddings**    | Visual meaning        | VLMs, classification    | CLIP ViT, ViT                |\n",
    "| **Audio Embeddings**    | Speech features       | STT, speaker ID         | Whisper, Wav2Vec2            |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
