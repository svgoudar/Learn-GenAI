{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35e9b803",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "##  Why These Algorithms Matter in Generative AI\n",
    "\n",
    "LLMs turn text/images/audio into **embeddings** (vectors).\n",
    "To retrieve relevant information during RAG, we must find the **closest vectors** to a query vector **fast**, even among millions/billions of vectors.\n",
    "\n",
    "Exact search is too slow → **ANN (Approximate Nearest Neighbor)** is used.\n",
    "The three most widely used ANN engines are:\n",
    "\n",
    "* **FAISS** (Meta / Facebook AI)\n",
    "* **HNSW** (Hierarchical Navigable Small Worlds)\n",
    "* **ScaNN** (Google)\n",
    "\n",
    "Each has different strengths and trade-offs.\n",
    "\n",
    "---\n",
    "\n",
    "###  **FAISS** (Facebook AI Similarity Search)\n",
    "\n",
    "#### What it is:\n",
    "\n",
    "FAISS is a highly optimized library from Meta for **fast similarity search** and **clustering** in large vector datasets.\n",
    "\n",
    "#### Why it is popular:\n",
    "\n",
    "* Very fast on **GPU**\n",
    "* Very fast on CPU\n",
    "* Handles **billions of vectors**\n",
    "* Highly tunable indexing mechanisms\n",
    "* Open-source and widely used\n",
    "\n",
    "#### Core Algorithms in FAISS:\n",
    "\n",
    "FAISS contains multiple index types:\n",
    "\n",
    "##### **1. Flat Index (exact search)**\n",
    "\n",
    "```\n",
    "IndexFlatL2 / IndexFlatIP\n",
    "```\n",
    "\n",
    "* brute force\n",
    "* 100% accurate\n",
    "* fast only for small datasets\n",
    "\n",
    "##### **2. IVF (Inverted File Index)**\n",
    "\n",
    "```\n",
    "IndexIVFFlat\n",
    "```\n",
    "\n",
    "* clusters vectors using k-means\n",
    "* search only in few clusters\n",
    "* 10×–50× faster than brute force\n",
    "* small accuracy loss\n",
    "\n",
    "##### **3. PQ (Product Quantization)**\n",
    "\n",
    "```\n",
    "IndexIVFPQ\n",
    "```\n",
    "\n",
    "* compresses embeddings\n",
    "* reduces RAM by 4×–16×\n",
    "* used for billion-scale datasets\n",
    "\n",
    "##### **4. HNSW + FAISS**\n",
    "\n",
    "FAISS can also use HNSW internally.\n",
    "\n",
    "---\n",
    "\n",
    "### When FAISS is best:\n",
    "\n",
    "* GPU-accelerated search\n",
    "* Large datasets (millions–billions of embeddings)\n",
    "* High throughput\n",
    "* Cloud + on-premise systems\n",
    "\n",
    "---\n",
    "\n",
    "### **HNSW** (Hierarchical Navigable Small Worlds)\n",
    "\n",
    "#### What it is:\n",
    "\n",
    "HNSW is a **graph-based ANN algorithm** where each vector is a node in a multi-layer graph.\n",
    "\n",
    "#### How it works (intuition):\n",
    "\n",
    "* Create a **multi-layer graph**\n",
    "* Top layers: few nodes, long-range links\n",
    "* Lower layers: dense graph, short-range links\n",
    "* Search moves from top → bottom\n",
    "* “Navigates” quickly toward closest neighbors\n",
    "\n",
    "#### Why it is fast:\n",
    "\n",
    "* logarithmic-like search time\n",
    "* extremely low recall loss\n",
    "* does not require GPUs\n",
    "* high accuracy even with large datasets\n",
    "\n",
    "#### Where it's used:\n",
    "\n",
    "* **Qdrant**\n",
    "* **Weaviate**\n",
    "* **Milvus**\n",
    "* **Pinecone** (HNSW index option)\n",
    "\n",
    "#### Strengths:\n",
    "\n",
    "* Super accurate\n",
    "* Super fast on CPU\n",
    "* Great for mid-size datasets (up to hundreds of millions)\n",
    "* Works well for real-time updates\n",
    "\n",
    "#### Weakness:\n",
    "\n",
    "* Index building is expensive\n",
    "* Memory-heavy (stores graphs = many pointers)\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **ScaNN** (Google’s Scalable Nearest Neighbor)\n",
    "\n",
    "#### What it is:\n",
    "\n",
    "ScaNN (“Scalable Nearest Neighbors”) is Google’s ANN engine optimized for:\n",
    "\n",
    "* TPU / CPU\n",
    "* high recall\n",
    "* high efficiency\n",
    "* production search systems (YouTube, Google Search)\n",
    "\n",
    "#### How it works:\n",
    "\n",
    "ScaNN uses:\n",
    "\n",
    "1. **Tree-based partitioning**\n",
    "2. **Asymmetric hashing**\n",
    "3. **Re-ranking** with distance-aware quantization\n",
    "\n",
    "#### Key strengths:\n",
    "\n",
    "* Extremely high recall\n",
    "* Very fast on CPU\n",
    "* Efficient memory usage\n",
    "* Optimized for semantic search (text embeddings)\n",
    "* Great for multi-billion–scale embeddings\n",
    "\n",
    "#### Weakness:\n",
    "\n",
    "* Less friendly to frequent index updates\n",
    "* Harder to configure compared to FAISS/HNSW\n",
    "* Python integration is weaker than FAISS\n",
    "\n",
    "---\n",
    "\n",
    "**Comparison Summary**\n",
    "\n",
    "| Feature           | FAISS                     | HNSW                | ScaNN                     |\n",
    "| ----------------- | ------------------------- | ------------------- | ------------------------- |\n",
    "| Developed by      | Meta                      | Yury Malkov         | Google                    |\n",
    "| Best Hardware     | GPU + CPU                 | CPU                 | CPU/TPU                   |\n",
    "| Type              | Clustering + Quantization | Graph-based         | Tree + Compression        |\n",
    "| Speed             | Very high                 | Extremely high      | Highest (CPU)             |\n",
    "| Accuracy          | High                      | Very high           | Very high                 |\n",
    "| Memory usage      | Medium / Low (PQ)         | High (graphs)       | Medium                    |\n",
    "| Real-time updates | Moderate                  | Excellent           | Weak                      |\n",
    "| Best for          | Billions of vectors       | Fast queries on CPU | Production search systems |\n",
    "\n",
    "---\n",
    "\n",
    "### Which One Should You Use?\n",
    "\n",
    "#### **Use FAISS if:**\n",
    "\n",
    "* you need GPU acceleration\n",
    "* your dataset is extremely large\n",
    "* you want the best performance per dollar\n",
    "* you need heavy index compression\n",
    "\n",
    "#### **Use HNSW if:**\n",
    "\n",
    "* you're using a vector database (Qdrant, Weaviate, Milvus)\n",
    "* you want high accuracy with low latency\n",
    "* your dataset fits in RAM\n",
    "* you need frequent inserts / deletes\n",
    "\n",
    "#### **Use ScaNN if:**\n",
    "\n",
    "* you're running on CPU or TPU\n",
    "* you want the highest recall\n",
    "* you're building a semantic search engine\n",
    "* you have billions of embeddings (Google-scale workloads)\n",
    "\n",
    "---\n",
    "\n",
    "**Complete Intuition Summary**\n",
    "\n",
    "| Algorithm | Intuition                                             |\n",
    "| --------- | ----------------------------------------------------- |\n",
    "| **FAISS** | Cluster → search few clusters instead of all vectors  |\n",
    "| **HNSW**  | Build a multi-layer graph → “walk” toward the nearest |\n",
    "| **ScaNN** | Partition → compress → refine                         |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
