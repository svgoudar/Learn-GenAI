{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bde981b",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Flow-Based Generative Models\n",
    "\n",
    "---\n",
    "\n",
    "### Core Idea\n",
    "\n",
    "Flow-based models are **generative models** that learn to map complex data (like images) into a **simple, tractable distribution** (like a Gaussian) — and vice versa — using a **reversible (invertible) transformation**.\n",
    "\n",
    "In simple terms:\n",
    "\n",
    "> They **learn an exact mathematical mapping** between real data and random noise — allowing both **generation** and **density estimation** directly.\n",
    "\n",
    "So, unlike GANs (which are implicit) or diffusion models (which are iterative), **flow-based models** are **explicit and invertible**.\n",
    "\n",
    "---\n",
    "\n",
    "### Intuition\n",
    "\n",
    "Imagine you have:\n",
    "\n",
    "* A **complex distribution** $p_X(x)$: real images, audio, etc.\n",
    "* A **simple distribution** $p_Z(z)$: standard normal $\\mathcal{N}(0, I)$.\n",
    "\n",
    "Flow models learn a **bijective function** $f_\\theta$ such that:\n",
    "\n",
    "$$\n",
    "z = f_\\theta(x) \\quad \\text{and} \\quad x = f_\\theta^{-1}(z)\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "* **Forward pass:** maps data → latent noise\n",
    "* **Inverse pass:** maps noise → realistic data\n",
    "\n",
    "This **invertibility** makes flows unique.\n",
    "\n",
    "---\n",
    "\n",
    "### Probability Transformation (Change of Variables)\n",
    "\n",
    "The probability density of $x$ is derived from $z = f_\\theta(x)$:\n",
    "\n",
    "$$\n",
    "p_X(x) = p_Z(f_\\theta(x)) \\cdot \\left| \\det \\frac{\\partial f_\\theta(x)}{\\partial x} \\right|\n",
    "$$\n",
    "\n",
    "Taking log:\n",
    "\n",
    "$$\n",
    "\\log p_X(x) = \\log p_Z(f_\\theta(x)) + \\log \\left| \\det J_{f_\\theta}(x) \\right|\n",
    "$$\n",
    "\n",
    "where $J_{f_\\theta}(x)$ is the **Jacobian matrix** of the transformation.\n",
    "\n",
    "Thus, **training** means maximizing the likelihood of real data under this model — i.e.,\n",
    "$$\n",
    "\\max_\\theta \\sum_x \\log p_X(x)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Key Requirements\n",
    "\n",
    "For a transformation $f_\\theta$ to be valid in flow-based models, it must be:\n",
    "\n",
    "1. **Invertible** (can compute both forward and inverse mapping)\n",
    "2. **Differentiable** (for gradient-based learning)\n",
    "3. **Jacobian determinant computable efficiently**\n",
    "\n",
    "---\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "Flow-based models are composed of **a sequence of invertible transformations** (flows):\n",
    "\n",
    "$$\n",
    "z = f_K \\circ f_{K-1} \\circ \\dots \\circ f_1(x)\n",
    "$$\n",
    "\n",
    "Each transformation layer simplifies the data progressively.\n",
    "\n",
    "At the end, you get a latent variable $z$ following a Gaussian distribution.\n",
    "\n",
    "---\n",
    "\n",
    "### Common Building Blocks\n",
    "\n",
    "| **Component**                  | **Purpose**                                              |\n",
    "| ------------------------------ | -------------------------------------------------------- |\n",
    "| **Affine coupling layer**      | Enables invertibility and efficient Jacobian computation |\n",
    "| **ActNorm**                    | Normalization for stable training                        |\n",
    "| **1×1 Invertible Convolution** | Improves expressiveness by permuting channels            |\n",
    "| **Squeeze Layer**              | Reshapes spatial dimensions for hierarchical processing  |\n",
    "\n",
    "---\n",
    "\n",
    "### Training and Sampling\n",
    "\n",
    "#### Training (Maximum Likelihood Estimation)\n",
    "\n",
    "* Compute $z = f_\\theta(x)$\n",
    "* Compute log-likelihood:\n",
    "  $$\n",
    "  \\log p_X(x) = \\log p_Z(z) + \\log \\left| \\det J_{f_\\theta}(x) \\right|\n",
    "  $$\n",
    "* Maximize this with respect to $\\theta$\n",
    "\n",
    "#### Sampling\n",
    "\n",
    "* Sample $z \\sim \\mathcal{N}(0, I)$\n",
    "* Generate $x = f_\\theta^{-1}(z)$\n",
    "\n",
    "This makes generation **fast and exact**.\n",
    "\n",
    "---\n",
    "\n",
    "### Example Models\n",
    "\n",
    "| **Model**                                               | **Year** | **Key Idea**                                            |\n",
    "| ------------------------------------------------------- | -------- | ------------------------------------------------------- |\n",
    "| **NICE** (Non-linear Independent Components Estimation) | 2014     | Additive coupling layers, invertible mapping            |\n",
    "| **RealNVP** (Real-valued Non-Volume Preserving)         | 2016     | Affine coupling layers for better flexibility           |\n",
    "| **Glow** (Generative Flow)                              | 2018     | 1×1 invertible convolutions for improved expressiveness |\n",
    "\n",
    "---\n",
    "\n",
    "### Workflow Summary\n",
    "\n",
    "| **Step** | **Operation**                                               | **Description**                                   |\n",
    "| -------- | ----------------------------------------------------------- | ------------------------------------------------- |\n",
    "| 1        | Input $x$                                                 | Real data (e.g., image)                           |\n",
    "| 2        | Apply sequence of invertible functions $f_1, f_2, …, f_K$ | Forward transformation (data → latent)            |\n",
    "| 3        | Compute log-likelihood                                      | Use change-of-variable formula                    |\n",
    "| 4        | Optimize parameters $\\theta$                              | Maximize log-likelihood                           |\n",
    "| 5        | For generation, sample $z \\sim \\mathcal{N}(0, I)$         | Reverse transformation $f^{-1}(z)$ to get $x$ |\n",
    "\n",
    "---\n",
    "\n",
    "###  Intuitive Comparison\n",
    "\n",
    "| **Model**       | **Key Idea**                    | **Pros**                        | **Cons**                         |\n",
    "| --------------- | ------------------------------- | ------------------------------- | -------------------------------- |\n",
    "| **GANs**        | Learn to fool discriminator     | Sharp images, fast inference    | No explicit likelihood, unstable |\n",
    "| **VAEs**        | Encode–decode with latent space | Probabilistic, efficient        | Blurry samples                   |\n",
    "| **Diffusion**   | Learn to denoise noise          | High quality, flexible          | Slow sampling                    |\n",
    "| **Flow Models** | Exact invertible mapping        | Exact likelihood, fast sampling | High memory, training complexity |\n",
    "\n",
    "---\n",
    "\n",
    "###  Applications\n",
    "\n",
    "| **Domain**             | **Use Case**                                        |\n",
    "| ---------------------- | --------------------------------------------------- |\n",
    "| **Image generation**   | Generate realistic images (Glow, RealNVP)           |\n",
    "| **Density estimation** | Compute exact data likelihoods                      |\n",
    "| **Super-resolution**   | Generate high-resolution versions of low-res images |\n",
    "| **Audio synthesis**    | Models like WaveGlow for speech                     |\n",
    "| **Anomaly detection**  | Detect out-of-distribution samples by likelihood    |\n",
    "\n",
    "---\n",
    "\n",
    "### Visual Summary\n",
    "\n",
    "```\n",
    "Forward (Training):         Inverse (Sampling):\n",
    "\n",
    "  Data x ─► $$Flows f1→f2→f3→...$$ ─► Latent z      z ~ N(0, I)\n",
    "                                               └──► x = f⁻¹(z)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| **Concept**        | **Formula / Description**                    |                      |   |\n",
    "| ------------------ | -------------------------------------------- | -------------------- | - |\n",
    "| Mapping            | $z = f_\\theta(x)$, invertible              |                      |   |\n",
    "| Log-likelihood     | $\\log p_X(x) = \\log p_Z(f_\\theta(x)) + \\log | \\det J_{f_\\theta}(x) |$ |\n",
    "| Training objective | Maximize likelihood of data                  |                      |   |\n",
    "| Generation         | $x = f_\\theta^{-1}(z)$                     |                      |   |\n",
    "| Core property      | Exact invertibility and tractable density    |                      |   |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
