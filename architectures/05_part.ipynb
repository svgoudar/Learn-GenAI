{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7203105e",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "## Transformer Architecture\n",
    "\n",
    "---\n",
    "\n",
    "### Overview\n",
    "\n",
    "The **Transformer** is a deep learning architecture introduced by Vaswani et al. in *“Attention Is All You Need” (2017)*.\n",
    "It completely **replaces recurrence (RNNs)** and **convolution (CNNs)** with a mechanism called **Self-Attention**, making it highly parallel, scalable, and efficient for sequence processing (e.g., text, audio, code).\n",
    "\n",
    "---\n",
    "\n",
    "### Why Transformers\n",
    "\n",
    "Before Transformers:\n",
    "\n",
    "* **RNNs / LSTMs** processed data **sequentially**, so training couldn’t be parallelized.\n",
    "* Long sequences led to **vanishing gradients** and loss of long-term context.\n",
    "* **CNNs** handled local dependencies well but struggled with long-range relationships.\n",
    "\n",
    "**Transformers solve all of this** using **attention**, which allows the model to directly relate any two positions in a sequence — regardless of their distance.\n",
    "\n",
    "---\n",
    "\n",
    "### Core Idea\n",
    "\n",
    "> Instead of processing sequences step-by-step, the Transformer looks at the *entire sequence at once* and decides which parts are important for understanding each word or token.\n",
    "\n",
    "This is done using **self-attention**.\n",
    "\n",
    "---\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "The Transformer follows an **Encoder–Decoder** structure:\n",
    "\n",
    "```\n",
    "Input Sequence  ──► Encoder Stack ──► Context Representation ──► Decoder Stack ──► Output Sequence\n",
    "```\n",
    "\n",
    "Each **Encoder** and **Decoder** is made of **N = 6 identical layers** (in the original paper).\n",
    "\n",
    "---\n",
    "\n",
    "### Encoder Structure\n",
    "\n",
    "Each encoder layer has **two main sub-layers**:\n",
    "\n",
    "1. **Multi-Head Self-Attention**\n",
    "\n",
    "   * Allows each token to “look at” every other token in the sequence and learn relationships.\n",
    "   * Example: In “The cat sat on the mat,” the model can learn that “cat” relates strongly to “sat”.\n",
    "\n",
    "2. **Feed-Forward Neural Network (FFN)**\n",
    "\n",
    "   * Applies a position-wise fully connected network to each token representation.\n",
    "\n",
    "Each sub-layer has a **residual connection** followed by **Layer Normalization**:\n",
    "\n",
    "[\n",
    "\\text{Output} = \\text{LayerNorm}(x + \\text{Sublayer}(x))\n",
    "]\n",
    "\n",
    "---\n",
    "\n",
    "### Decoder Structure\n",
    "\n",
    "Each decoder layer has **three sub-layers**:\n",
    "\n",
    "1. **Masked Multi-Head Self-Attention**\n",
    "\n",
    "   * Similar to encoder self-attention, but uses **masking** so a token can only attend to *previous* tokens.\n",
    "   * This preserves the autoregressive property (no “peeking ahead”).\n",
    "\n",
    "2. **Encoder–Decoder Attention**\n",
    "\n",
    "   * Allows the decoder to attend to encoder outputs — i.e., focus on relevant parts of the input sequence.\n",
    "\n",
    "3. **Feed-Forward Neural Network (FFN)**\n",
    "\n",
    "   * Same as encoder FFN.\n",
    "\n",
    "Again, residual connections and layer normalization are applied after each sub-layer.\n",
    "\n",
    "---\n",
    "\n",
    "### The Self-Attention Mechanism\n",
    "\n",
    "Self-Attention computes a weighted combination of token representations based on **query (Q)**, **key (K)**, and **value (V)** matrices.\n",
    "\n",
    "Given:\n",
    "[\n",
    "Q = XW_Q,\\quad K = XW_K,\\quad V = XW_V\n",
    "]\n",
    "where ( X ) is the input sequence embedding matrix.\n",
    "\n",
    "Then:\n",
    "[\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}!\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "]\n",
    "\n",
    "**Key Intuition:**\n",
    "\n",
    "* ( QK^T ) gives **similarity scores** between tokens.\n",
    "* The **softmax** turns these scores into **attention weights**.\n",
    "* The model then takes a **weighted sum** of all token representations (( V )) to produce context-aware outputs.\n",
    "\n",
    "---\n",
    "\n",
    "### Multi-Head Attention\n",
    "\n",
    "Rather than performing one attention calculation, Transformers use multiple **attention heads** in parallel.\n",
    "\n",
    "Each head focuses on **different relationships** (e.g., grammar, context, position).\n",
    "\n",
    "The outputs of all heads are concatenated and linearly projected:\n",
    "[\n",
    "\\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W_O\n",
    "]\n",
    "where each head ( i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) ).\n",
    "\n",
    "---\n",
    "\n",
    "### Positional Encoding\n",
    "\n",
    "Since the Transformer has no recurrence, it needs to know **token order**.\n",
    "\n",
    "A **positional encoding** is added to the input embeddings:\n",
    "[\n",
    "PE_{(pos, 2i)} = \\sin!\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right), \\quad\n",
    "PE_{(pos, 2i+1)} = \\cos!\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\n",
    "]\n",
    "\n",
    "These sinusoidal patterns allow the model to infer both **absolute** and **relative** positions of tokens.\n",
    "\n",
    "---\n",
    "\n",
    "### Feed-Forward Network (FFN)\n",
    "\n",
    "Each token is independently passed through:\n",
    "[\n",
    "\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2\n",
    "]\n",
    "\n",
    "It provides **non-linearity** and helps mix information within each token representation.\n",
    "\n",
    "---\n",
    "\n",
    "## Residual Connection + Layer Normalization\n",
    "\n",
    "Each sub-layer output is:\n",
    "\n",
    "[\n",
    "\\text{Output} = \\text{LayerNorm}(x + \\text{Sublayer}(x))\n",
    "]\n",
    "This stabilizes training and helps gradient flow through deep networks (solving vanishing gradient problems).\n",
    "\n",
    "---\n",
    "\n",
    "### Training and Objective\n",
    "\n",
    "Transformers are trained to **predict the next token** (autoregressive training) using **cross-entropy loss**:\n",
    "[\n",
    "L = -\\sum_{t} \\log P(y_t | y_{<t}, x)\n",
    "]\n",
    "\n",
    "---\n",
    "\n",
    "### Advantages\n",
    "\n",
    "| **Aspect**         | **Transformer**              | **RNN/LSTM**      |\n",
    "| ------------------ | ---------------------------- | ----------------- |\n",
    "| Parallelization    | Fully parallel               | Sequential        |\n",
    "| Long-range context | Excellent                    | Weak              |\n",
    "| Training speed     | Fast (GPU-optimized)         | Slow              |\n",
    "| Interpretability   | Attention maps interpretable | Hard to visualize |\n",
    "\n",
    "---\n",
    "\n",
    "### Variants of Transformers\n",
    "\n",
    "| **Model**                    | **Type**        | **Application**                                 |\n",
    "| ---------------------------- | --------------- | ----------------------------------------------- |\n",
    "| **BERT**                     | Encoder-only    | Text understanding (classification, QA)         |\n",
    "| **GPT**                      | Decoder-only    | Text generation                                 |\n",
    "| **T5 / BART**                | Encoder–Decoder | Text-to-text tasks (translation, summarization) |\n",
    "| **Vision Transformer (ViT)** | Encoder-only    | Image classification                            |\n",
    "| **Perceiver / AudioLM**      | Multimodal      | Audio, video, text fusion                       |\n",
    "\n",
    "---\n",
    "\n",
    "### Intuitive Analogy\n",
    "\n",
    "Think of **attention** as a \"spotlight\" in a text:\n",
    "\n",
    "* When the model reads a word like “bank,” it scans the whole sentence (“river bank” vs “money bank”) to find which context is most relevant.\n",
    "* The attention mechanism lets it focus on the right context dynamically.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**\n",
    "\n",
    "| **Component**                       | **Function**                               |\n",
    "| ----------------------------------- | ------------------------------------------ |\n",
    "| **Embedding + Positional Encoding** | Represent tokens and their positions       |\n",
    "| **Self-Attention**                  | Capture relationships between tokens       |\n",
    "| **Multi-Head Attention**            | Look at multiple relationships in parallel |\n",
    "| **Feed-Forward Network**            | Nonlinear transformation for each token    |\n",
    "| **Residual + LayerNorm**            | Stabilize and speed up training            |\n",
    "| **Encoder–Decoder**                 | Map input sequence to output sequence      |\n",
    "\n",
    "---\n",
    "\n",
    "In short, **Transformers revolutionized deep learning** by removing recurrence and convolution entirely, replacing them with **attention mechanisms** that enable massive parallelization and long-context understanding — the foundation of models like **BERT**, **GPT**, and **T5**.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
