{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "260d6681",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```\n",
    "\n",
    "## Energy-Based Models (EBMs)\n",
    "\n",
    "An **Energy-Based Model (EBM)** is a **probabilistic deep learning framework** that learns to **assign low energy (high probability)** to desirable or realistic configurations (e.g., real images, valid sentences)\n",
    "and **high energy (low probability)** to undesirable or unlikely configurations.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Intuitive Analogy\n",
    "\n",
    "Think of an EBM as a **landscape of hills and valleys**:\n",
    "\n",
    "* Each point represents a data configuration (an image, sentence, etc.).\n",
    "* The **energy function** assigns a height to each configuration.\n",
    "* **Realistic data ‚Üí valleys (low energy)**\n",
    "* **Unrealistic data ‚Üí hills (high energy)**\n",
    "\n",
    "The model learns to **shape this energy landscape** so real samples lie in valleys, and fake ones lie on peaks.\n",
    "\n",
    "---\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "Let $x$ be a data point (e.g., image) and $E_\\theta(x)$ be an **energy function** parameterized by neural network parameters $\\theta$.\n",
    "\n",
    "The model defines a probability distribution as:\n",
    "\n",
    "$$\n",
    "p_\\theta(x) = \\frac{\\exp(-E_\\theta(x))}{Z_\\theta}\n",
    "$$\n",
    "\n",
    "where $Z_\\theta = \\int \\exp(-E_\\theta(x)) dx$ is the **partition function**\n",
    "that normalizes the probabilities (like in physics or Boltzmann statistics).\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Interpretation\n",
    "\n",
    "* **Low energy ‚áí high probability**\n",
    "* **High energy ‚áí low probability**\n",
    "\n",
    "So, learning an EBM means **minimizing energy for real data** and **increasing energy for fake data**.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Training Objective**\n",
    "\n",
    "Training tries to minimize the **negative log-likelihood (NLL)** of the data:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = -\\mathbb{E}*{x \\sim p*{data}} [\\log p_\\theta(x)]\n",
    "$$\n",
    "\n",
    "Expanding:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = \\mathbb{E}*{x \\sim p*{data}} [E_\\theta(x)] + \\log Z_\\theta\n",
    "$$\n",
    "\n",
    "Computing $Z_\\theta$ exactly is intractable for high-dimensional data,\n",
    "so **approximation methods** are used.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Gradient of the Loss**\n",
    "\n",
    "The gradient of the log-likelihood w.r.t. parameters is:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta \\log p_\\theta(x) = -\\nabla_\\theta E_\\theta(x) + \\mathbb{E}*{x' \\sim p*\\theta} [\\nabla_\\theta E_\\theta(x')]\n",
    "$$\n",
    "\n",
    "This has **two competing terms**:\n",
    "\n",
    "| **Term**                                                       | **Effect**                                                           |\n",
    "| -------------------------------------------------------------- | -------------------------------------------------------------------- |\n",
    "| $-\\nabla_\\theta E_\\theta(x)$                                 | Lowers energy for real data (pulls real data to valleys)             |\n",
    "| $\\mathbb{E}*{x' \\sim p*\\theta} [\\nabla_\\theta E_\\theta(x')]$ | Raises energy for fake samples (pushes unrealistic samples to hills) |\n",
    "\n",
    "Thus, the model **learns to separate real and fake data** by shaping the energy landscape.\n",
    "\n",
    "---\n",
    "\n",
    "### Sampling from the Model\n",
    "\n",
    "To generate data from an EBM, we must **sample** from $p_\\theta(x)$, which is challenging.\n",
    "\n",
    "A common method is **Markov Chain Monte Carlo (MCMC)**, especially **Langevin Dynamics**:\n",
    "\n",
    "$$\n",
    "x_{t+1} = x_t - \\frac{\\alpha}{2} \\nabla_x E_\\theta(x_t) + \\sqrt{\\alpha} , \\eta_t\n",
    "$$\n",
    "\n",
    "where $\\eta_t \\sim \\mathcal{N}(0, I)$\n",
    "\n",
    "This is like a **noisy gradient descent** over the energy surface ‚Äî samples move downhill into energy valleys.\n",
    "\n",
    "---\n",
    "\n",
    "### Training Algorithms\n",
    "\n",
    "| **Method**                                              | **Description**                                                                   |\n",
    "| ------------------------------------------------------- | --------------------------------------------------------------------------------- |\n",
    "| **Contrastive Divergence (CD)**                         | Approximate the expectation using a few Gibbs sampling steps (used in RBMs).      |\n",
    "| **Persistent Contrastive Divergence (PCD)**             | Maintain a persistent chain of samples to stabilize training.                     |\n",
    "| **Score Matching / Noise-Contrastive Estimation (NCE)** | Avoid computing partition function by comparing energy on real vs. noise samples. |\n",
    "| **Langevin Dynamics (Modern EBMs)**                     | Iteratively refine samples using gradients of the energy function.                |\n",
    "\n",
    "---\n",
    "\n",
    "### Relationship with Other Models\n",
    "\n",
    "| **Model**                              | **Connection to EBM**                                                                  |\n",
    "| -------------------------------------- | -------------------------------------------------------------------------------------- |\n",
    "| **Boltzmann Machine**                  | Original EBM with binary neurons and stochastic sampling.                              |\n",
    "| **Restricted Boltzmann Machine (RBM)** | Simplified EBM (visible and hidden units with no intra-layer connections).             |\n",
    "| **GANs**                               | Generator creates low-energy samples, discriminator implicitly learns energy function. |\n",
    "| **VAEs**                               | Use explicit latent variable models; EBMs are implicit.                                |\n",
    "| **Diffusion Models**                   | Can be seen as learning an energy landscape over noise trajectories.                   |\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Intuition with Image Modeling\n",
    "\n",
    "* Suppose $E_\\theta(x)$ is a CNN that outputs a scalar energy for image $x$.\n",
    "* During training:\n",
    "\n",
    "  1. Real images ‚Üí lower energy (valleys)\n",
    "  2. Random/noisy images ‚Üí higher energy (hills)\n",
    "* At inference:\n",
    "\n",
    "  * Start from random noise and use **Langevin dynamics** to descend into a low-energy region ‚Äî this yields a realistic-looking image.\n",
    "\n",
    "---\n",
    "\n",
    "### Applications of EBMs\n",
    "\n",
    "| **Domain**                   | **Use Case**                                              |\n",
    "| ---------------------------- | --------------------------------------------------------- |\n",
    "| **Image Generation**         | Generate realistic samples (e.g., CIFAR-10, CelebA).      |\n",
    "| **Anomaly Detection**        | Abnormal data has high energy (low likelihood).           |\n",
    "| **Reinforcement Learning**   | Model energy as potential landscape for optimal policies. |\n",
    "| **Representation Learning**  | Learn useful feature embeddings without supervision.      |\n",
    "| **Denoising and Inpainting** | Model ‚Äúnatural‚Äù energy surfaces of clean data.            |\n",
    "\n",
    "---\n",
    "\n",
    "###  Advantages and Limitations\n",
    "\n",
    "| **Aspect**                | **Advantage**                       | **Limitation**                   |\n",
    "| ------------------------- | ----------------------------------- | -------------------------------- |\n",
    "| **Likelihood estimation** | Exact formula (up to normalization) | Partition function intractable   |\n",
    "| **Training stability**    | No adversarial training needed      | MCMC sampling is slow            |\n",
    "| **Flexibility**           | Works for any data type             | Hard to scale to high-res data   |\n",
    "| **Interpretability**      | Energy landscape is meaningful      | Gradient estimation can be noisy |\n",
    "\n",
    "---\n",
    "\n",
    "### Modern Implementations\n",
    "\n",
    "Modern deep EBMs use **neural networks** (usually CNNs or ResNets) to parameterize $E_\\theta(x)$.\n",
    "\n",
    "Examples:\n",
    "\n",
    "* **Deep Energy Models (LeCun et al.)**\n",
    "* **Score-Based Models** (e.g., NCSN, diffusion-based reformulations)\n",
    "* **EBMs trained with Langevin sampling** (Du & Mordatch, 2020)\n",
    "\n",
    "---\n",
    "\n",
    "**Summary**\n",
    "\n",
    "| **Concept**            | **Description**                                                |\n",
    "| ---------------------- | -------------------------------------------------------------- |\n",
    "| **Goal**               | Learn energy surface over data space                           |\n",
    "| **Energy Function**    | $E_\\theta(x)$: neural network assigns ‚Äúenergy‚Äù to each input |\n",
    "| **Training Objective** | Push down energy of real data, raise for fake data             |\n",
    "| **Sampling**           | MCMC or Langevin dynamics                                      |\n",
    "| **Key Equation**       | $p_\\theta(x) = \\frac{e^{-E_\\theta(x)}}{Z_\\theta}$            |\n",
    "| **Applications**       | Generation, anomaly detection, unsupervised learning           |\n",
    "\n",
    "---\n",
    "\n",
    "**In short:**\n",
    "\n",
    "An **Energy-Based Model** learns a function $E_\\theta(x)$ that represents how ‚Äúcompatible‚Äù or ‚Äúrealistic‚Äù a configuration is.\n",
    "It bridges the gap between **explicit probabilistic modeling (like VAEs)** and **implicit learning (like GANs)** by shaping an energy landscape that defines the data distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b91d95",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
