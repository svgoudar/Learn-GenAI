{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b329c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q langchain langchain-community langchain-openai \\\n",
    "    chromadb pypdf pandas unstructured selenium beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09981ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import pandas as pd\n",
    "\n",
    "from langchain_community.document_loaders import (\n",
    "    DirectoryLoader,\n",
    "    TextLoader,\n",
    "    PyPDFLoader,\n",
    "    CSVLoader,\n",
    "    WebBaseLoader,\n",
    "    UnstructuredURLLoader\n",
    ")\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    CharacterTextSplitter\n",
    ")\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bd0e08",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563abb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base paths\n",
    "BASE_DIR = Path(r\"c:\\Github\\GENAI\\Learn-GenAI\\datasets\")\n",
    "TXT_DIR = BASE_DIR / \"txt\"\n",
    "PDF_DIR = BASE_DIR / \"pdf\"\n",
    "CSV_DIR = BASE_DIR / \"csv\"\n",
    "DB_DIR = BASE_DIR / \"db\"\n",
    "WEB_FILE = BASE_DIR / \"web\" / \"websites.txt\"\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = Path(r\"c:\\Github\\GENAI\\Learn-GenAI\\rag_data\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "CHROMA_DIR = OUTPUT_DIR / \"langchain_chroma\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0de38bc",
   "metadata": {},
   "source": [
    "## 1. Loading Text Files\n",
    "\n",
    "Use DirectoryLoader with TextLoader for efficient batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f1b279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_categorized_texts() -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load text files from categorized folders using DirectoryLoader.\n",
    "    Adds category metadata to each document.\n",
    "    \"\"\"\n",
    "    all_documents = []\n",
    "    \n",
    "    # Get all category folders\n",
    "    categories = [d.name for d in TXT_DIR.iterdir() if d.is_dir()]\n",
    "    \n",
    "    print(f\"Found categories: {categories}\")\n",
    "    \n",
    "    for category in categories:\n",
    "        category_path = TXT_DIR / category\n",
    "        \n",
    "        # Use DirectoryLoader for batch loading\n",
    "        loader = DirectoryLoader(\n",
    "            str(category_path),\n",
    "            glob=\"**/*.txt\",\n",
    "            loader_cls=TextLoader,\n",
    "            show_progress=True,\n",
    "            use_multithreading=True  # Enable parallel loading\n",
    "        )\n",
    "        \n",
    "        documents = loader.load()\n",
    "        \n",
    "        # Add metadata\n",
    "        for doc in documents:\n",
    "            doc.metadata.update({\n",
    "                'category': category,\n",
    "                'source_type': 'text',\n",
    "                'file_name': Path(doc.metadata.get('source', '')).name\n",
    "            })\n",
    "        \n",
    "        all_documents.extend(documents)\n",
    "        print(f\"Loaded {len(documents)} documents from {category}\")\n",
    "    \n",
    "    print(f\"\\nTotal text documents: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "# Load text documents\n",
    "text_documents = load_categorized_texts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ae1185",
   "metadata": {},
   "source": [
    "## 2. Loading PDF Files\n",
    "\n",
    "Use PyPDFLoader for page-by-page extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9d8110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf_documents() -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load PDF files using PyPDFLoader.\n",
    "    Each page becomes a separate document.\n",
    "    \"\"\"\n",
    "    all_documents = []\n",
    "    \n",
    "    pdf_files = list(PDF_DIR.glob('*.pdf'))\n",
    "    print(f\"Found {len(pdf_files)} PDF files\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Enrich metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata.update({\n",
    "                    'source_type': 'pdf',\n",
    "                    'file_name': pdf_file.name,\n",
    "                    'category': 'technical_documentation'\n",
    "                })\n",
    "            \n",
    "            all_documents.extend(documents)\n",
    "            print(f\"Loaded {len(documents)} pages from {pdf_file.name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {pdf_file.name}: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal PDF documents: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "# Load PDFs\n",
    "pdf_documents = load_pdf_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e062b5",
   "metadata": {},
   "source": [
    "## 3. Loading CSV Files\n",
    "\n",
    "Use CSVLoader to convert rows to documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead106df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_documents() -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load CSV files using CSVLoader.\n",
    "    Each row becomes a document.\n",
    "    \"\"\"\n",
    "    all_documents = []\n",
    "    \n",
    "    csv_files = list(CSV_DIR.glob('*.csv'))\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        try:\n",
    "            loader = CSVLoader(\n",
    "                file_path=str(csv_file),\n",
    "                encoding='utf-8',\n",
    "                csv_args={'delimiter': ','}\n",
    "            )\n",
    "            \n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Add metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata.update({\n",
    "                    'source_type': 'csv',\n",
    "                    'file_name': csv_file.name\n",
    "                })\n",
    "            \n",
    "            all_documents.extend(documents)\n",
    "            print(f\"Loaded {len(documents)} rows from {csv_file.name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {csv_file.name}: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal CSV documents: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "# Load CSV data\n",
    "csv_documents = load_csv_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3a9072",
   "metadata": {},
   "source": [
    "## 4. Loading SQLite Database\n",
    "\n",
    "Query database and convert to documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249878cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "def load_database_documents(db_path: Path) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load SQLite database tables as documents.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    if not db_path.exists():\n",
    "        print(f\"Database not found: {db_path}\")\n",
    "        return documents\n",
    "    \n",
    "    conn = sqlite3.connect(str(db_path))\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Get all tables\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tables = [row[0] for row in cursor.fetchall()]\n",
    "    \n",
    "    print(f\"Found tables: {tables}\")\n",
    "    \n",
    "    for table in tables:\n",
    "        # Get column names\n",
    "        cursor.execute(f\"PRAGMA table_info({table})\")\n",
    "        columns = [col[1] for col in cursor.fetchall()]\n",
    "        \n",
    "        # Fetch rows\n",
    "        cursor.execute(f\"SELECT * FROM {table}\")\n",
    "        rows = cursor.fetchall()\n",
    "        \n",
    "        # Convert to documents\n",
    "        for row in rows:\n",
    "            row_dict = dict(zip(columns, row))\n",
    "            \n",
    "            # Create text content\n",
    "            content = \"\\n\".join([f\"{k}: {v}\" for k, v in row_dict.items()])\n",
    "            \n",
    "            doc = Document(\n",
    "                page_content=content,\n",
    "                metadata={\n",
    "                    'source_type': 'database',\n",
    "                    'table_name': table,\n",
    "                    'db_name': db_path.name,\n",
    "                    **{k: str(v) for k, v in row_dict.items()}\n",
    "                }\n",
    "            )\n",
    "            documents.append(doc)\n",
    "        \n",
    "        print(f\"Loaded {len(rows)} rows from table '{table}'\")\n",
    "    \n",
    "    conn.close()\n",
    "    print(f\"\\nTotal database documents: {len(documents)}\")\n",
    "    return documents\n",
    "\n",
    "# Load database\n",
    "db_path = DB_DIR / \"movies.sqlite\"\n",
    "db_documents = load_database_documents(db_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f507db",
   "metadata": {},
   "source": [
    "## 5. Loading Web URLs\n",
    "\n",
    "Fetch content from web pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95716d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_web_documents() -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load web pages from URLs using WebBaseLoader.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    if not WEB_FILE.exists():\n",
    "        print(\"No websites.txt file found\")\n",
    "        return documents\n",
    "    \n",
    "    # Read URLs\n",
    "    with open(WEB_FILE, 'r') as f:\n",
    "        urls = [line.strip() for line in f if line.strip() and line.startswith('http')]\n",
    "    \n",
    "    print(f\"Found {len(urls)} URLs\")\n",
    "    \n",
    "    try:\n",
    "        # Use WebBaseLoader for batch loading\n",
    "        loader = WebBaseLoader(urls)\n",
    "        documents = loader.load()\n",
    "        \n",
    "        # Add metadata\n",
    "        for doc in documents:\n",
    "            doc.metadata.update({\n",
    "                'source_type': 'web',\n",
    "                'category': 'documentation'\n",
    "            })\n",
    "        \n",
    "        print(f\"Successfully loaded {len(documents)} web pages\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading web pages: {e}\")\n",
    "        print(\"Note: Web scraping may be blocked or require browser automation\")\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Load web documents\n",
    "web_documents = load_web_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b3af59",
   "metadata": {},
   "source": [
    "## 6. Combine All Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5e4844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all documents\n",
    "all_documents = (\n",
    "    text_documents + \n",
    "    pdf_documents + \n",
    "    csv_documents + \n",
    "    db_documents + \n",
    "    web_documents\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"DOCUMENT LOADING SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Text documents: {len(text_documents)}\")\n",
    "print(f\"PDF documents: {len(pdf_documents)}\")\n",
    "print(f\"CSV documents: {len(csv_documents)}\")\n",
    "print(f\"Database documents: {len(db_documents)}\")\n",
    "print(f\"Web documents: {len(web_documents)}\")\n",
    "print(f\"{'-'*60}\")\n",
    "print(f\"TOTAL DOCUMENTS: {len(all_documents)}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5105e1f5",
   "metadata": {},
   "source": [
    "## 7. Split Documents into Chunks\n",
    "\n",
    "Use text splitters for optimal chunk sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a67b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Split documents\n",
    "splits = text_splitter.split_documents(all_documents)\n",
    "\n",
    "print(f\"\\nSplit {len(all_documents)} documents into {len(splits)} chunks\")\n",
    "print(f\"Average chunks per document: {len(splits)/len(all_documents):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cad8a26",
   "metadata": {},
   "source": [
    "## 8. Create Vector Store with ChromaDB\n",
    "\n",
    "Build a persistent vector store for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0e9fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Initialize embeddings (using free local model)\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "print(\"Creating ChromaDB vector store...\")\n",
    "\n",
    "# Create vector store\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=str(CHROMA_DIR),\n",
    "    collection_name=\"rag_collection\"\n",
    ")\n",
    "\n",
    "print(f\"Vector store created with {len(splits)} chunks\")\n",
    "print(f\"Persisted to: {CHROMA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495f7b96",
   "metadata": {},
   "source": [
    "## 9. Test Retrieval\n",
    "\n",
    "Query the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d220d21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 5}\n",
    ")\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"What are recent business developments?\",\n",
    "    \"Explain machine learning transformers\",\n",
    "    \"What's in the crime safety data?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    results = retriever.get_relevant_documents(query)\n",
    "    \n",
    "    for i, doc in enumerate(results[:3], 1):\n",
    "        print(f\"\\nResult {i}:\")\n",
    "        print(f\"Category: {doc.metadata.get('category', 'N/A')}\")\n",
    "        print(f\"Source: {doc.metadata.get('source_type', 'N/A')}\")\n",
    "        print(f\"Content: {doc.page_content[:200]}...\")\n",
    "    print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ba6a33",
   "metadata": {},
   "source": [
    "## 10. Metadata Filtering\n",
    "\n",
    "Filter by source type or category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0188459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search with metadata filter - business documents only\n",
    "business_retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\n",
    "        \"k\": 5,\n",
    "        \"filter\": {\"category\": \"business\"}\n",
    "    }\n",
    ")\n",
    "\n",
    "results = business_retriever.get_relevant_documents(\n",
    "    \"What are the latest business trends?\"\n",
    ")\n",
    "\n",
    "print(\"Business-filtered results:\")\n",
    "for i, doc in enumerate(results[:3], 1):\n",
    "    print(f\"\\n{i}. {doc.metadata.get('file_name', 'Unknown')}\")\n",
    "    print(f\"   {doc.page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b4fd96",
   "metadata": {},
   "source": [
    "## 11. Create QA Chain\n",
    "\n",
    "Build a complete RAG pipeline with LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54af12c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Initialize LLM (using local Ollama)\n",
    "# Make sure Ollama is running with: ollama serve\n",
    "# And a model is pulled: ollama pull llama2\n",
    "\n",
    "try:\n",
    "    llm = Ollama(model=\"llama2\", temperature=0)\n",
    "    \n",
    "    # Create QA chain\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    \n",
    "    # Test the chain\n",
    "    query = \"What are the key concepts in machine learning?\"\n",
    "    result = qa_chain({\"query\": query})\n",
    "    \n",
    "    print(f\"Question: {query}\")\n",
    "    print(f\"\\nAnswer: {result['result']}\")\n",
    "    print(f\"\\nSources used: {len(result['source_documents'])}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"LLM not available: {e}\")\n",
    "    print(\"Install Ollama and run: ollama pull llama2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690a3500",
   "metadata": {},
   "source": [
    "## 12. Advanced: Incremental Loading\n",
    "\n",
    "Add new documents to existing vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944ea569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_documents_incrementally(new_documents: List[Document]):\n",
    "    \"\"\"\n",
    "    Add new documents to existing ChromaDB collection.\n",
    "    \"\"\"\n",
    "    # Load existing vector store\n",
    "    existing_vectorstore = Chroma(\n",
    "        persist_directory=str(CHROMA_DIR),\n",
    "        embedding_function=embeddings,\n",
    "        collection_name=\"rag_collection\"\n",
    "    )\n",
    "    \n",
    "    # Split new documents\n",
    "    new_splits = text_splitter.split_documents(new_documents)\n",
    "    \n",
    "    # Add to vector store\n",
    "    existing_vectorstore.add_documents(new_splits)\n",
    "    \n",
    "    print(f\"Added {len(new_splits)} new chunks to vector store\")\n",
    "    return existing_vectorstore\n",
    "\n",
    "# Example: Add new documents\n",
    "# new_docs = [Document(page_content=\"New content\", metadata={\"source\": \"new\"})]\n",
    "# add_documents_incrementally(new_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c87f59c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. ✅ Batch loading text files with DirectoryLoader\n",
    "2. ✅ PDF loading with PyPDFLoader (page-level)\n",
    "3. ✅ CSV loading with CSVLoader\n",
    "4. ✅ SQLite database querying\n",
    "5. ✅ Web scraping with WebBaseLoader\n",
    "6. ✅ Document chunking with RecursiveCharacterTextSplitter\n",
    "7. ✅ Vector store creation with ChromaDB\n",
    "8. ✅ Metadata filtering for targeted retrieval\n",
    "9. ✅ Complete RAG pipeline with QA chain\n",
    "10. ✅ Incremental document addition\n",
    "\n",
    "**LangChain Advantages:**\n",
    "- Rich ecosystem of loaders for different formats\n",
    "- Easy integration with LLMs and chains\n",
    "- Built-in text splitters with smart chunking\n",
    "- Multiple vector store backends\n",
    "\n",
    "**Next Steps:**\n",
    "- Configure OpenAI or Azure OpenAI embeddings\n",
    "- Implement conversational retrieval chains\n",
    "- Add memory for multi-turn conversations\n",
    "- Experiment with different retrievers (MMR, similarity threshold)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
