{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1025118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q llama-index llama-index-readers-file llama-index-readers-web \\\n",
    "    llama-index-readers-database pypdf pandas sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6216af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import pandas as pd\n",
    "\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    Document,\n",
    "    StorageContext,\n",
    "    Settings\n",
    ")\n",
    "from llama_index.core.node_parser import (\n",
    "    SentenceSplitter,\n",
    "    SemanticSplitterNodeParser\n",
    ")\n",
    "from llama_index.readers.file import PDFReader, CSVReader\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "from llama_index.core.schema import MetadataMode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff85cbaa",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c62ed05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base paths\n",
    "BASE_DIR = Path(r\"c:\\Github\\GENAI\\Learn-GenAI\\datasets\")\n",
    "TXT_DIR = BASE_DIR / \"txt\"\n",
    "PDF_DIR = BASE_DIR / \"pdf\"\n",
    "CSV_DIR = BASE_DIR / \"csv\"\n",
    "DB_DIR = BASE_DIR / \"db\"\n",
    "WEB_FILE = BASE_DIR / \"web\" / \"websites.txt\"\n",
    "\n",
    "# Output directory for processed data\n",
    "OUTPUT_DIR = Path(r\"c:\\Github\\GENAI\\Learn-GenAI\\rag_data\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d262f776",
   "metadata": {},
   "source": [
    "## 1. Loading Text Files (Categorized Content)\n",
    "\n",
    "Efficiently load text files from multiple categories with metadata enrichment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f89462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_categorized_texts() -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load text files from categorized folders with metadata.\n",
    "    Uses SimpleDirectoryReader with recursive loading and metadata extraction.\n",
    "    \"\"\"\n",
    "    all_documents = []\n",
    "    \n",
    "    # Get all category folders\n",
    "    categories = [d.name for d in TXT_DIR.iterdir() if d.is_dir()]\n",
    "    \n",
    "    print(f\"Found categories: {categories}\")\n",
    "    \n",
    "    for category in categories:\n",
    "        category_path = TXT_DIR / category\n",
    "        \n",
    "        # Use SimpleDirectoryReader for efficient batch loading\n",
    "        reader = SimpleDirectoryReader(\n",
    "            input_dir=str(category_path),\n",
    "            recursive=False,\n",
    "            required_exts=['.txt'],\n",
    "            filename_as_id=True  # Use filename as document ID\n",
    "        )\n",
    "        \n",
    "        documents = reader.load_data()\n",
    "        \n",
    "        # Enrich with category metadata\n",
    "        for doc in documents:\n",
    "            doc.metadata.update({\n",
    "                'category': category,\n",
    "                'source_type': 'text',\n",
    "                'file_name': Path(doc.metadata.get('file_path', '')).name\n",
    "            })\n",
    "        \n",
    "        all_documents.extend(documents)\n",
    "        print(f\"Loaded {len(documents)} documents from {category}\")\n",
    "    \n",
    "    print(f\"\\nTotal text documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "# Load text documents\n",
    "text_documents = load_categorized_texts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5187ff24",
   "metadata": {},
   "source": [
    "## 2. Loading PDF Files (Books)\n",
    "\n",
    "Load PDF books with page-level metadata for precise retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e775306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf_documents() -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load PDF documents with page-level granularity.\n",
    "    \"\"\"\n",
    "    # SimpleDirectoryReader handles PDFs natively\n",
    "    reader = SimpleDirectoryReader(\n",
    "        input_dir=str(PDF_DIR),\n",
    "        recursive=False,\n",
    "        required_exts=['.pdf'],\n",
    "        filename_as_id=True\n",
    "    )\n",
    "    \n",
    "    documents = reader.load_data()\n",
    "    \n",
    "    # Add metadata\n",
    "    for doc in documents:\n",
    "        file_name = Path(doc.metadata.get('file_path', '')).name\n",
    "        doc.metadata.update({\n",
    "            'source_type': 'pdf',\n",
    "            'file_name': file_name,\n",
    "            'category': 'technical_documentation'\n",
    "        })\n",
    "    \n",
    "    print(f\"Loaded {len(documents)} PDF documents\")\n",
    "    return documents\n",
    "\n",
    "# Load PDFs\n",
    "pdf_documents = load_pdf_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff6fc19",
   "metadata": {},
   "source": [
    "## 3. Loading CSV Files\n",
    "\n",
    "Load CSV data with pandas for structured data handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285a63b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_documents() -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load CSV files and convert rows to documents.\n",
    "    Each row becomes a document with column names as metadata.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    csv_files = list(CSV_DIR.glob('*.csv'))\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        \n",
    "        # Convert each row to a document\n",
    "        for idx, row in df.iterrows():\n",
    "            # Create text content from all columns\n",
    "            text_parts = [f\"{col}: {val}\" for col, val in row.items()]\n",
    "            text_content = \"\\n\".join(text_parts)\n",
    "            \n",
    "            # Create document with metadata\n",
    "            doc = Document(\n",
    "                text=text_content,\n",
    "                metadata={\n",
    "                    'source_type': 'csv',\n",
    "                    'file_name': csv_file.name,\n",
    "                    'row_index': idx,\n",
    "                    **{k: str(v) for k, v in row.items()}  # Add all columns as metadata\n",
    "                }\n",
    "            )\n",
    "            documents.append(doc)\n",
    "        \n",
    "        print(f\"Loaded {len(df)} rows from {csv_file.name}\")\n",
    "    \n",
    "    print(f\"\\nTotal CSV documents: {len(documents)}\")\n",
    "    return documents\n",
    "\n",
    "# Load CSV data\n",
    "csv_documents = load_csv_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6311a24",
   "metadata": {},
   "source": [
    "## 4. Loading SQLite Database\n",
    "\n",
    "Query database and convert results to documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b153f73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "def load_database_documents(db_path: Path, table_name: str = None) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load data from SQLite database.\n",
    "    If table_name is None, loads all tables.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    conn = sqlite3.connect(str(db_path))\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Get all tables if not specified\n",
    "    if table_name is None:\n",
    "        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "        tables = [row[0] for row in cursor.fetchall()]\n",
    "    else:\n",
    "        tables = [table_name]\n",
    "    \n",
    "    for table in tables:\n",
    "        # Get column names\n",
    "        cursor.execute(f\"PRAGMA table_info({table})\")\n",
    "        columns = [col[1] for col in cursor.fetchall()]\n",
    "        \n",
    "        # Fetch all rows\n",
    "        cursor.execute(f\"SELECT * FROM {table}\")\n",
    "        rows = cursor.fetchall()\n",
    "        \n",
    "        # Convert each row to a document\n",
    "        for row in rows:\n",
    "            row_dict = dict(zip(columns, row))\n",
    "            \n",
    "            # Create text content\n",
    "            text_parts = [f\"{col}: {val}\" for col, val in row_dict.items()]\n",
    "            text_content = \"\\n\".join(text_parts)\n",
    "            \n",
    "            doc = Document(\n",
    "                text=text_content,\n",
    "                metadata={\n",
    "                    'source_type': 'database',\n",
    "                    'table_name': table,\n",
    "                    'db_name': db_path.name,\n",
    "                    **{k: str(v) for k, v in row_dict.items()}\n",
    "                }\n",
    "            )\n",
    "            documents.append(doc)\n",
    "        \n",
    "        print(f\"Loaded {len(rows)} rows from table '{table}'\")\n",
    "    \n",
    "    conn.close()\n",
    "    print(f\"\\nTotal database documents: {len(documents)}\")\n",
    "    return documents\n",
    "\n",
    "# Load database\n",
    "db_path = DB_DIR / \"movies.sqlite\"\n",
    "if db_path.exists():\n",
    "    db_documents = load_database_documents(db_path)\n",
    "else:\n",
    "    db_documents = []\n",
    "    print(\"No database file found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bc6cbd",
   "metadata": {},
   "source": [
    "## 5. Loading Web URLs\n",
    "\n",
    "Fetch content from web pages listed in the websites file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d7669c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_web_documents() -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load web pages from URLs listed in websites.txt\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    if not WEB_FILE.exists():\n",
    "        print(\"No websites.txt file found\")\n",
    "        return documents\n",
    "    \n",
    "    # Read URLs from file\n",
    "    with open(WEB_FILE, 'r') as f:\n",
    "        urls = [line.strip() for line in f if line.strip() and line.startswith('http')]\n",
    "    \n",
    "    print(f\"Found {len(urls)} URLs to process\")\n",
    "    \n",
    "    # Use SimpleWebPageReader\n",
    "    try:\n",
    "        reader = SimpleWebPageReader(html_to_text=True)\n",
    "        documents = reader.load_data(urls)\n",
    "        \n",
    "        # Add metadata\n",
    "        for doc, url in zip(documents, urls):\n",
    "            doc.metadata.update({\n",
    "                'source_type': 'web',\n",
    "                'url': url,\n",
    "                'category': 'documentation'\n",
    "            })\n",
    "        \n",
    "        print(f\"Successfully loaded {len(documents)} web pages\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading web pages: {e}\")\n",
    "        print(\"Note: Web scraping may require additional setup or may be blocked\")\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Load web documents\n",
    "web_documents = load_web_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25790d3",
   "metadata": {},
   "source": [
    "## 6. Combine All Documents\n",
    "\n",
    "Merge all data sources into a unified document collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ee0fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all documents\n",
    "all_documents = (\n",
    "    text_documents + \n",
    "    pdf_documents + \n",
    "    csv_documents + \n",
    "    db_documents + \n",
    "    web_documents\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"DOCUMENT LOADING SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Text documents: {len(text_documents)}\")\n",
    "print(f\"PDF documents: {len(pdf_documents)}\")\n",
    "print(f\"CSV documents: {len(csv_documents)}\")\n",
    "print(f\"Database documents: {len(db_documents)}\")\n",
    "print(f\"Web documents: {len(web_documents)}\")\n",
    "print(f\"{'-'*60}\")\n",
    "print(f\"TOTAL DOCUMENTS: {len(all_documents)}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56988f7",
   "metadata": {},
   "source": [
    "## 7. Parse Documents into Nodes\n",
    "\n",
    "Use efficient node parsers to chunk documents for optimal retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac54fa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create node parser with optimal chunk sizes\n",
    "node_parser = SentenceSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50,\n",
    "    separator=\" \"\n",
    ")\n",
    "\n",
    "# Parse documents into nodes\n",
    "nodes = node_parser.get_nodes_from_documents(all_documents)\n",
    "\n",
    "print(f\"\\nCreated {len(nodes)} nodes from {len(all_documents)} documents\")\n",
    "print(f\"Average nodes per document: {len(nodes)/len(all_documents):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae74f219",
   "metadata": {},
   "source": [
    "## 8. Create Vector Store Index\n",
    "\n",
    "Build an index for efficient semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7bd8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index from nodes\n",
    "# Note: This will use the default embedding model\n",
    "# You can configure Settings.embed_model for custom embeddings\n",
    "\n",
    "print(\"Creating vector store index...\")\n",
    "index = VectorStoreIndex(nodes)\n",
    "\n",
    "# Persist index to disk\n",
    "index.storage_context.persist(persist_dir=str(OUTPUT_DIR / \"llamaindex_storage\"))\n",
    "\n",
    "print(f\"Index created and saved to {OUTPUT_DIR / 'llamaindex_storage'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f67e6e",
   "metadata": {},
   "source": [
    "## 9. Query the Index\n",
    "\n",
    "Test retrieval with sample queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc7897a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create query engine\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=5,\n",
    "    response_mode=\"compact\"\n",
    ")\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"What are the main topics in business articles?\",\n",
    "    \"Tell me about machine learning concepts from the PDFs\",\n",
    "    \"What information is in the crime safety dataset?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(\"-\" * 80)\n",
    "    response = query_engine.query(query)\n",
    "    print(f\"Response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0404ea3a",
   "metadata": {},
   "source": [
    "## 10. Metadata Filtering\n",
    "\n",
    "Demonstrate filtered queries by source type or category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc431f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores import MetadataFilters, ExactMatchFilter\n",
    "\n",
    "# Query only business documents\n",
    "business_query_engine = index.as_query_engine(\n",
    "    similarity_top_k=5,\n",
    "    filters=MetadataFilters(\n",
    "        filters=[ExactMatchFilter(key=\"category\", value=\"business\")]\n",
    "    )\n",
    ")\n",
    "\n",
    "response = business_query_engine.query(\"What are the recent business trends?\")\n",
    "print(f\"Business-filtered response: {response}\")\n",
    "\n",
    "# Query only PDFs\n",
    "pdf_query_engine = index.as_query_engine(\n",
    "    similarity_top_k=5,\n",
    "    filters=MetadataFilters(\n",
    "        filters=[ExactMatchFilter(key=\"source_type\", value=\"pdf\")]\n",
    "    )\n",
    ")\n",
    "\n",
    "response = pdf_query_engine.query(\"Explain transformers in machine learning\")\n",
    "print(f\"\\nPDF-filtered response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d6efc3",
   "metadata": {},
   "source": [
    "## 11. Advanced: Batch Processing for Large Datasets\n",
    "\n",
    "Process documents in batches to manage memory efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7254902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_index_in_batches(batch_size: int = 100):\n",
    "    \"\"\"\n",
    "    Load and index documents in batches for memory efficiency.\n",
    "    \"\"\"\n",
    "    from llama_index.core import load_index_from_storage\n",
    "    \n",
    "    # Initialize or load existing index\n",
    "    storage_dir = OUTPUT_DIR / \"llamaindex_storage_batched\"\n",
    "    \n",
    "    if storage_dir.exists():\n",
    "        storage_context = StorageContext.from_defaults(persist_dir=str(storage_dir))\n",
    "        index = load_index_from_storage(storage_context)\n",
    "        print(\"Loaded existing index\")\n",
    "    else:\n",
    "        index = VectorStoreIndex([])\n",
    "        print(\"Created new index\")\n",
    "    \n",
    "    # Process documents in batches\n",
    "    total_docs = len(all_documents)\n",
    "    \n",
    "    for i in range(0, total_docs, batch_size):\n",
    "        batch = all_documents[i:i+batch_size]\n",
    "        print(f\"Processing batch {i//batch_size + 1} ({len(batch)} documents)...\")\n",
    "        \n",
    "        # Parse and insert batch\n",
    "        batch_nodes = node_parser.get_nodes_from_documents(batch)\n",
    "        index.insert_nodes(batch_nodes)\n",
    "        \n",
    "        # Persist after each batch\n",
    "        index.storage_context.persist(persist_dir=str(storage_dir))\n",
    "    \n",
    "    print(f\"\\nBatch indexing complete. Processed {total_docs} documents.\")\n",
    "    return index\n",
    "\n",
    "# Uncomment to run batch processing\n",
    "# batched_index = load_and_index_in_batches(batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9213927f",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. ✅ Loading text files from categorized folders\n",
    "2. ✅ Loading PDF documents with metadata\n",
    "3. ✅ Converting CSV data to searchable documents\n",
    "4. ✅ Querying SQLite databases\n",
    "5. ✅ Fetching web content\n",
    "6. ✅ Combining all sources into unified index\n",
    "7. ✅ Efficient node parsing and chunking\n",
    "8. ✅ Metadata-based filtering\n",
    "9. ✅ Batch processing for scalability\n",
    "\n",
    "**Next Steps:**\n",
    "- Configure custom embedding models (OpenAI, HuggingFace, etc.)\n",
    "- Experiment with different node parsers (SemanticSplitter, SentenceWindow)\n",
    "- Implement hybrid search (keyword + semantic)\n",
    "- Add reranking for improved retrieval accuracy"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
